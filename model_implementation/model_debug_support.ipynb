{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is more of a debugging tool to check the correctness of the model than an informational notebook.\n",
    "# I have blocks of code that helped me identify specific issues in the model. I am keeping this notebook so that\n",
    "# it can be used as a reference when trying to debug issues with Deep Learning models in the future.\n",
    "#\n",
    "# In this notebook, we do the following:\n",
    "# \n",
    "# 1) We will try to count (manually) the number of parameters in the Machine Translation model that we built in \n",
    "#    the .py files. The expected count (from built modules) should be the same as the manual count. Since, we \n",
    "#    will train the model using the code written in .py files, we need to be sure that the model is built is \n",
    "#    correctly. This is a good way to check that. \n",
    "# 2) We will try to create and run the Data Loaders for the Machine Translation model.\n",
    "# 3) I have realized that the Telugu tokenizer is dividing the sentences into too many tokens. This made me set\n",
    "#    the vocabulary size for Telugu to a higher value (50000). This still doesn't seem to solve the problem very\n",
    "#    well, but it's an improvement. This also made me realize that we want to control the vocabulary size for \n",
    "#    the English and Telugu separately.\n",
    "# \n",
    "# We did the parameters counting exercise before in 'step_15_machine_translation_model.ipynb' notebook \n",
    "# (LINK TO THE NOTEBOOK) before with smaller model using dummy parameters. This time, we will do the same with \n",
    "# the actual model that we built in the .py files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN THE BELOW CELL THAT SETS THE PATH BEFORE RUNNING ANY IMPORTS IN THIS NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_implementation.model_building.machine_translation_model import MachineTranslationModel\n",
    "from model_implementation.utils.constants import ( \n",
    "    DROPOUT_PROB, D_FEED_FORWARD, D_MODEL, MAX_INPUT_SEQUENCE_LENGTH, NUM_HEADS, NUM_LAYERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These sizes depend on our trained tokenizers. For now, I am using these defaults.\n",
    "SRC_VOCAB_SIZE = 32000\n",
    "TGT_VOCAB_SIZE = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identified the following issues:\n",
    "#\n",
    "# 1) This block helped me find the bugs associated with the layer connections in the model. I wasn't copying\n",
    "#    some of the MultiHeadAttention layers properly when passing them to EncoderLayer and DecoderLayer \n",
    "#    classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_model = MachineTranslationModel(d_model=D_MODEL, \n",
    "                                            d_feed_forward=D_FEED_FORWARD,\n",
    "                                            dropout_prob=DROPOUT_PROB, \n",
    "                                            num_heads=NUM_HEADS, \n",
    "                                            src_vocab_size=SRC_VOCAB_SIZE, \n",
    "                                            tgt_vocab_size=TGT_VOCAB_SIZE, \n",
    "                                            num_layers=NUM_LAYERS, \n",
    "                                            max_seq_len=MAX_INPUT_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_params:  111774544\n",
      "total_params_with_grad:  111774544\n",
      "total_params_without_grad:  0\n"
     ]
    }
   ],
   "source": [
    "# Finding out the number of parameters in the build model.\n",
    "total_params = sum(params.numel() for params in translation_model.parameters())\n",
    "print(\"total_params: \", total_params)\n",
    "total_params_with_grad = sum(params.numel() for params in translation_model.parameters() if params.requires_grad)\n",
    "print(\"total_params_with_grad: \", total_params_with_grad)\n",
    "total_params_without_grad = sum(params.numel() for params in translation_model.parameters() if not params.requires_grad)\n",
    "print(\"total_params_without_grad: \", total_params_without_grad)\n",
    "assert total_params == total_params_with_grad + total_params_without_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_embedding.look_up_table.weight   16384000\n",
      "tgt_embedding.look_up_table.weight   25600000\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.0.weight   262144\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.0.bias   512\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.1.weight   262144\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.1.bias   512\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.2.weight   262144\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.2.bias   512\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.3.weight   262144\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.3.bias   512\n",
      "encoder.encoder_layers.0.feed_forward.linear_layer_1.weight   1048576\n",
      "encoder.encoder_layers.0.feed_forward.linear_layer_1.bias   2048\n",
      "encoder.encoder_layers.0.feed_forward.linear_layer_2.weight   1048576\n",
      "encoder.encoder_layers.0.feed_forward.linear_layer_2.bias   512\n",
      "encoder.encoder_layers.0.sublayer_wrappers.0.layer_norm.weight   512\n",
      "encoder.encoder_layers.0.sublayer_wrappers.0.layer_norm.bias   512\n",
      "encoder.encoder_layers.0.sublayer_wrappers.1.layer_norm.weight   512\n",
      "encoder.encoder_layers.0.sublayer_wrappers.1.layer_norm.bias   512\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.0.weight   262144\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.0.bias   512\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.1.weight   262144\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.1.bias   512\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.2.weight   262144\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.2.bias   512\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.3.weight   262144\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.3.bias   512\n",
      "encoder.encoder_layers.1.feed_forward.linear_layer_1.weight   1048576\n",
      "encoder.encoder_layers.1.feed_forward.linear_layer_1.bias   2048\n",
      "encoder.encoder_layers.1.feed_forward.linear_layer_2.weight   1048576\n",
      "encoder.encoder_layers.1.feed_forward.linear_layer_2.bias   512\n",
      "encoder.encoder_layers.1.sublayer_wrappers.0.layer_norm.weight   512\n",
      "encoder.encoder_layers.1.sublayer_wrappers.0.layer_norm.bias   512\n",
      "encoder.encoder_layers.1.sublayer_wrappers.1.layer_norm.weight   512\n",
      "encoder.encoder_layers.1.sublayer_wrappers.1.layer_norm.bias   512\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.0.weight   262144\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.0.bias   512\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.1.weight   262144\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.1.bias   512\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.2.weight   262144\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.2.bias   512\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.3.weight   262144\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.3.bias   512\n",
      "encoder.encoder_layers.2.feed_forward.linear_layer_1.weight   1048576\n",
      "encoder.encoder_layers.2.feed_forward.linear_layer_1.bias   2048\n",
      "encoder.encoder_layers.2.feed_forward.linear_layer_2.weight   1048576\n",
      "encoder.encoder_layers.2.feed_forward.linear_layer_2.bias   512\n",
      "encoder.encoder_layers.2.sublayer_wrappers.0.layer_norm.weight   512\n",
      "encoder.encoder_layers.2.sublayer_wrappers.0.layer_norm.bias   512\n",
      "encoder.encoder_layers.2.sublayer_wrappers.1.layer_norm.weight   512\n",
      "encoder.encoder_layers.2.sublayer_wrappers.1.layer_norm.bias   512\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.0.weight   262144\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.0.bias   512\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.1.weight   262144\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.1.bias   512\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.2.weight   262144\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.2.bias   512\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.3.weight   262144\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.3.bias   512\n",
      "encoder.encoder_layers.3.feed_forward.linear_layer_1.weight   1048576\n",
      "encoder.encoder_layers.3.feed_forward.linear_layer_1.bias   2048\n",
      "encoder.encoder_layers.3.feed_forward.linear_layer_2.weight   1048576\n",
      "encoder.encoder_layers.3.feed_forward.linear_layer_2.bias   512\n",
      "encoder.encoder_layers.3.sublayer_wrappers.0.layer_norm.weight   512\n",
      "encoder.encoder_layers.3.sublayer_wrappers.0.layer_norm.bias   512\n",
      "encoder.encoder_layers.3.sublayer_wrappers.1.layer_norm.weight   512\n",
      "encoder.encoder_layers.3.sublayer_wrappers.1.layer_norm.bias   512\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.0.weight   262144\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.0.bias   512\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.1.weight   262144\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.1.bias   512\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.2.weight   262144\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.2.bias   512\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.3.weight   262144\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.3.bias   512\n",
      "encoder.encoder_layers.4.feed_forward.linear_layer_1.weight   1048576\n",
      "encoder.encoder_layers.4.feed_forward.linear_layer_1.bias   2048\n",
      "encoder.encoder_layers.4.feed_forward.linear_layer_2.weight   1048576\n",
      "encoder.encoder_layers.4.feed_forward.linear_layer_2.bias   512\n",
      "encoder.encoder_layers.4.sublayer_wrappers.0.layer_norm.weight   512\n",
      "encoder.encoder_layers.4.sublayer_wrappers.0.layer_norm.bias   512\n",
      "encoder.encoder_layers.4.sublayer_wrappers.1.layer_norm.weight   512\n",
      "encoder.encoder_layers.4.sublayer_wrappers.1.layer_norm.bias   512\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.0.weight   262144\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.0.bias   512\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.1.weight   262144\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.1.bias   512\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.2.weight   262144\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.2.bias   512\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.3.weight   262144\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.3.bias   512\n",
      "encoder.encoder_layers.5.feed_forward.linear_layer_1.weight   1048576\n",
      "encoder.encoder_layers.5.feed_forward.linear_layer_1.bias   2048\n",
      "encoder.encoder_layers.5.feed_forward.linear_layer_2.weight   1048576\n",
      "encoder.encoder_layers.5.feed_forward.linear_layer_2.bias   512\n",
      "encoder.encoder_layers.5.sublayer_wrappers.0.layer_norm.weight   512\n",
      "encoder.encoder_layers.5.sublayer_wrappers.0.layer_norm.bias   512\n",
      "encoder.encoder_layers.5.sublayer_wrappers.1.layer_norm.weight   512\n",
      "encoder.encoder_layers.5.sublayer_wrappers.1.layer_norm.bias   512\n",
      "encoder.layer_norm.weight   512\n",
      "encoder.layer_norm.bias   512\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.0.weight   262144\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.0.bias   512\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.1.weight   262144\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.1.bias   512\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.2.weight   262144\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.2.bias   512\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.3.weight   262144\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.3.bias   512\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.0.weight   262144\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.0.bias   512\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.1.weight   262144\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.1.bias   512\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.2.weight   262144\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.2.bias   512\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.3.weight   262144\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.3.bias   512\n",
      "decoder.decoder_layers.0.feed_forward.linear_layer_1.weight   1048576\n",
      "decoder.decoder_layers.0.feed_forward.linear_layer_1.bias   2048\n",
      "decoder.decoder_layers.0.feed_forward.linear_layer_2.weight   1048576\n",
      "decoder.decoder_layers.0.feed_forward.linear_layer_2.bias   512\n",
      "decoder.decoder_layers.0.sublayer_wrappers.0.layer_norm.weight   512\n",
      "decoder.decoder_layers.0.sublayer_wrappers.0.layer_norm.bias   512\n",
      "decoder.decoder_layers.0.sublayer_wrappers.1.layer_norm.weight   512\n",
      "decoder.decoder_layers.0.sublayer_wrappers.1.layer_norm.bias   512\n",
      "decoder.decoder_layers.0.sublayer_wrappers.2.layer_norm.weight   512\n",
      "decoder.decoder_layers.0.sublayer_wrappers.2.layer_norm.bias   512\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.0.weight   262144\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.0.bias   512\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.1.weight   262144\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.1.bias   512\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.2.weight   262144\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.2.bias   512\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.3.weight   262144\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.3.bias   512\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.0.weight   262144\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.0.bias   512\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.1.weight   262144\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.1.bias   512\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.2.weight   262144\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.2.bias   512\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.3.weight   262144\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.3.bias   512\n",
      "decoder.decoder_layers.1.feed_forward.linear_layer_1.weight   1048576\n",
      "decoder.decoder_layers.1.feed_forward.linear_layer_1.bias   2048\n",
      "decoder.decoder_layers.1.feed_forward.linear_layer_2.weight   1048576\n",
      "decoder.decoder_layers.1.feed_forward.linear_layer_2.bias   512\n",
      "decoder.decoder_layers.1.sublayer_wrappers.0.layer_norm.weight   512\n",
      "decoder.decoder_layers.1.sublayer_wrappers.0.layer_norm.bias   512\n",
      "decoder.decoder_layers.1.sublayer_wrappers.1.layer_norm.weight   512\n",
      "decoder.decoder_layers.1.sublayer_wrappers.1.layer_norm.bias   512\n",
      "decoder.decoder_layers.1.sublayer_wrappers.2.layer_norm.weight   512\n",
      "decoder.decoder_layers.1.sublayer_wrappers.2.layer_norm.bias   512\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.0.weight   262144\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.0.bias   512\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.1.weight   262144\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.1.bias   512\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.2.weight   262144\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.2.bias   512\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.3.weight   262144\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.3.bias   512\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.0.weight   262144\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.0.bias   512\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.1.weight   262144\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.1.bias   512\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.2.weight   262144\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.2.bias   512\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.3.weight   262144\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.3.bias   512\n",
      "decoder.decoder_layers.2.feed_forward.linear_layer_1.weight   1048576\n",
      "decoder.decoder_layers.2.feed_forward.linear_layer_1.bias   2048\n",
      "decoder.decoder_layers.2.feed_forward.linear_layer_2.weight   1048576\n",
      "decoder.decoder_layers.2.feed_forward.linear_layer_2.bias   512\n",
      "decoder.decoder_layers.2.sublayer_wrappers.0.layer_norm.weight   512\n",
      "decoder.decoder_layers.2.sublayer_wrappers.0.layer_norm.bias   512\n",
      "decoder.decoder_layers.2.sublayer_wrappers.1.layer_norm.weight   512\n",
      "decoder.decoder_layers.2.sublayer_wrappers.1.layer_norm.bias   512\n",
      "decoder.decoder_layers.2.sublayer_wrappers.2.layer_norm.weight   512\n",
      "decoder.decoder_layers.2.sublayer_wrappers.2.layer_norm.bias   512\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.0.weight   262144\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.0.bias   512\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.1.weight   262144\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.1.bias   512\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.2.weight   262144\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.2.bias   512\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.3.weight   262144\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.3.bias   512\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.0.weight   262144\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.0.bias   512\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.1.weight   262144\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.1.bias   512\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.2.weight   262144\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.2.bias   512\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.3.weight   262144\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.3.bias   512\n",
      "decoder.decoder_layers.3.feed_forward.linear_layer_1.weight   1048576\n",
      "decoder.decoder_layers.3.feed_forward.linear_layer_1.bias   2048\n",
      "decoder.decoder_layers.3.feed_forward.linear_layer_2.weight   1048576\n",
      "decoder.decoder_layers.3.feed_forward.linear_layer_2.bias   512\n",
      "decoder.decoder_layers.3.sublayer_wrappers.0.layer_norm.weight   512\n",
      "decoder.decoder_layers.3.sublayer_wrappers.0.layer_norm.bias   512\n",
      "decoder.decoder_layers.3.sublayer_wrappers.1.layer_norm.weight   512\n",
      "decoder.decoder_layers.3.sublayer_wrappers.1.layer_norm.bias   512\n",
      "decoder.decoder_layers.3.sublayer_wrappers.2.layer_norm.weight   512\n",
      "decoder.decoder_layers.3.sublayer_wrappers.2.layer_norm.bias   512\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.0.weight   262144\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.0.bias   512\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.1.weight   262144\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.1.bias   512\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.2.weight   262144\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.2.bias   512\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.3.weight   262144\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.3.bias   512\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.0.weight   262144\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.0.bias   512\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.1.weight   262144\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.1.bias   512\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.2.weight   262144\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.2.bias   512\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.3.weight   262144\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.3.bias   512\n",
      "decoder.decoder_layers.4.feed_forward.linear_layer_1.weight   1048576\n",
      "decoder.decoder_layers.4.feed_forward.linear_layer_1.bias   2048\n",
      "decoder.decoder_layers.4.feed_forward.linear_layer_2.weight   1048576\n",
      "decoder.decoder_layers.4.feed_forward.linear_layer_2.bias   512\n",
      "decoder.decoder_layers.4.sublayer_wrappers.0.layer_norm.weight   512\n",
      "decoder.decoder_layers.4.sublayer_wrappers.0.layer_norm.bias   512\n",
      "decoder.decoder_layers.4.sublayer_wrappers.1.layer_norm.weight   512\n",
      "decoder.decoder_layers.4.sublayer_wrappers.1.layer_norm.bias   512\n",
      "decoder.decoder_layers.4.sublayer_wrappers.2.layer_norm.weight   512\n",
      "decoder.decoder_layers.4.sublayer_wrappers.2.layer_norm.bias   512\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.0.weight   262144\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.0.bias   512\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.1.weight   262144\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.1.bias   512\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.2.weight   262144\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.2.bias   512\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.3.weight   262144\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.3.bias   512\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.0.weight   262144\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.0.bias   512\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.1.weight   262144\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.1.bias   512\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.2.weight   262144\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.2.bias   512\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.3.weight   262144\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.3.bias   512\n",
      "decoder.decoder_layers.5.feed_forward.linear_layer_1.weight   1048576\n",
      "decoder.decoder_layers.5.feed_forward.linear_layer_1.bias   2048\n",
      "decoder.decoder_layers.5.feed_forward.linear_layer_2.weight   1048576\n",
      "decoder.decoder_layers.5.feed_forward.linear_layer_2.bias   512\n",
      "decoder.decoder_layers.5.sublayer_wrappers.0.layer_norm.weight   512\n",
      "decoder.decoder_layers.5.sublayer_wrappers.0.layer_norm.bias   512\n",
      "decoder.decoder_layers.5.sublayer_wrappers.1.layer_norm.weight   512\n",
      "decoder.decoder_layers.5.sublayer_wrappers.1.layer_norm.bias   512\n",
      "decoder.decoder_layers.5.sublayer_wrappers.2.layer_norm.weight   512\n",
      "decoder.decoder_layers.5.sublayer_wrappers.2.layer_norm.bias   512\n",
      "decoder.layer_norm.weight   512\n",
      "decoder.layer_norm.bias   512\n",
      "token_predictor.linear.weight   25600000\n",
      "token_predictor.linear.bias   50000\n"
     ]
    }
   ],
   "source": [
    "# Prints out all the layers and the number of parameters in each layer.\n",
    "for name, params in translation_model.named_parameters():\n",
    "    print(name, \" \", params.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of parameters associated with the model:  111774544\n"
     ]
    }
   ],
   "source": [
    "# This cell is the same as the one used in 'step_15_machine_translation_model.ipynb' notebook (LINK TO THE \n",
    "# NOTEBOOK). \n",
    "#\n",
    "# Lets try to count the number of parameters in the model manually by going through each component and counting \n",
    "# parameters. \n",
    "# The number of parameters associated with the Embeddings class for src sentences. We have 1 embedding vector \n",
    "# per token in the source vocabulary. We have 40000 tokens and each token is represented by an 512-dimensional vector. \n",
    "# So, the total number of parameters associated with the Embeddings class for src sentences is 40000 * 512 = 20480000.\n",
    "num_src_embedding_params = SRC_VOCAB_SIZE * D_MODEL\n",
    "# The number of parameters associated with the Embeddings class for tgt sentences. We have 1 embedding vector\n",
    "# per token in the target vocabulary. We have 40000 tokens and each token is represented by an 512-dimensional vector.\n",
    "# So, the total number of parameters associated with the Embeddings class for tgt sentences is 40000 * 512 = 20480000.\n",
    "num_tgt_embedding_params = TGT_VOCAB_SIZE * D_MODEL\n",
    "# There are no parameters associated with the PositionalEncoding class. These are calculated based on a predefined\n",
    "# formula and are not learned during the training process.\n",
    "num_positional_encoding_params = 0\n",
    "# Now, lets calculate the number of parameters associated with the Encoder class. The 'Encoder' class has 6 \n",
    "# identical EncoderLayers stacked on top of each other. Lets calculate the number of parameters associated with \n",
    "# each 'EncoderLayer' class. The EncoderLayer class has MultiHeadedAttention and FeedForwardNN classes as its child \n",
    "# classes. Each MultiHeadedAttention class has 4 linear layers (query, key, value and output). Note that a single \n",
    "# linear layer is used to calculate the queries, keys, values and outputs for all the heads. So, we don't need to \n",
    "# do this calculation for each head separately. Lets take the linear layer associated with the query calculation. \n",
    "# The input to this linear layer is a 512-dimensional vector (d_model) and the output is also an 512-dimensional vector \n",
    "# (d_model=512). So, the number of parameters in this linear layer associated with the weight matrix is \n",
    "# 512 * 512 = 262144. We also have bias terms (d_model=512) associated with this linear layer. So, the total number \n",
    "# of parameters associated with the query linear layer is 262144 + 512 = 262656.\n",
    "num_encoder_query_params = D_MODEL * D_MODEL + D_MODEL\n",
    "num_encoder_key_params = D_MODEL * D_MODEL + D_MODEL\n",
    "num_encoder_value_params = D_MODEL * D_MODEL + D_MODEL\n",
    "num_encoder_attention_output_params = D_MODEL * D_MODEL + D_MODEL\n",
    "# Now lets calculate the number of parameters associated with FeedForward neural network class in the EncoderLayer. \n",
    "# The first linear layer in the feed forward expands the input to a higher dimension (d_model to d_feed_forward). \n",
    "# The input to this linear layer is a 512-dimensional vector (d_model) and the output is a 2048-dimensional vector \n",
    "# (d_feed_forward). So, the number of parameters in this linear layer associated with the weight matrix is \n",
    "# 512 * 2048 = 1048576. We also have bias terms (d_feed_forward=2048) associated with this linear layer. So, the \n",
    "# total number of parameters associated with the first linear layer in the feed forward neural network is \n",
    "# 1048576 + 2048 = 1050624. The second linear layer in the feed forward neural network compresses the input back \n",
    "# to its original dimension (d_feed_forward to d_model). The input to this linear layer is a 2048-dimensional \n",
    "# vector (d_feed_forward) and the output is an 512-dimensional vector (d_model). So, the number of parameters in \n",
    "# this linear layer associated with the weight matrix is 2048 * 512 = 1048576. We also have bias terms (d_model=512) \n",
    "# associated with this linear layer. So, the total number of parameters associated with the second linear layer in \n",
    "# the feed forward neural network is 1048576 + 512 = 1049088.\n",
    "num_encoder_feed_forward_linear_layer_1_params = D_MODEL * D_FEED_FORWARD + D_FEED_FORWARD\n",
    "num_encoder_feed_forward_linear_layer_2_params = D_FEED_FORWARD * D_MODEL + D_MODEL\n",
    "# The output of MultiHeadedAttention and FeedForward neural network is normalized using Layer Normalization. Layer\n",
    "# Normalization is applied along the last dimension of the input tensor (input to Layer Normalization). Each of the\n",
    "# features is scaled independently with the learned paramaters. So, the number of parameters is the number of \n",
    "# features in the last dimension multiplied by 2 (1 parameter for gamma and 1 parameter for beta per feature). Both \n",
    "# the output of MultiHeadedAttention and FeedForward neural network have the same size in the last dimension which \n",
    "# is 512 (d_model). So, the number of parameters associated with Layer Normalization layer that is applied after \n",
    "# MultiHeadedAttention is 512 (gamma) + 512 (beta) = 1024. Similarly, the number of parameters associated with Layer\n",
    "# Normalization layer that is applied after FeedForward neural network is 512 (gamma) + 512 (beta) = 1024.\n",
    "num_encoder_attention_layer_norm_params = D_MODEL + D_MODEL\n",
    "num_encoder_feed_forward_layer_norm_params = D_MODEL + D_MODEL\n",
    "# The total number of parameters associated with a single EncoderLayer is sum of the above 8 variables = 3152384.\n",
    "num_encoder_layer_params = num_encoder_query_params + num_encoder_key_params + num_encoder_value_params + num_encoder_attention_output_params + num_encoder_feed_forward_linear_layer_1_params + num_encoder_feed_forward_linear_layer_2_params + num_encoder_attention_layer_norm_params + num_encoder_feed_forward_layer_norm_params\n",
    "# We also apply Layer Normalization to the output of the last EncoderLayer and pass this as the output of the Encoder.\n",
    "# This layer again has same number of parameters associated with it as other Layer Normalization layers i.e.,\n",
    "# 512 (gamma) + 512 (beta) = 1024\n",
    "num_encoder_layer_layer_norm_params = D_MODEL + D_MODEL\n",
    "# The transformer model has 6 EncoderLayers stacked on top of each other. So, the total number of parameters \n",
    "# associated with Encoder is (6 * 3152384) + 1024 = 18915328\n",
    "num_total_encoder_params = (NUM_LAYERS * num_encoder_layer_params) + num_encoder_layer_layer_norm_params\n",
    "# Now, lets calculate the number of parameters associated with the DecoderLayer and the Decoder.\n",
    "# The method to calculate number of parameters in the DecoderLayer is very similar to how it was done for the \n",
    "# EncoderLayer. DecoderLayer just contains 1 additional MultiHeadedAttention Layer (for source attention) and 1\n",
    "# additional Layer Normalization layer associated with this source attention layer.\n",
    "# Same as in EncoderLayer ==> Linear Layer parameters ==> 262144 (weights) + 512 (bias) = 262656\n",
    "num_decoder_self_attention_query_params = D_MODEL * D_MODEL + D_MODEL\n",
    "num_decoder_self_attention_key_params = D_MODEL * D_MODEL + D_MODEL\n",
    "num_decoder_self_attention_value_params = D_MODEL * D_MODEL + D_MODEL\n",
    "num_decoder_self_attention_output_params = D_MODEL * D_MODEL + D_MODEL\n",
    "# The next 4 variables correspond to the 1 additional MultiHeadedAttention layer (src attention) present in the \n",
    "# DecoderLayer.\n",
    "# Same as in EncoderLayer ==> Linear Layer parameters ==> 262144 (weights) + 512 (bias) = 262656\n",
    "num_decoder_src_attention_query_params = D_MODEL * D_MODEL + D_MODEL\n",
    "num_decoder_src_attention_key_params = D_MODEL * D_MODEL + D_MODEL\n",
    "num_decoder_src_attention_value_params = D_MODEL * D_MODEL + D_MODEL\n",
    "num_decoder_src_attention_output_params = D_MODEL * D_MODEL + D_MODEL\n",
    "# The FeedForward neural network is exactly the same as in EncoderLayer.\n",
    "# Same as in EncoderLayer ==> 512 * 2048 + 2048 = 1050624\n",
    "num_decoder_feed_forward_linear_layer_1_params = D_MODEL * D_FEED_FORWARD + D_FEED_FORWARD\n",
    "# Same as in EncoderLayer ==> 2048 * 512 + 512 = 1049088\n",
    "num_decoder_feed_forward_linear_layer_2_params = D_MODEL * D_FEED_FORWARD + D_MODEL\n",
    "# We have 1 additional LayerNormalization layer associated with the source attention. However, its architecture and\n",
    "# the parameters are the same as in EncoderLayer.\n",
    "# Same as in EncoderLayer ==> 512 (gamma) + 512 (beta) = 1024\n",
    "num_decoder_self_attention_layer_norm_params = D_MODEL + D_MODEL\n",
    "num_decoder_src_attention_layer_norm_params = D_MODEL + D_MODEL\n",
    "num_decoder_feed_forward_layer_norm_params = D_MODEL + D_MODEL\n",
    "# The total number of parameters associated with a single DecoderLayer is the sum of the above 13 variables = 4204032.\n",
    "num_decoder_layer_params = num_decoder_self_attention_query_params + num_decoder_self_attention_key_params + num_decoder_self_attention_value_params + num_decoder_self_attention_output_params + num_decoder_src_attention_query_params + num_decoder_src_attention_key_params + num_decoder_src_attention_value_params + num_decoder_src_attention_output_params + num_decoder_feed_forward_linear_layer_1_params + num_decoder_feed_forward_linear_layer_2_params + num_decoder_self_attention_layer_norm_params + num_decoder_src_attention_layer_norm_params + num_decoder_feed_forward_layer_norm_params\n",
    "# We also apply Layer Normalization to the output of the last DecoderLayer and pass this as the output of the Decoder. \n",
    "# This layer again has same number of parameters associated with it as other Layer Normalization layers i.e.,\n",
    "# 512 (gamma) + 512 (beta) = 1024\n",
    "num_decoder_layer_layer_norm_params = D_MODEL + D_MODEL\n",
    "# The transformer model has 6 EncoderLayers stacked on top of each other. So, the total number of parameters \n",
    "# associated with Encoder is (6 * 4204032) + 1024 = 25225216.\n",
    "num_total_decoder_params = (NUM_LAYERS * num_decoder_layer_params) + num_decoder_layer_layer_norm_params\n",
    "# The output of the decoder is passed to a linear layer that projects the output to the target vocabulary space.\n",
    "# These parameters are associated with the 'TokenPredictor' layer in the transformer. The input to the linear\n",
    "# layer are 512-dimensional vectors (d_model) and output of the linear layers are 40000-dimensional vectors (tgt_vocab_size).\n",
    "# So, the number of parameters associated with the TokenPredictor layer is 512 * 40000 (weights) + 40000 (bias) = 20520000.\n",
    "num_vocab_projection_params = D_MODEL * TGT_VOCAB_SIZE + TGT_VOCAB_SIZE\n",
    "# Finally, the total number of parameters in the model is the number of parameters associated with the Embeddings plus\n",
    "# the number of parameters in the Encoder plus the number of parameters in the Decoder plus the number of parameters\n",
    "# in the TokenPredictor.\n",
    "num_total_model_params = num_src_embedding_params + num_tgt_embedding_params + num_total_encoder_params + num_total_decoder_params + num_vocab_projection_params\n",
    "print(\"Total Number of parameters associated with the model: \", num_total_model_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the number of trainable parameters calculated manually is the same as the number of trainable parameters\n",
    "# calculated by PyTorch.\n",
    "assert total_params_with_grad == num_total_model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data from disk and creating DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block helped me identify the following issues:\n",
    "#\n",
    "# 1) The tokenizer and the vocabulary created in the 'step_2_training_bpe_tokenizer.ipynb' notebook has '<pad>' \n",
    "#    token mapped to token id 3. However, it is supposed to be mapped to 2 in the model created in these '.py' \n",
    "#    files.\n",
    "# 2) I set the limit of MAX_SEQUENCE_LENGTH to 150 in the 'constants.py' file. However, by running the \n",
    "#    data loader creation here, I realized the batch lengths are exceeding 150 which would have created issues\n",
    "#    with Positional Encoding. I have later increased the MAX_SEQUENCE_LENGTH to 200 in the 'constants.py' \n",
    "#    file. I also fixed this issue by skipping the batches with length greater than MAX_SEQUENCE_LENGTH in the\n",
    "#    in the 'model_trainer.py' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_implementation.data_processing.data_preparation.dataset_wrapper import DatasetWrapper\n",
    "from model_implementation.data_processing.data_preparation.data_helpers import get_tokenizers, load_data_from_disk\n",
    "from model_implementation.data_processing.data_preparation.data_loader import create_data_loader\n",
    "from model_implementation.utils.constants import BATCH_SIZE, DEBUG_DATASET_PATH, FULL_EN_TE_DATASET_PATH\n",
    "\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train dataset from disk.\n",
    "train_dataset: datasets.arrow_dataset.Dataset = load_data_from_disk(dataset_relative_path=DEBUG_DATASET_PATH)\n",
    "# Wrap the hugging face dataset in a pytorch Dataset to be able to use with pytorch DataLoader.\n",
    "translation_dataset = DatasetWrapper(hf_dataset=train_dataset)\n",
    "# Get the tokenizers for the English and Telugu languages.\n",
    "english_tokenizer, telugu_tokenizer = get_tokenizers(dataset_relative_path=FULL_EN_TE_DATASET_PATH, tokenizer_type=\"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_sentence:  Have you heard about Foie gras?\n",
      "tgt_sentence:  ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  I never thought of acting in films.\n",
      "tgt_sentence:  సూర్య సినిమాల్లో నటించాలని ఎప్పుడూ అనుకోలేదు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Installed Software\n",
      "tgt_sentence:  స్థాపించబడిన సాఫ్ట్‍వేర్\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  A case has been registered under Sections 302 and 376, IPC.\n",
      "tgt_sentence:  నిందితులపై సెక్షన్ 376 మరియు 302ల కింద కేసు నమోదు చేశాం.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Of this, 10 people succumbed to the injuries.\n",
      "tgt_sentence:  అందులో 10 మంది తీవ్రంగా గాయపడ్డారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Her acting has been praised by critics.\n",
      "tgt_sentence:  నటనకు గాను విమర్శకుల నుంచి ప్రశంసలు పొందింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The Bibles viewpoint on this is clearly indicated at Colossians 3: 9: Do not be lying to one another.\n",
      "tgt_sentence:  ఈ విషయంపై బైబిలు దృక్కోణం కొలొస్సయులు 3 :⁠ 9లో “ఒకనితో ఒకడు అబద్ధమాడకుడి ” అని స్పష్టంగా సూచించబడింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The incident was recorded in the CCTV footage.\n",
      "tgt_sentence:  ఈ ప్రమాద దృశ్యాలు సీసీటీవీ ఫుటేజ్‌లో రికార్డ్ అయ్యాయి.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Respect privacy\n",
      "tgt_sentence:  గోప్యత పాటించండి\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  5 lakh would be provided.\n",
      "tgt_sentence:  5లక్షలు సాయం అందజేశారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Super Bowl.\n",
      "tgt_sentence:  \"\"\"సూపర్ బౌల్.\"\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Education institutions are closed across the country in the wake of lockdown due to coronavirus outbreak.\n",
      "tgt_sentence:  కరోనా వైరస్ లాక్ డౌన్ కారణంగా అంతటా విద్యాసంస్థలు మూసివేశారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The smartphone was recently launched in Indonesia.\n",
      "tgt_sentence:  ఈ స్మార్ట్ ఫోన్ ఇప్పటికే ఇండోనేషియాలో లాంచ్ అయింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Australian batsman David Warner.\n",
      "tgt_sentence:  ఆస్ట్రేలియా స్టార్ ఆటగాడు డేవిడ్ వార్నర్.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Woman killed in stampede\n",
      "tgt_sentence:  తూ. గో. లో పిడుగుపాటుకు మహిళ మృతి\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  On the second day of Navratri, Maa Brahmacharini is worshipped.\n",
      "tgt_sentence:  నవరాత్రులలో రెండవ రోజున ఈమెను పూజిస్తారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The movie also stars Kajal Agarwal in a prominent role.\n",
      "tgt_sentence:  కాజల్‌ అగర్వాల్‌ కూడా ఇందులో ఓ పాత్ర చేస్తున్నట్లు చెబుతున్నారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  It cannot work.\n",
      "tgt_sentence:  ఏ పనిచేయలేరు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Key inflammatory markers IL-6, TNF etc., were presented to have reduced significantly with the drug thereby preventing hyper- inflammation in COVID-19 patients.\n",
      "tgt_sentence:  కీల‌క‌ ఇన్ఫ్లమేటరీ మార్కర్స్ ఐఎల్‌-6, టీఎన్ఏ మొదలైనవి ఈ ఔషధం వాడ‌కంతో గణనీయంగా తగ్గినట్లు తెలియ‌జేయ‌డ‌మైంది. తద్వారా కోవిడ్-19 రోగులలో హైపర్-ఇన్ఫ్లమేషన్‌ నివారించవచ్చ‌ని తెలిపింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Salman Khan will be essaying the role of circus artist in the film.\n",
      "tgt_sentence:  ఈ చిత్రంలో సల్మాన్‌ ఖాన్ సర్కస్‌ ఆర్టిస్టుగా నటిస్తుండటం విశేషం.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  A few days ag...\n",
      "tgt_sentence:  కొద్దిరోజులు గడిచాయ్ .\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  He was immediately rushed to a nearby hospital where doctors declared him dead, the official said.\n",
      "tgt_sentence:  అతన్ని వెంటనే సమీపంలోని ఆసుపత్రికి తరలించినా అప్పటికే అతను మృతి చెందినట్లు వైద్యలు తెలిపారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Unlike in Gujarat State, alcohol is legal in Diu.\n",
      "tgt_sentence:  గుజరాత్ రాష్ట్రంలో కాకుండా, డయులో మద్యం చట్టబద్ధమైనది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  He had lost 2014 election.\n",
      "tgt_sentence:  ఆ తరువాత 2014 ఎన్నికలకు కూడా ఆయన దూరంగా ఉన్నారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Both the teams are yet to win the IPL trophy.\n",
      "tgt_sentence:  ఇప్పటివరకు ఐపీఎల్‌ టైటిల్‌ అందుకోని ఈ రెండు జట్లు విజయమే లక్ష్యంగా బరిలోకి దిగుతున్నాయి.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Rs 6.02 crore.\n",
      "tgt_sentence:  02 లక్షల వరకు తగ్గించింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  It had to be written.\n",
      "tgt_sentence:  విధాత వ్రాయవలసినదే.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  He is just one...\n",
      "tgt_sentence:  అతనొక్కడే .\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Exams also postponed.\n",
      "tgt_sentence:  పరీక్షలు సైతం రద్దయ్యాయి.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The deceased has four children and a wife.\n",
      "tgt_sentence:  మృతునికి భార్య, నాలుగు నెలల వయసుకల కుమారుడు ఉన్నారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Chief Minister YS Jagan Mohan Reddy clearly stated that the building was constructed in blatant violation of all laws and regulations, hence it should be demolished.\n",
      "tgt_sentence:  ఇది నిబంధ‌న‌ల‌కు విరుద్ధంగా నిర్మించిన భ‌వ‌న‌మ‌నీ, అక్ర‌మ క‌ట్ట‌డాల తొల‌గింపు ఇక్క‌డి నుంచీ ప్రారంభం అవుతుందంటూ ముఖ్య‌మంత్రి జ‌గ‌న్మోహ‌న్ రెడ్డి చెప్ప‌డం, ఆయ‌న ఆదేశాల‌కు అనుగుణంగా కూల్చేయ‌డం కూడా జ‌రిగిపోయింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Man gets death sentence for raping, killing 9-yr-old girl\n",
      "tgt_sentence:  9 నెలల చిన్నారిపై లైంగిక దాడి, హత్య కేసులో సంచలన తీర్పు నిందితుడికి ఉరి శిక్ష\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Will you enter politics?\n",
      "tgt_sentence:  నయనతార రాజకీయాల్లోకి అడుగుపెట్టనున్నారా?\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  For I received from the Lord that which also I delivered to you, that the Lord Jesus on the night in which he was betrayed took bread.\n",
      "tgt_sentence:  నేను మీకు అప్పగించిన దానిని ప్రభువువలన పొందితిని. ప్రభువైన యేసు తాను అప్పగింప బడిన రాత్రి యొక రొట్టెను ఎత్తికొని కృతజ్ఞ తాస్తుతులు చెల్లించి\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  \"In the upcoming elections, the BJP is going to get a majority,\"\" added Yeddyurappa\"\n",
      "tgt_sentence:  వచ్చే సాధారణ ఎన్నికల్లో బీజేపీ తిరిగి అధికార పగ్గాలు చేపడుతుందని అన్నారు\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The children were terrified.\n",
      "tgt_sentence:  పిల్లలు తీవ్ర భయాందోళనకు గురయ్యారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Starring Amitabh Bachchan and Ayushmann Khurana, Gulabo Sitabo is directed by Shoojit Sircar.\n",
      "tgt_sentence:  అమితాబ్ బచ్చన్, ఆయుష్మాన్ ఖురానా ప్రధాన పాత్రల్లో సుజిత్ సర్కార్ దర్శకత్వంలో తెరకెక్కిన గులాబో సితాబో సినిమాను సన్ ఫిల్మ్స్ ప్రొడక్షన్ నిర్మించింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Tirumala: Tollywood director Koratala Siva offered prayers at Sri Venkateswara Swami temple in Tirumala on Thursday\n",
      "tgt_sentence:  తిరుమల: తిరుమలలోని శ్రీవారి ఆలయంలో మంగళవారం కోయిల్ ఆళ్వార్ తిరుమంజనం నిర్వహించనున్నారు\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The government has no plans of making taxpayers deposit 18 per cent of their income\n",
      "tgt_sentence:  పన్ను చెల్లింపుదారులు తప్పనిసరిగా 18శాతం డబ్బును డిపాజిట్ చేయాలంటూ ప్రభుత్వం ఎలాంటి ఆదేశాలు జారీ చేయలేదు\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Abrogation of Article 370, that scrapped Kashmirs special status, is a new sticking point.\n",
      "tgt_sentence:  , కాశ్మీర్‌కు ప్రత్యేక ప్రతిపత్తి కల్పిస్తున్న 370 అధికరణను రద్దు చేసింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  India getting close!\n",
      "tgt_sentence:  భారత్ దూసుకుపోతోంది!\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The meeting with PAAS leaders was attended by Gujarat Congress chief Bharatsinh Solanki and senior Congress leaders Siddharth Patel and Babubhai Mangukia.\n",
      "tgt_sentence:  ఈ చర్చల్లో కాంగ్రెస్ పార్టీ గుజరాత్ పీసీసీ అధ్యక్షుడు భరత్ సింగ్ సోలంకి, సీనియర్ నేతలు సిద్ధార్థ్ పటేల్, బాబూభాయి మంగుకియా తదితరులు పాల్గొన్నారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  I have never sulked.\n",
      "tgt_sentence:  నేనెప్పుడూ చీట్ చేయలేదు\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Nivetha Thomas and Aditi Rao Hydari are playing the heroines in the film.\n",
      "tgt_sentence:  నాని, సుధీర్ బాబు ప్రధాన పాత్రల్లో నటిస్తోన్న ఈ చిత్రంలో నివేదా థామస్, అదితీరావు హైదరి హీరోయిన్లుగా నటిస్తున్నారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  This is on record.\n",
      "tgt_sentence:  ఈ రికార్డ్.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Pawan Kalyan broke the silence\n",
      "tgt_sentence:  షూటింగ్ కి బ్రేక్ ఇచ్చిన పవన్ కళ్యాణ్\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The tweet is going viral on social media.\n",
      "tgt_sentence:  ఆ ట్వీట్ సోషల్ మీడియాలో వైరల్ అవుతుంది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Have you, though, heard people claim that science can tell us all about life? German weekly Die Woche points out: As true as science is, it is spiritually weak.\n",
      "tgt_sentence:  అలా తమ జీవితాలను విషాదకరంగా అంతం చేసుకున్నవారి గురించి, ఒంటరితనంతో బాధపడుతున్నవారి గురించి మీకు తెలిసి ఉండవచ్చు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  They all get along well with each other.\n",
      "tgt_sentence:  వీరంతా ఒకరికొకరు చాలా బాగా సాయం చేసుకుంటారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  One person was killed in the accident and four were injured.\n",
      "tgt_sentence:  ఈ ఘర్షణలో ఒకరు మరణించగా నలుగురికి తీవ్ర గాయాలయ్యాయి.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Cheese also contains calcium and vitamin D, important minerals for bone strength.\n",
      "tgt_sentence:  అలాగే రాగుల్లో క్యాల్షియంతో పాటు విటమిన్‌ ‘డి’ కూడా ఎక్కువగా ఉండటం వల్ల ఎముకలు బలంగా అవుతాయ్​.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The new Toyota Fortuner is based on the company's modular Toyota New Generation Architecture (TNGA) platform\n",
      "tgt_sentence:  టయోటా నూతన డిజైన్ పరిజ్ఞానం టయోటా న్యూ జనరేషన్ ఆర్కిటెక్చర్ (టిఎన్జిఎ) ఆధారంగా ఈ సరికొత్త ఫార్చ్యూనర్ రూపొందించబడింది\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The phone will have just single camera unit.\n",
      "tgt_sentence:  ఫోన్ మాత్రమే ఒక కెమెరా.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  This will control blood pressure.\n",
      "tgt_sentence:  వీటి ద్వారా బ్లడ్‌ ప్రెషర్‌ కంట్రోల్‌లో ఉంటుంది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  He told a different story.\n",
      "tgt_sentence:  తనకు పూర్తిగా వేరే కథను వివరించారని అన్నారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  In this temple Lord Murugan is worshipped in the form of Lord Palani Murugan\n",
      "tgt_sentence:  ఈ ఆలయం లో స్వామి పలని మురుగన్ రూపం లో మురుగన్ స్వామి పూజలందుకుంటారు\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The IAF has been pressing the government to expedite acquisition of combat jets to overcome the shortage.\n",
      "tgt_sentence:  ఈ లోపాన్ని అధిగమించేందుకు తక్షణ ప్రాతిపదికన యుద్ధ విమానాలను సమకూర్చుకోవాల్సిన అవసరం ఉందని, దీర్ఘకాలంగా ప్రభుత్వంపై ఐఏఎఫ్ ఒత్తిడి తెస్తూనే ఉంది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  We had informed the court about this.\n",
      "tgt_sentence:  ఆ విషయం తెలిసి మేం కోర్టుకి వెళ్లాం.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Doctors, too, are delighted.\n",
      "tgt_sentence:  వైద్యులు సంతోషం వ్యక్తం చేస్తున్నారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Mohan Babu, Chiranjeevi and T Subbarami Reddy among others attended the event.\n",
      "tgt_sentence:  ఈ కార్యక్రమానికి చిరంజీవి, మోహన్ బాబు, మురళి మోహన్, టి సుబ్బిరామిరెడ్డి అతిథులుగా హాజరయ్యారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  There are many ways of answering this.\n",
      "tgt_sentence:  మీరు స్పందించడానికి చాలా రకాలుగా ఈ ప్రశ్నపై.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Diarrhea and vomiting.\n",
      "tgt_sentence:  వాంతులు, విరోచనలు చేశాయి.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  In fact, a Thracian wife saw it as an honor to be sacrificed and buried alongside her husband!\n",
      "tgt_sentence:  వాస్తవానికి త్రాసియన్‌ తెగకు చెందిన భార్య, బలి అర్పించబడి తన భర్తతోపాటు పాతిపెట్టబడడం ఒక గౌరవంగా భావించేది!\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  I was not living in India at that time.\n",
      "tgt_sentence:  ఆ రోజు నేను ఇండియాలో ఉండటం లేదు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  This incident has shocked the state.\n",
      "tgt_sentence:  ఈ సంఘటన రాష్ట్ర ప్రజలను నివ్వెరపరిచింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Geographical indications in the name of Bhutias' last names are common.\n",
      "tgt_sentence:  భూటియాసు చివరి పేర్ల పేరిట భౌగోళిక సూచనలు సాధారణం.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The smartphone will have a 23-megapixel rear camera and dual 16-megapixel sensors on the front.\n",
      "tgt_sentence:  23 మెగా పిక్సల్ రేర్ ఫేసింగ్ కెమెరాతో పాటు 16 మెగా పిక్సల్ ఫ్రంట్ ఫేసింగ్ కెమెరాలను కూడా ఫోన్‌లో అమర్చనున్నారట.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Green tea is a powerful antioxidant.\n",
      "tgt_sentence:  గ్రీన్ టీలో పవర్ ఫుల్ యాంటీఆక్సిడెంట్స్ ఉంటాయి .\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  But no special effort is necessary for this.\n",
      "tgt_sentence:  కానీ ఈ కోసం ప్రత్యేకంగా ఇక్కడ వెళ్ళడానికి అవసరం లేదు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  He has been an MLA four times and MP once.\n",
      "tgt_sentence:  తన రాజకీయ ప్రస్థానంలో ఐదుసార్లు ఎంపీ, నాలుగు సార్లు ఎమ్మెల్యేగా ఎన్నికయ్యారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Locals have demanded strict action against the encroachers.\n",
      "tgt_sentence:  నిర్వాహాకులపై కఠిన చర్యలు తీసుకోవాలని స్థానికులు డిమాండ్ చేస్తున్నారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Police has arrested two suspects\n",
      "tgt_sentence:  నిందితుడిని పోలీసులు అరెస్టు చేశారు\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  So are actors.\n",
      "tgt_sentence:  అలాగే నటుడు కూడా.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Forgiveness also reflects understanding.\n",
      "tgt_sentence:  క్షమించడంలో అర్థం చేసుకోవడం కూడా ఉంది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  It overlooks the Mediterranean Sea to the north and the Atlantic Ocean to the west.\n",
      "tgt_sentence:  దక్షిణాన మధ్యధరా సముద్రం, ఉత్తరం మరియు పశ్చిమాన అట్లాంటిక్ మహాసముద్రం ద్వారా కడుగుతుంది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The results will be out by evening.\n",
      "tgt_sentence:  ఈ రాత్రికే ఫలితాలు వెలువడుతాయి.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The Congress, Telugu Desam Party (TDP), Telangana Jana Samithi (TJS) and the Communist Party of India (CPI), have decided to forge an alliance for the December 7 elections and are currently engaged in seat-sharing talks\n",
      "tgt_sentence:  కాంగ్రెస్‌, టీడీపీ, టీజేఎస్‌, సీపీఐ కలిసి ప్రజాకూటమిగా ఏర్పడి తెలంగాణ అసెంబ్లీ ఎన్నికల బరిలోకి దిగుతున్న సంగతి తెలిసిందే\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  He was convicted by the court in this case.\n",
      "tgt_sentence:  ఈ కేసులో అతడిని కోర్టు దోషిగా తేల్చింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  \"There will be no compromise on security or sovereignty,\"\" Ghafoor said.\"\n",
      "tgt_sentence:  దేశ భద్రత, సమగ్రత అంశంలో రాజీపడే ప్రసక్తే లేదు’ అని గఫూర్‌ అన్నారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  3 lakh upwards.\n",
      "tgt_sentence:  3లక్షల వరకు వసూలు చేస్తున్నారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  SONGS: 49, 73\n",
      "tgt_sentence:  పాటలు: 49, 73\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  It is still possible to climb the Campanile during these works.\n",
      "tgt_sentence:  ఈ లోపాలు ఉన్నప్పుడు సహజంగానే ఎదుగుదల ఆగి గర్భస్రావమవుతుంది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  But they will disappear soon.\n",
      "tgt_sentence:  కానీ వెంటనే అదృశ్యమవుతుంది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Village - Piprithan\n",
      "tgt_sentence:  గ్రామం - పిప్రితాన్\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Nirav Modi, accused of defrauding the Punjab National Bank of over Rs.\n",
      "tgt_sentence:  ప్రముఖ వజ్రాల వ్యాపారి నీరవ్‌ మోదీ పంజాబ్‌ నేషనల్‌ బ్యాంకులో దాదాపు రూ.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Three died after car falls into the canal\n",
      "tgt_sentence:  కాల్వలోకి దూసుకెళ్లిన కారు:ముగ్గురి మృతి\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The problem will be resolved soon.\n",
      "tgt_sentence:  త్వరలో సమస్య పరిష్కారమవుతుందన్నారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Bollywood stars, including Salman Khan, too performed at the ceremony.\n",
      "tgt_sentence:  అందులో భాగంగా బాలీవుడ్ సూపర్ స్టార్ సల్మాన్ ఖాన్ కూడా స్వచ్చభారత్ అభియాన్ కార్యక్రమంలో పాల్గొన్నాడు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Items exempted from GST\n",
      "tgt_sentence:  జీఎస్టీ నుంచి మినహాయింపు పొందిన వస్తువులు\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  This movie was a hit too.\n",
      "tgt_sentence:  ఈ సినిమా కూడా మంచి హిట్ అయినది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Inability to control urination\n",
      "tgt_sentence:  మూత్రవిసర్జన నియంత్రణ కోల్పోవడం\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Profile couldnt be created. Probably the chosen folder isnt writable.\n",
      "tgt_sentence:  ప్రొఫైలు సృష్టించబడలేదు. బహూశా ఎన్నుకున్న సంచయం వ్రాయుటుకు వీలుకానిదై వుంటుంది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  And you wont be left feeling frustrated.\n",
      "tgt_sentence:  మరియు మీరు నిరాశ వుండదు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  In a small bowl, combine the flour, baking powder, and salt.\n",
      "tgt_sentence:  ఒక చిన్న కంటైనర్ లో, పిండి, బేకింగ్ పౌడర్ మరియు ఉప్పు whisk కలిసి.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  From culture to commerce.\n",
      "tgt_sentence:  సంస్కృతి నుంచి వాణిజ్యం వ‌ర‌కు\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  \"Funds from UK, Italy, France and Australia were sent for this,\"\" he said.\"\n",
      "tgt_sentence:  బ్రిటన్, ఇటలీ, ఫ్రాన్స్, ఆస్ర్టేలియా నుంచి వీరికి నిధులు అందుతున్నాయి” అని వివరించారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The statue was purchased from Karnataka for Rs 35 lakh.\n",
      "tgt_sentence:  కర్ణాటక నుంచి రూ 35 లక్షలు వెచ్చించి కొనుగోలు చేసిన ఈ విగ్రహాన్ని మ్యూజియంలో ప్రదర్శనకు ఉంచుతారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Spilling pepper means that you will have an argument with your best friend Spilling salt brings bad luck unless a pinch is thrown over the left shoulder\n",
      "tgt_sentence:  ఉప్పు పడేయడం దురదృష్టాన్ని తెస్తుంది, అలా కాకూడదంటే మీ ఎడమ భుజం మీదుగా ఒక చిటికెడు ఉప్పును పడవేయాలి\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Prices drop\n",
      "tgt_sentence:  ధరలు తగ్గేవి ఇవే\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Towards the rear\n",
      "tgt_sentence:  వెనుక వైపు\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  She has two daughters.\n",
      "tgt_sentence:  జీత్‌రాంకు ఇద్దరు కూతుళ్లు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  BJP workers say that the police had kicked Biju.\n",
      "tgt_sentence:  పోలీస్‌ జులుంతో బీజేపీ కార్యకర్తలను చితకబాదారని ఆరోపించారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  It means elder sister.\n",
      "tgt_sentence:  అంటే పెద్దన్నయ్య అని అర్థం.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The hair is porous.\n",
      "tgt_sentence:  జుట్టు బోలెడంత ఉంది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Her husband was away on night duty.\n",
      "tgt_sentence:  భర్త విధుల్లో భాగంగా నైట్ డ్యూటీ కి వెళ్లాడు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Union Minister Nitin Gadkari and wife Kanchan casting their vote in Nagpur.\n",
      "tgt_sentence:  నాగ్‌పూర్‌లో సతీమణితో కలిసి ఓటు హక్కు వినియోగించుకున్న కేంద్రమంత్రి నితిన్ గడ్కరీ\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The meeting at Sonia Gandhi's residence was attended by Punjab Chief Minister Captain Amarinder Singh, Madhya Pradesh Chief Minister Kamal Nath, Rajasthan Chief Minister Ashok Gehlot and his deputy Sachin Pilot, Chhattisgarh Chief Minister Bhupesh Baghel, Puducherry Chief Minister V. Narayamsami, MP party in-charge Deepak Babaria, Rajasthan in-charge Avinash Pandey and Punjab in-charge Asha Kumari.\n",
      "tgt_sentence:  సోనియా నివాసంలో జరిగిన ఈ కార్యక్రమంలో పంజాబ్ సీఎం అమరీందర్ సింగ్, మధ్యప్రదేశ్ ముఖ్యమంత్రి కమల్‌నాథ్, రాజస్థాన్ ముఖ్యమంత్రి అశోక్ గెహ్లట్, చత్తీస్‌గఢ్ సీఎం భూషేశ్ బఘేల్, పుదుచ్చేరి ముఖ్యమంత్రి వి. నారాయణస్వామి పాల్గొన్నారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Power Star Pawan Kalyan is making a comeback to the movies.\n",
      "tgt_sentence:  పవర్ స్టార్ పవన్ కళ్యాణ్ మళ్ళీ సినిమాలతో బిజీ కానున్నారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The programme was also organised on the occasion.\n",
      "tgt_sentence:  అందుకోసం ఓ కార్యక్రమాన్ని కూడా నిర్వహించారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  entrance test\n",
      "tgt_sentence:  ప్రవేశ పరీక్షవిధానం\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Chemical detoxification\n",
      "tgt_sentence:  రసాయన శుద్ధి\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  She will be starring with Farhan Akhtar in the film.\n",
      "tgt_sentence:  ఈ మూవీలో ఫర్హాన్ అక్తర్ తో ఆమె కనిపించనుంది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The film also starred Madhuri and Karisma Kapoor.\n",
      "tgt_sentence:  కార్తీక్ కుమార్, బిందు మాధవి కూడా చిత్రంలో భాగమయ్యారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  They contain practical advice for daily life and offer a wonderful hope for the future. 9 / 1, pages 4 - 7.\n",
      "tgt_sentence:  దానిలో దైనందిన జీవితానికి అవసరమయ్యే ఆచరణాత్మక సలహా ఉంది, అంతేకాక అది భవిష్యత్తు విషయంలో అద్భుతమైన నిరీక్షణనిస్తుంది. — 9 / 1, 4 - 7 పేజీలు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Then I recalled that while attending the seminary in Belgium, I used to read the Crampon Bible.\n",
      "tgt_sentence:  “ నేను బెల్జియంలో సెమినరీలో ఉన్నప్పుడు క్రాంపన్‌ బైబిల్‌ చదివేవాడినని నాకు గుర్తుకొచ్చింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Of the 464 people who were symptomatic, only five of them tested positive.\n",
      "tgt_sentence:  అందులో 464 మందికి లక్షణాలు ఉంటే కేవలం ఐదుగురికే పాజిటీవ్ వచ్చింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  We see it everyday.\n",
      "tgt_sentence:  మనం అది జరగడం రోజూ చూస్తుంటాం.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Nivas K Prasanna is the music director.\n",
      "tgt_sentence:  నివాస్ కె ప్రసన్న సంగీత దర్శకుడు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The girl died in the incident.\n",
      "tgt_sentence:  ఈ ఘటనలో కూతురు చనిపోయింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Indias fight against the Corona global pandemic is moving ahead with great strength and steadfastness\n",
      "tgt_sentence:  ప్రపంచ మహమ్మారి కరోనాపై భారత్‌ పోరాటం అత్యంత శక్తిమంతంగా, స్థిరంగా ముందుకు సాగుతోంది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The Prime Minister of Bangladesh, Sheikh Hasina, was also present at the event.\n",
      "tgt_sentence:  ఈ కార్యక్రమానికి బంగ్లాదేశ్ ప్రధాని షేక్ హసీనీ వాజెద్ కూడా హజరయ్యారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Meanwhile, the Chennai Super Kings have not been able to perform at their best in this year's IPL and have constantly struggled.\n",
      "tgt_sentence:  కాగా, ఈ ఐపీఎల్ సీజన్‌లో చెన్నై సూపర్ కింగ్స్ పేలవ ప్రదర్శన కనబరుస్తోంది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Union Railway Minister Piyush Goyal\n",
      "tgt_sentence:  కేంద్ర రైల్వే మంత్రి పీయూష్‌ గోయల్‌\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Shes a deity.\n",
      "tgt_sentence:  ఈమె శివభక్తురాలు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Maize and guar\n",
      "tgt_sentence:  గజ్జి మరియు తామర\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  So, it was left for later.\n",
      "tgt_sentence:  అందుకే తర్వాత పక్కన పెట్టాడు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Drowsiness of the driver was the cause of accident, said passengers.\n",
      "tgt_sentence:  డ్రైవర్ నిర్లక్ష్యమే ప్రమాదానికి కారణమని ప్రయాణికులు తెలిపారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Midtown Manhattan\n",
      "tgt_sentence:  మిడ్ టౌన్ మన్హట్టన్, న్యూయార్క్\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  He can summon objects (He once summoned an over-sized lawnmowers to mow down his foes).\n",
      "tgt_sentence:  దీనినే శూన్యస్థితి అంటారు (కారణం ఆప్రాంతంలో వున్న గాలినంతటిని తొలగించడం వలన ఆనిర్ధిష్ట స్థలంలో శూన్యంగా వుంటుంది కనుక).\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  _Search for keys containing:\n",
      "tgt_sentence:  దీనిని కలగివున్న కీల కొరకు శోధించుము (_S):\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  How to dismantle it?\n",
      "tgt_sentence:  దీన్ని చక్కదిద్దడం ఎలా?\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  He is working very hard for this film.\n",
      "tgt_sentence:  తను ఈ సినిమా కోసం బాగా కష్టపడింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Nuzivedu Road\n",
      "tgt_sentence:  నూజివీడు రోడ్డు\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Others are taking cue.\n",
      "tgt_sentence:  మరికొందరు కటింగ్‌లు చేస్తుంటారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Hold for a few seconds and then return slowly to the starting position.\n",
      "tgt_sentence:  కొన్ని సెకన్ల స్థానానికి పట్టుకోండి, మరియు నెమ్మదిగా ప్రారంభ స్థానం తిరిగి.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Sindhu produced a dominating game to notch up a 21-9 21-23 21-14 win against sixth seed Mitani in a women's singles match that lasted an hour and six minutes here\n",
      "tgt_sentence:  66 నిమిషాలు పాటు సాగిన మ్యాచ్‌లో తాను గెలిచిన రెండు సెట్లలో పూర్తి ఆధిపత్యం చూపించిన సింధు 21- 9, 21- 23, 21- 14 తేడాతో మితానిని ఓడించింది\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The investigating officer said a case had been registered and investigations were on.\n",
      "tgt_sentence:  ఈ మేరకు కేసు నమోదు చేసుకుని దర్యాప్తు చేస్తున్నట్టు జైలు సూపరింటెండెంట్ తెలిపారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  BJP leaders\n",
      "tgt_sentence:  బీజేపీ నాయకుల డ్రామాలు\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  And he made in Jerusalem engines, invented by cunning men, to be on the towers and upon the bulwarks, to shoot arrows and great stones withal. And his name spread far abroad. for he was marvellously helped, till he was strong.\n",
      "tgt_sentence:  మరియు అతడు అంబుల నేమి పెద్దరాళ్లనేమి ప్రయోగించుటకై ఉపాయ శాలులు కల్పించిన యంత్రములను యెరూషలేములో చేయించి దుర్గములలోను బురుజులలోను ఉంచెను. అతడు స్థిరపడువరకు అతనికి ఆశ్చర్యకర మైన సహాయము కలిగెను గనుక అతని కీర్తి దూరముగా వ్యాపించెను.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  But its human.\n",
      "tgt_sentence:  అయితే ఆ వ్యక్తీ మానవుడే.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Six killed in different road mishaps\n",
      "tgt_sentence:  వేర్వేరు రోడ్డు ప్రమాదాల్లో ఆరుగురు దుర్మరణం చెందారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The State should own responsibility for the death.\n",
      "tgt_sentence:  ఈ హత్యకు ప్రభుత్వమే బాధ్యత వహించాలన్నారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Peoples lifestyle started changing.\n",
      "tgt_sentence:  మనుషుల లైఫ్ స్టైల్ మారింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The parents had lodged an FIR with the police regarding missing of the child.\n",
      "tgt_sentence:  దీంతో చిన్నారుల అదృశ్యంపై తల్లిదండ్రులు పోలీసులకు ఫిర్యాదు చేశారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Having taken the lead, the Indian raced away to a 5-0 lead in the second game as well\n",
      "tgt_sentence:  ఆ తర్వాత మళ్లీ రెండోసారి 5-0 తో క్లీన్ చేసే అవకాశం భారత్ ముందు నిలిచింది\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  In the last five years, we have changed both the meaning of dialogue and the way of communication with the Indian diaspora\n",
      "tgt_sentence:  గడచిన ఐదేళ్లలో ప్రవాస భారతీయులతో చర్చల అర్థాన్ని, సమాచార ఆదానప్రదాన రీతిని మేం పూర్తిగా మార్చేశాం.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Its fun for everyone.\n",
      "tgt_sentence:  అందరినీ తప్పకుండా అలరిస్తుంది' అని చెప్పారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Water resources minister\n",
      "tgt_sentence:  జల వనరులశాఖ మంత్రి దేవినేని వెల్లడి\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Yes, as in Noahs day, the present proliferation of evil is evidence that Gods judgment is impending, not just against wicked humans but also against Satan and his demons, who will be hurled into an abyss of inactivity as a prelude to their eventual destruction.\n",
      "tgt_sentence:  ( కీర్తన 92 :⁠ 7) అవును, నోవహు కాలంలోలాగే ప్రస్తుతం పెరిగిపోతున్న దుష్టత్వం దేవుని తీర్పు రాబోతోందనడానికి సూచనగా ఉంది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The automobile industry has been asking the government to reduce GST on passenger vehicles from the current 28 per cent to 18 per cent.\n",
      "tgt_sentence:  టూవీలర్లు, ప్యాసింజర్ వాహనాలను 28 శాతం జీఎస్‌టీ శ్లాబ్ నుంచి 18 శాతం జీఎస్టీ శ్లాబ్‌కు తీసుకురావాలని వాహన పరిశ్రమ సమాఖ్య సియామ్ డిమాండ్ చేస్తోంది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Ram scored a blockbuster with the film, iSmart Shankar directed by Puri Jagannadh.\n",
      "tgt_sentence:  వరుస ప్లాప్ లో ఉన్న రామ్ కి పూరి జగన్నాధ్ దర్శకత్వంలో వచ్చిన ఇస్మార్ట్ శంకర్ సినిమా మంచి విజయాన్ని అందించింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  India's Bharat Biotech is at the forefront in this regard.\n",
      "tgt_sentence:  ఈ నేపథ్యంలో భారతదేశానికి చెందిన భారత్‌ బయోటెక్‌ కంపెనీ సైతం ముందంజలో ఉంది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Every Household pays Rs.\n",
      "tgt_sentence:  ప్రతి పేద నాయీబ్రాహ్మణ కుటుంబానికి రూ.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Sushil Chowdhury, 76, the owner and editor of regional daily Dainik Gandoot, was found guilty on Monday by an Agartala court which will pronounce the sentence on Thursday\n",
      "tgt_sentence:  తీర్పు ప్రతి చదువుతున్నప్పుడు 76 ఏళ్ల దైనిక్ గనదూత్ ఎడిటర్ సుశీల్ చౌధురి కోర్టులోనే ఉన్నారు\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Spanish (espaol (helpinfo)) or Castilian (/kstlin/ (listen), castellano (helpinfo)) is a Romance language that originated in the Iberian Peninsula of Europe.\n",
      "tgt_sentence:  స్పానిష్ ( español (సహాయం·సమాచారం)) లేక కస్తీలియన్ (castellano) ఒక రోమనుల భాష.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  I have five children.\n",
      "tgt_sentence:  నాన్నగారికి మేం ఐదుగురు పిల్లలం.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Farmers have no money.\n",
      "tgt_sentence:  రైతులకు నిధులు అందడం లేదు అనడం అవాస్త‌వ‌మ‌ని.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  I was looking at it like What?.\n",
      "tgt_sentence:  “ఏంటి “అన్నట్లు చూసింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Not too broad.\n",
      "tgt_sentence:  రీ వెడల్పుగా కాదు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Its called illeism.\n",
      "tgt_sentence:  ఇది దిగజారుడుతనానికి గా పిలవబడింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Vijay Mallya says he met Arun Jaitley before leaving India\n",
      "tgt_sentence:  రుణాలు చెల్లిస్తా, భారత్ వచ్చేముందు జైట్లీని కలిశా!: విజయ్ మాల్యా సంచలనం | Vijay Mallya says he met Arun Jaitley before leaving India, FM denies - Telugu Oneindia\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Why Do You Need the Power?\n",
      "tgt_sentence:  నీకు శక్తి ఎందుకు అవసరం?\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Temples and Mosques\n",
      "tgt_sentence:  దేవాలయాలు, దర్గాలు\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The picture has gone viral on social media\n",
      "tgt_sentence:  ఈ ఫోటోలు సోషల్‌ మీడియాలో వ్యాప్తి చెందాయి\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  \"New Delhi, Aug 13: Prime Minister Narendra Modi on Thursday launched a platform 'Transparent Taxation Honoring the Honest' via video conference, which, he said will strengthen efforts of \"\"reforming and simplifying our tax system\"\"\"\n",
      "tgt_sentence:  ప్రధాని నరేంద్ర మోదీ 'పారదర్శక పన్ను-నిజాయితీని గౌరవించడం' అనే ప్లాట్‌ఫామ్‌ను గురువారం(అగస్టు 13) వీడియో కాన్ఫరెన్స్ ద్వారా ప్రారంభించారు\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  However, police were silent on the matter.\n",
      "tgt_sentence:  అయితే పోలీసులు ఈ ఘటన గురించి పెదవి విప్పడం లేదు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  \"This programme will be launched next month,\"\" Naqvi wrote on Twitter.\"\n",
      "tgt_sentence:  ఈ కార్యక్రమాన్ని వచ్చే నెలలో ప్రారంభిస్తామని ట్విటర్‌లో నఖ్వీ వెల్లడించారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The High Court rejected the plea upholding the decision of the Government.\n",
      "tgt_sentence:  ఈ అంశంపై విచారణ జరిపిన హైకోర్టు ప్రభుత్వ ఆదేశాలను కొట్టివేస్తున్నట్లు తీర్పు చెప్పింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  He said even if the Congress and the BJP came together, they would not be able to form government at the Centre.\n",
      "tgt_sentence:  బీజేపీ, కాంగ్రెస్‌ కలిసినా కేంద్రంలో ప్రభుత్వం ఏర్పాటు చేసే పరిస్థితి ఉండదన్నారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  TRS chief and Telangana chief minister K. Chandrashekar Rao.\n",
      "tgt_sentence:  తెలంగాణ ముఖ్యమంత్రి, టీఆర్ఎస్ అధినేత కేసీఆర్‌కు ఫోన్ చేశారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Police crackdown continues\n",
      "tgt_sentence:  కొనసాగుతున్న పోలీసుల కూంబింగ్\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Harish Shankar previously remade blockbuster Dabangg in Telugu as Gabbar Singh, starring Pawan Kalyan .\n",
      "tgt_sentence:  ఇదివరకే గబ్బర్ సింగ్ సినిమాలో హరీష్ శంకర్ దర్శకత్వంలో నటించిన పవన్ కళ్యాణ్ మంచి విజయం సాధించగా.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Romans chapter 8 speaks of those who walk according to the flesh in contrast with those who walk according to the spirit.\n",
      "tgt_sentence:  రోమీయులు 8వ అధ్యాయంలో అపొస్తలుడైన పౌలు రెండు రకాల ప్రజల గురించి అంటే, ‘ శరీరానుసారంగా ’ జీవించే వాళ్ల గురించి, ‘ ఆత్మానుసారంగా ’ జీవించే వాళ్ల గురించి చెప్పాడు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The situation is likely to stabilise in the next few days,he said.\n",
      "tgt_sentence:  కొద్ది రోజుల్లో రాష్ట్రంలో పరిస్థితులు చక్కబడతాయని ఆయన పేర్కొన్నారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Shamshad Ansari, Ravindra Kumar, Munna Ram, Pappu Singh, Ashok Thakur, Ramesh Singh, Nasim Khan and others participated in the event.\n",
      "tgt_sentence:  ఆనంద్ ప్రసాద్, ఆదిశేషగిరి రావు, జెమిని కిరణ్, అశోక్ కుమార్, ప్రసన్నకుమార్, ఎన్. వి. రెడ్డి, సాగర్, రామసత్యనారాయణ తదితరులు పాల్గొన్నారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  He took selfies.\n",
      "tgt_sentence:  సెల్ఫీ కూడా తీసుకున్నాడు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  I dont want any credit\n",
      "tgt_sentence:  నాకు క్రెడిట్ అవసరం లేదు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Close the current window.\n",
      "tgt_sentence:  ప్రస్తుత విండొ లేక పత్రాన్ని మూసివేయి\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  We can do this thing.\n",
      "tgt_sentence:  ఆ పని మనం చెయ్యగలం.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Two of them died.\n",
      "tgt_sentence:  అందులో ఇద్దరు మృత్యువాత పడ్డారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The child had died due to a head injury.\n",
      "tgt_sentence:  తలకి తీవ్రగాయాలవడంతో చిన్నారి కొద్దిసేపటికే మృతి చెందాడు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Advantage BJP!\n",
      "tgt_sentence:  అసోం బిజెపికే అనుకూలం!\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  About 360 faculty belonging to science and engineering departments and centres of the Institute are engaged in teaching, research and industrial consultancy.\n",
      "tgt_sentence:  సైన్స్‌, ఇంజినీరింగ్‌ విభాగాలు , సంస్థలోని వివిధ అనుబంధ కేంద్రాలకు చెందిన 360 మందికి పైగా అధ్యాపకులు బోధన, పరిశోధన , ఇండస్ట్రియల్‌ కన్సెల్టీకి సంబంధించిన పనులు చేస్తున్నారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Samples were taken from them and sent for tests.\n",
      "tgt_sentence:  వీరి నుంచి నమూనాలు సేకరించి పరీక్షల నిమిత్తం పూణె పంపించారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  After marriage, your name got changed.\n",
      "tgt_sentence:  ఆ తర్వాత సరళకి పెళ్లి కావడంతో ఇంటి పేరు మారి పోయింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  A traffic policeman tried to stop them.\n",
      "tgt_sentence:  ఓ ట్రాఫిక్ పోలీస్ మాత్రం వాళ్ళను నిలువరించడానికి ప్రయత్నించాడు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The remnants of where many of the old forgotten business once stood can still be seen today.\n",
      "tgt_sentence:  అమెరికాకి చెందిన మత ప్రబోధకులు ఉన్నత విద్య కొరకు స్థాపించిన అనేక ఇతర సంస్థలు ఈ రోజుకు కూడా పనిచేస్తున్నాయి.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  \"\"\"I am very fond of acting.\"\n",
      "tgt_sentence:  ‘నాకు నటన అంటే ఎంతో ఆసక్తి.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  For more information, see chapter 2 of the book What Does the Bible Really Teach?\n",
      "tgt_sentence:  ఆ పుస్తకాన్ని www.jw.org వెబ్‌సైట్‌లో చూడొచ్చు లేదా ఈ కోడ్‌ స్కాన్‌ చేయవచ్చు\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Floods in Kerala\n",
      "tgt_sentence:  కేరళలో వరదల బీభత్సం\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  3 lakh upwards.\n",
      "tgt_sentence:  3కోట్ల పైమాటేనన్న మాట.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Bollywood actor Neha Dhupia and Angad Bedi are one of the most adorable couples of the film industry.\n",
      "tgt_sentence:  బాలీవుడ్ హీరోయిన్ నేహా దూపియా, నటుడు అంగద్ బేడీతో పెళ్లికి ముందు నుంచే మంచి స్నేహితులు. వీరి మధ్య లవ్ ఎఫైర్ ఉందనే విషయం కూడా చాలా మందికి తెలియదు. ఈ ఏడ...\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Everyone accepted that.\n",
      "tgt_sentence:  దానికి అందరూ వత్తాసు పలికారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  ( b) What is implied by the Scriptural question, Where is Jehovah?\n",
      "tgt_sentence:  ( ఇటాలిక్కులు మావి.)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Heavy rainfall...\n",
      "tgt_sentence:  ఏకధాటిగా కురుస్తున్న భారీ వర్షాలకు .\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  The Bharatiya Janata Party (BJP) under the leadership of Narendra Modi has swept India once again.\n",
      "tgt_sentence:  నరేంద్రమోడీ నేతృత్వంలోని బిజెపి ప్రభుత్వం రాష్ట్రానికి మరోసారి మొండిచెయ్యి చూపింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Most people don't know.\n",
      "tgt_sentence:  చాలా మందికి అతనెవరో తెలియదు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  Patna, Jan 8: Senior JD(U) leader Nitish Kumar today dismissed speculation that his going to Delhi was meant to discuss the fate of Bihar CM with party chief Sharad Yadav, RJD and SP leaders\n",
      "tgt_sentence:  పాట్నా: భారతీయ జనతా పార్టీ జాతీయ అధ్యక్షులు అమిత్ షా శుక్రవారం నాడు జేడీయూ నేత, బీహార్ మాజీ ముఖ్యమంత్రి నితీష్ కుమార్ పైన, ఆర్జేడీ చీఫ్, మాజీ సీఎం లాలూ ప్రసాద్ యాదవ్ పైన తీవ్రస్థాయిలో నిప్పులు చెరిగారు\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  However, the vehicle stops a little ahead.\n",
      "tgt_sentence:  అయితే, ఆ కారు కొద్ది దూరం వెళ్లగానే ఆగిపోయింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence:  She was the chief guest on the occasion.\n",
      "tgt_sentence:  దీనికి ముఖ్య అతిథిగా హాజరై ఆమె మాట్లాడారు.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for data in translation_dataset:\n",
    "    src_sentences = data['src']\n",
    "    tgt_sentences = data['tgt']\n",
    "    print(\"src_sentence: \", src_sentences)\n",
    "    print(\"tgt_sentence: \", tgt_sentences)\n",
    "    print(\"-\" * 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3130, 508, 2861, 2560, 2383, 812, 7625, 3099, 361, 265, 3103, 358, 5666, 285, 31261, 443, 10665, 289, 479, 4057, 297, 11564, 15, 11015, 371, 755, 306, 20023, 17]\n",
      "[400, 264, 299, 264, 309, 269, 308, 292, 268, 292, 270, 292, 271, 266, 291, 264, 267, 266, 273, 263, 308, 269, 284, 265, 299, 264, 267, 263, 281, 294, 286, 264, 268, 331, 292, 280, 292, 268, 292, 281, 292, 268, 439, 512, 263, 267, 292, 281, 289, 292, 277, 263, 277, 292, 282, 265, 270, 304, 310, 270, 292, 284, 294, 279, 266, 663, 263, 271, 292, 282, 264, 299, 312, 286, 283, 285, 263, 267, 265, 267, 269, 318, 269, 376, 266, 272, 312, 273, 269, 277, 298, 293, 266, 343, 263, 274, 292, 281, 269, 272, 263, 267, 264, 323, 292, 284, 292, 268, 263, 281, 275, 313, 292, 268, 263, 307, 287, 282, 263, 282, 264, 296, 287, 279, 263, 279, 292, 282, 451, 766, 292, 268, 562, 276, 306, 265, 270, 292, 271, 266, 330, 266, 284, 266, 315, 269, 284, 265, 289, 298, 270, 263, 286, 276, 274, 292, 282, 269, 289, 298, 282, 265, 323, 292, 267, 264, 284, 264, 279, 275, 274, 294, 273, 301]\n"
     ]
    }
   ],
   "source": [
    "print(english_tokenizer.encode(text=\"Chief Minister YS Jagan Mohan Reddy clearly stated that the building was constructed in blatant violation of all laws and regulations, hence it should be demolished.\"))\n",
    "print(telugu_tokenizer.encode(text=\"ఇది నిబంధ‌న‌ల‌కు విరుద్ధంగా నిర్మించిన భ‌వ‌న‌మ‌నీ, అక్ర‌మ క‌ట్ట‌డాల తొల‌గింపు ఇక్క‌డి నుంచీ ప్రారంభం అవుతుందంటూ ముఖ్య‌మంత్రి జ‌గ‌న్మోహ‌న్ రెడ్డి చెప్ప‌డం, ఆయ‌న ఆదేశాల‌కు అనుగుణంగా కూల్చేయ‌డం కూడా జ‌రిగిపోయింది.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader to load the training data.\n",
    "train_dataloader = create_data_loader(dataset=translation_dataset, \n",
    "                                      english_tokenizer=english_tokenizer, \n",
    "                                      telugu_tokenizer=telugu_tokenizer, \n",
    "                                      num_workers=0, \n",
    "                                      batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of src_batch:  torch.Size([16, 176])\n",
      "src_batch:  tensor([[ 8347,  1283,   285,  ...,     2,     2,     2],\n",
      "        [ 9316,  3290, 10897,  ...,     2,     2,     2],\n",
      "        [ 1850,   960,    17,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [  298, 13223, 23579,  ...,     2,     2,     2],\n",
      "        [ 3653,  3239,  1042,  ...,     2,     2,     2],\n",
      "        [ 1097,   265,  1224,  ...,     2,     2,     2]])\n",
      "shape of tgt_batch:  torch.Size([16, 176])\n",
      "tgt_batch:  tensor([[   0,  272,  408,  ...,    2,    2,    2],\n",
      "        [   0,  271,  283,  ...,    2,    2,    2],\n",
      "        [   0, 2991,  460,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [   0,  333,  291,  ...,    2,    2,    2],\n",
      "        [   0,   28,  299,  ...,    2,    2,    2],\n",
      "        [   0, 2602,  265,  ...,    2,    2,    2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of src_batch:  torch.Size([16, 131])\n",
      "src_batch:  tensor([[ 9316,  3290, 10897,  ...,     2,     2,     2],\n",
      "        [ 2351,  4851,   357,  ...,     2,     2,     2],\n",
      "        [ 1015,  2336,  2747,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [ 1097,   265,  1224,  ...,     2,     2,     2],\n",
      "        [  572,   479,   831,  ...,     2,     2,     2],\n",
      "        [  392,   358,  1603,  ...,     2,     2,     2]])\n",
      "shape of tgt_batch:  torch.Size([16, 131])\n",
      "tgt_batch:  tensor([[    0,   271,   283,  ...,   273,   301,     1],\n",
      "        [    0, 11296,   266,  ...,     2,     2,     2],\n",
      "        [    0,   318,   265,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [    0,  2602,   265,  ...,     2,     2,     2],\n",
      "        [    0,   280,   283,  ...,     2,     2,     2],\n",
      "        [    0,   665,   263,  ...,     2,     2,     2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of src_batch:  torch.Size([16, 131])\n",
      "src_batch:  tensor([[ 7505,  5204,   351,  ...,     2,     2,     2],\n",
      "        [ 9316,  3290, 10897,  ...,     2,     2,     2],\n",
      "        [ 3653,  3239,  1042,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [ 1097,   265,  1224,  ...,     2,     2,     2],\n",
      "        [  392,   358,  1603,  ...,     2,     2,     2],\n",
      "        [  298,   675,   358,  ...,     2,     2,     2]])\n",
      "shape of tgt_batch:  torch.Size([16, 131])\n",
      "tgt_batch:  tensor([[   0,  357,  275,  ...,    2,    2,    2],\n",
      "        [   0,  271,  283,  ...,  273,  301,    1],\n",
      "        [   0,   28,  299,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [   0, 2602,  265,  ...,    2,    2,    2],\n",
      "        [   0,  665,  263,  ...,    2,    2,    2],\n",
      "        [   0,  333,  285,  ...,    2,    2,    2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of src_batch:  torch.Size([16, 116])\n",
      "src_batch:  tensor([[  298,  1103,   445,  ...,     2,     2,     2],\n",
      "        [ 5766, 22055,    79,  ...,     2,     2,     2],\n",
      "        [ 2351,  4851,   357,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [ 6781,  1367,   393,  ...,     2,     2,     2],\n",
      "        [12516,  5437,  4298,  ...,     2,     2,     2],\n",
      "        [  298, 13223, 23579,  ...,     2,     2,     2]])\n",
      "shape of tgt_batch:  torch.Size([16, 116])\n",
      "tgt_batch:  tensor([[    0,   279,   264,  ...,     2,     2,     2],\n",
      "        [    0,   580,   278,  ...,     2,     2,     2],\n",
      "        [    0, 11296,   266,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [    0,   333,   296,  ...,     2,     2,     2],\n",
      "        [    0,   747,   264,  ...,   273,   301,     1],\n",
      "        [    0,   333,   291,  ...,     2,     2,     2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of src_batch:  torch.Size([16, 93])\n",
      "src_batch:  tensor([[ 1097,   265,  1224,  ...,     2,     2,     2],\n",
      "        [18933,  7884,   297,  ...,     2,     2,     2],\n",
      "        [  392,   303,  1056,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [ 5766, 22055,    79,  ...,     2,     2,     2],\n",
      "        [  417,  2256,   634,  ...,     2,     2,     2],\n",
      "        [  298, 13223, 23579,  ...,     2,     2,     2]])\n",
      "shape of tgt_batch:  torch.Size([16, 93])\n",
      "tgt_batch:  tensor([[   0, 2602,  265,  ...,    2,    2,    2],\n",
      "        [   0,  268,  265,  ...,  267,  297,    1],\n",
      "        [   0,  665,  310,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [   0,  580,  278,  ...,    2,    2,    2],\n",
      "        [   0,  474,  456,  ...,    2,    2,    2],\n",
      "        [   0,  333,  291,  ...,    2,    2,    2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of src_batch:  torch.Size([16, 93])\n",
      "src_batch:  tensor([[   24,   769,   782,  ...,     2,     2,     2],\n",
      "        [   24,   769,   782,  ...,     2,     2,     2],\n",
      "        [   24,   769,   782,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [  298, 13223, 23579,  ...,     2,     2,     2],\n",
      "        [  298, 13223, 23579,  ...,     2,     2,     2],\n",
      "        [   44,   386,  1536,  ...,     2,     2,     2]])\n",
      "shape of tgt_batch:  torch.Size([16, 93])\n",
      "tgt_batch:  tensor([[  0,  24, 332,  ...,   2,   2,   2],\n",
      "        [  0,  24, 332,  ...,   2,   2,   2],\n",
      "        [  0,  24, 332,  ...,   2,   2,   2],\n",
      "        ...,\n",
      "        [  0, 333, 291,  ...,   2,   2,   2],\n",
      "        [  0, 333, 291,  ...,   2,   2,   2],\n",
      "        [  0, 268, 276,  ...,   2,   2,   2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of src_batch:  torch.Size([16, 176])\n",
      "src_batch:  tensor([[ 3130,   508,  2861,  ...,     2,     2,     2],\n",
      "        [    5,   575,   265,  ...,     2,     2,     2],\n",
      "        [   24,   769,   782,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [ 5766, 22055,    79,  ...,     2,     2,     2],\n",
      "        [  298, 13223, 23579,  ...,     2,     2,     2],\n",
      "        [  298,   675,   358,  ...,     2,     2,     2]])\n",
      "shape of tgt_batch:  torch.Size([16, 176])\n",
      "tgt_batch:  tensor([[  0, 400, 264,  ..., 273, 301,   1],\n",
      "        [  0, 473, 263,  ...,   2,   2,   2],\n",
      "        [  0,  24, 332,  ...,   2,   2,   2],\n",
      "        ...,\n",
      "        [  0, 580, 278,  ...,   2,   2,   2],\n",
      "        [  0, 333, 291,  ...,   2,   2,   2],\n",
      "        [  0, 333, 285,  ...,   2,   2,   2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of src_batch:  torch.Size([16, 176])\n",
      "src_batch:  tensor([[ 298,  675,  358,  ...,    2,    2,    2],\n",
      "        [4406, 1999, 4517,  ...,    2,    2,    2],\n",
      "        [ 392,  512, 1821,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [7505, 5204,  351,  ...,    2,    2,    2],\n",
      "        [8347, 1283,  285,  ...,    2,    2,    2],\n",
      "        [  44,  386, 1536,  ...,    2,    2,    2]])\n",
      "shape of tgt_batch:  torch.Size([16, 176])\n",
      "tgt_batch:  tensor([[  0, 333, 285,  ...,   2,   2,   2],\n",
      "        [  0, 302, 298,  ...,   2,   2,   2],\n",
      "        [  0, 342, 392,  ...,   2,   2,   2],\n",
      "        ...,\n",
      "        [  0, 357, 275,  ...,   2,   2,   2],\n",
      "        [  0, 272, 408,  ...,   2,   2,   2],\n",
      "        [  0, 268, 276,  ...,   2,   2,   2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of src_batch:  torch.Size([16, 65])\n",
      "src_batch:  tensor([[  298,  4907,   358,  ...,     2,     2,     2],\n",
      "        [  298,  4907,   358,  ...,     2,     2,     2],\n",
      "        [ 3156,   394,    15,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [   24,   769,   782,  ...,     2,     2,     2],\n",
      "        [ 5766, 22055,    79,  ...,     2,     2,     2],\n",
      "        [ 1097,   265,  1224,  ...,     2,     2,     2]])\n",
      "shape of tgt_batch:  torch.Size([16, 65])\n",
      "tgt_batch:  tensor([[   0,  333,  290,  ...,    2,    2,    2],\n",
      "        [   0,  333,  290,  ...,    2,    2,    2],\n",
      "        [   0,  325,  269,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [   0,   24,  332,  ...,    2,    2,    2],\n",
      "        [   0,  580,  278,  ...,    2,    2,    2],\n",
      "        [   0, 2602,  265,  ...,    2,    2,    2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of src_batch:  torch.Size([16, 66])\n",
      "src_batch:  tensor([[  298,  3255,   357,  ...,     2,     2,     2],\n",
      "        [ 5766, 22055,    79,  ...,     2,     2,     2],\n",
      "        [  298,   675,   358,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [18136,  5571,  3343,  ...,     2,     2,     2],\n",
      "        [18978, 13833,     2,  ...,     2,     2,     2],\n",
      "        [  392,   358,  1603,  ...,     2,     2,     2]])\n",
      "shape of tgt_batch:  torch.Size([16, 66])\n",
      "tgt_batch:  tensor([[  0, 281, 349,  ...,   2,   2,   2],\n",
      "        [  0, 580, 278,  ...,   2,   2,   2],\n",
      "        [  0, 333, 285,  ...,   2,   2,   2],\n",
      "        ...,\n",
      "        [  0, 916, 263,  ...,   2,   2,   2],\n",
      "        [  0, 284, 275,  ...,   2,   2,   2],\n",
      "        [  0, 665, 263,  ..., 267, 297,   1]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of src_batch:  torch.Size([16, 113])\n",
      "src_batch:  tensor([[  298,   675,   358,  ...,     2,     2,     2],\n",
      "        [ 1097,   265,  1224,  ...,     2,     2,     2],\n",
      "        [  298,   675,   358,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [ 1097,   265,  1224,  ...,     2,     2,     2],\n",
      "        [18978, 13833,     2,  ...,     2,     2,     2],\n",
      "        [  298,   675,   358,  ...,     2,     2,     2]])\n",
      "shape of tgt_batch:  torch.Size([16, 113])\n",
      "tgt_batch:  tensor([[   0,  333,  285,  ...,    2,    2,    2],\n",
      "        [   0, 2602,  265,  ...,    2,    2,    2],\n",
      "        [   0,  333,  285,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [   0, 2602,  265,  ...,    2,    2,    2],\n",
      "        [   0,  284,  275,  ...,    2,    2,    2],\n",
      "        [   0,  333,  285,  ...,    2,    2,    2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of src_batch:  torch.Size([16, 131])\n",
      "src_batch:  tensor([[ 8347,  1283,   285,  ...,     2,     2,     2],\n",
      "        [  392,   358,  1603,  ...,     2,     2,     2],\n",
      "        [30128,    29,  6775,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [  298,  4907,   358,  ...,     2,     2,     2],\n",
      "        [   24,   769,   782,  ...,     2,     2,     2],\n",
      "        [17814,   285,  2390,  ...,     2,     2,     2]])\n",
      "shape of tgt_batch:  torch.Size([16, 131])\n",
      "tgt_batch:  tensor([[  0, 272, 408,  ...,   2,   2,   2],\n",
      "        [  0, 665, 263,  ...,   2,   2,   2],\n",
      "        [  0, 279, 265,  ...,   2,   2,   2],\n",
      "        ...,\n",
      "        [  0, 333, 290,  ...,   2,   2,   2],\n",
      "        [  0,  24, 332,  ...,   2,   2,   2],\n",
      "        [  0, 284, 266,  ...,   2,   2,   2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of src_batch:  torch.Size([8, 131])\n",
      "src_batch:  tensor([[   24,   769,   782,  ...,     2,     2,     2],\n",
      "        [  298,  4907,   358,  ...,     2,     2,     2],\n",
      "        [  298,   675,   358,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [17814,   285,  2390,  ...,     2,     2,     2],\n",
      "        [18978, 13833,     2,  ...,     2,     2,     2],\n",
      "        [  392,   512,  1821,  ...,     2,     2,     2]])\n",
      "shape of tgt_batch:  torch.Size([8, 131])\n",
      "tgt_batch:  tensor([[  0,  24, 332,  ...,   2,   2,   2],\n",
      "        [  0, 333, 290,  ...,   2,   2,   2],\n",
      "        [  0, 333, 285,  ...,   2,   2,   2],\n",
      "        ...,\n",
      "        [  0, 284, 266,  ...,   2,   2,   2],\n",
      "        [  0, 284, 275,  ...,   2,   2,   2],\n",
      "        [  0, 342, 392,  ...,   2,   2,   2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Every thing seems alright with the data loader.\n",
    "for src_batch, tgt_batch in train_dataloader:\n",
    "    print(\"shape of src_batch: \", src_batch.shape)\n",
    "    print(\"src_batch: \", src_batch)\n",
    "    print(\"shape of tgt_batch: \", tgt_batch.shape)\n",
    "    print(\"tgt_batch: \", tgt_batch)\n",
    "    print(\"-\" * 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Find the number of input pairs getting lost because of the maximum sequence length condition\n",
    "- ### Quick peek at the src and tgt batches to confirm they seem to be grouped and created correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_implementation.data_processing.data_preparation.dataset_wrapper import DatasetWrapper\n",
    "from model_implementation.data_processing.data_preparation.data_helpers import get_tokenizers, load_data_from_disk\n",
    "from model_implementation.data_processing.data_preparation.data_loader import create_data_loader\n",
    "from model_implementation.utils.constants import BATCH_SIZE, FULL_EN_TE_DATASET_PATH, TRAIN_DATASET_PATH, VALIDATION_DATASET_PATH\n",
    "\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_OF_TOKENS_ALLOWED = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train dataset from disk.\n",
    "train_dataset: datasets.arrow_dataset.Dataset = load_data_from_disk(dataset_relative_path=TRAIN_DATASET_PATH)\n",
    "# Wrap the hugging face dataset in a pytorch Dataset to be able to use with pytorch DataLoader.\n",
    "translation_dataset_check_lost_pairs = DatasetWrapper(hf_dataset=train_dataset)\n",
    "\n",
    "# Load the validation dataset from disk.\n",
    "validation_dataset: datasets.arrow_dataset.Dataset = load_data_from_disk(dataset_relative_path=VALIDATION_DATASET_PATH)\n",
    "# Wrap the hugging face dataset in a pytorch Dataset to be able to use with pytorch DataLoader.\n",
    "validation_dataset_check_lost_pairs = DatasetWrapper(hf_dataset=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the tokenizers for the English and Telugu languages.\n",
    "bpe_english_tokenizer, bpe_telugu_tokenizer = get_tokenizers(dataset_relative_path=FULL_EN_TE_DATASET_PATH, tokenizer_type=\"bpe\", retrain_tokenizers=True, max_en_vocab_size=30000, max_te_vocab_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tokenizers for the English and Telugu languages.\n",
    "spacy_english_tokenizer, spacy_telugu_tokenizer = get_tokenizers(dataset_relative_path=FULL_EN_TE_DATASET_PATH, tokenizer_type=\"spacy\", retrain_tokenizers=True, max_en_vocab_size=30000, max_te_vocab_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader to load the training data.\n",
    "train_dataloader_check_lost_pairs = create_data_loader(dataset=translation_dataset_check_lost_pairs, \n",
    "                                                       english_tokenizer=bpe_english_tokenizer, \n",
    "                                                       telugu_tokenizer=bpe_telugu_tokenizer, \n",
    "                                                       num_workers=0, \n",
    "                                                       batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create the DataLoader to load the validation data.\n",
    "validation_dataloader_check_lost_pairs = create_data_loader(dataset=validation_dataset_check_lost_pairs, \n",
    "                                                            english_tokenizer=bpe_english_tokenizer, \n",
    "                                                            telugu_tokenizer=bpe_telugu_tokenizer, \n",
    "                                                            num_workers=0, \n",
    "                                                            batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the number of batches getting filtered because of the maximum sequence length allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_english_bad_batches = 0\n",
    "num_telugu_bad_batches = 0\n",
    "num_common_bad_batches = 0\n",
    "total_num_of_batches = 0\n",
    "\n",
    "for idx, (src_batch, tgt_batch) in enumerate(validation_dataloader_check_lost_pairs):\n",
    "    is_common_bad_batch = 0\n",
    "    total_num_of_batches += 1\n",
    "    if src_batch.size(1) > MAX_NUM_OF_TOKENS_ALLOWED:\n",
    "        num_english_bad_batches += 1\n",
    "        is_common_bad_batch += 1\n",
    "    if tgt_batch.size(1) > MAX_NUM_OF_TOKENS_ALLOWED:\n",
    "        num_telugu_bad_batches += 1\n",
    "        is_common_bad_batch += 1\n",
    "    if is_common_bad_batch == 2:\n",
    "        num_common_bad_batches += 1\n",
    "\n",
    "num_lost_english_translation_pairs = num_english_bad_batches * BATCH_SIZE\n",
    "num_lost_telugu_translation_pairs = num_telugu_bad_batches * BATCH_SIZE\n",
    "num_lost_common_translation_pairs = num_common_bad_batches * BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of english bad batches =  0\n",
      "Number of telugu bad batches =  9\n",
      "Number of common bad batches =  0\n",
      "Total Number of batches =  79\n",
      "Total number of translation pairs =  5056\n",
      "Number of english translation pairs lost because of tokenization =  0\n",
      "Number of telugu translation pairs lost because of tokenization =  576\n",
      "Number of common translation pairs lost because of tokenization =  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of english bad batches = \", num_english_bad_batches)\n",
    "print(\"Number of telugu bad batches = \", num_telugu_bad_batches)\n",
    "print(\"Number of common bad batches = \", num_common_bad_batches)\n",
    "print(\"Total Number of batches = \", total_num_of_batches)\n",
    "print(\"Total number of translation pairs = \", total_num_of_batches * BATCH_SIZE)\n",
    "print(\"Number of english translation pairs lost because of tokenization = \", num_lost_english_translation_pairs)\n",
    "print(\"Number of telugu translation pairs lost because of tokenization = \", num_lost_telugu_translation_pairs)\n",
    "print(\"Number of common translation pairs lost because of tokenization = \", num_lost_common_translation_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment how the token count is changing with different tokenizers and vocab sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000\n",
      "1 294\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "250000\n",
      "1 1038\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "250000\n",
      "2 409\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "250000\n",
      "1 4942\n"
     ]
    }
   ],
   "source": [
    "spacy_en_token_counts = [len(spacy_english_tokenizer.encode(data['src'])) for data in translation_dataset_check_lost_pairs]\n",
    "spacy_en_token_counts.sort()\n",
    "print(len(spacy_en_token_counts))\n",
    "print(spacy_en_token_counts[0], spacy_en_token_counts[-1])\n",
    "print(\"-\" * 150)\n",
    "\n",
    "spacy_te_token_counts = [len(spacy_telugu_tokenizer.encode(data['tgt'])) for data in translation_dataset_check_lost_pairs]\n",
    "spacy_te_token_counts.sort()\n",
    "print(len(spacy_te_token_counts))\n",
    "print(spacy_te_token_counts[0], spacy_te_token_counts[-1])\n",
    "print(\"-\" * 150)\n",
    "\n",
    "bpe_en_token_counts = [len(bpe_english_tokenizer.encode(data['src'])) for data in translation_dataset_check_lost_pairs]\n",
    "bpe_en_token_counts.sort()\n",
    "print(len(bpe_en_token_counts))\n",
    "print(bpe_en_token_counts[0], bpe_en_token_counts[-1])\n",
    "print(\"-\" * 150)\n",
    "\n",
    "bpe_te_token_counts = [len(bpe_telugu_tokenizer.encode(data['tgt'])) for data in translation_dataset_check_lost_pairs]\n",
    "bpe_te_token_counts.sort()\n",
    "print(len(bpe_te_token_counts))\n",
    "print(bpe_te_token_counts[0], bpe_te_token_counts[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_over_threshold(token_counts: list, threshold: int):\n",
    "    return len([count for count in token_counts if count > threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 4\n",
      "11 1245\n"
     ]
    }
   ],
   "source": [
    "print(get_count_over_threshold(spacy_en_token_counts, MAX_NUM_OF_TOKENS_ALLOWED), get_count_over_threshold(spacy_te_token_counts, MAX_NUM_OF_TOKENS_ALLOWED))\n",
    "print(get_count_over_threshold(bpe_en_token_counts, MAX_NUM_OF_TOKENS_ALLOWED), get_count_over_threshold(bpe_te_token_counts, MAX_NUM_OF_TOKENS_ALLOWED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping track of the results from different trained tokenizers.\n",
    "#\n",
    "# English:\n",
    "#     Spacy: \n",
    "#         vocab_size: 30000\n",
    "#         min_num_of_tokens: 1\n",
    "#         max_num_of_tokens: 294\n",
    "#         sentences over 100 tokens: 84\n",
    "#         sentences over 150 tokens: 23\n",
    "#         sentences over 200 tokens: 9\n",
    "#     ----------------------------------------------- \n",
    "#     BPE:\n",
    "#         vocab_size: 30000\n",
    "#         min_num_of_tokens: 2\n",
    "#         max_num_of_tokens: 449\n",
    "#         sentences over 100 tokens: 157\n",
    "#         sentences over 150 tokens: 37\n",
    "#         sentences over 200 tokens: 16\n",
    "#     ----------------------------------------------- \n",
    "#     Spacy: \n",
    "#         vocab_size: 50000\n",
    "#         min_num_of_tokens: 1\n",
    "#         max_num_of_tokens: 294\n",
    "#         sentences over 100 tokens: 84\n",
    "#         sentences over 150 tokens: 23\n",
    "#         sentences over 200 tokens: 9\n",
    "#     ----------------------------------------------- \n",
    "#     BPE:\n",
    "#         vocab_size: 50000\n",
    "#         min_num_of_tokens: 2\n",
    "#         max_num_of_tokens: 409\n",
    "#         sentences over 100 tokens: 129\n",
    "#         sentences over 150 tokens: 33\n",
    "#         sentences over 200 tokens: 11\n",
    "#     ----------------------------------------------- \n",
    "#\n",
    "#\n",
    "# Telugu\n",
    "#     Spacy: \n",
    "#         vocab_size: 30000\n",
    "#         min_num_of_tokens: 1\n",
    "#         max_num_of_tokens: 1038\n",
    "#         sentences over 100 tokens: 41\n",
    "#         sentences over 150 tokens: 7\n",
    "#         sentences over 200 tokens: 4\n",
    "#     ----------------------------------------------- \n",
    "#     BPE:\n",
    "#         vocab_size: 30000\n",
    "#         min_num_of_tokens: 1 \n",
    "#         max_num_of_tokens: 4950\n",
    "#         sentences over 100 tokens: 15239\n",
    "#         sentences over 150 tokens: 3862\n",
    "#         sentences over 200 tokens: 1247\n",
    "#     ----------------------------------------------- \n",
    "#     Spacy: \n",
    "#         vocab_size: 50000\n",
    "#         min_num_of_tokens: 1 \n",
    "#         max_num_of_tokens: 1038\n",
    "#         sentences over 100 tokens: 41\n",
    "#         sentences over 150 tokens: 7\n",
    "#         sentences over 200 tokens: 4\n",
    "#     ----------------------------------------------- \n",
    "#     BPE:\n",
    "#         vocab_size: 50000\n",
    "#         min_num_of_tokens: 1 \n",
    "#         max_num_of_tokens: 4942\n",
    "#         sentences over 100 tokens: 15226\n",
    "#         sentences over 150 tokens: 3861\n",
    "#         sentences over 200 tokens: 1245\n",
    "#\n",
    "#\n",
    "# I have the following insights from the above experimental data:\n",
    "#\n",
    "# 1) Spacy tokenizer seems a good choice from the perspective of the number of tokens.\n",
    "#       -- The number of tokens in telugu is considerable less spacy when compared to bpe.\n",
    "#       -- Less number of tokens for the same sentence is always good because the model has to make associations over less number of tokens. \n",
    "# 2) BPE tokenizer performance doesn't seem to change much when the vocab_size is increased from 30000 to 50000.\n",
    "#       -- It is good to use max vocab size of 30000 for Telugu in case BPE tokenizer is used for the model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of unknown tokens after the tokenization with different tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'unk' tokens in spacy english tokenizer:  70262\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Number of 'unk' tokens in spacy telugu tokenizer:  329344\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Number of 'unk' tokens in bpe english tokenizer:  0\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Number of 'unk' tokens in bpe telugu tokenizer:  0\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Counting the number of unknown tokens when tokenized with different tokenizers.\n",
    "# The 'unk' token is represented by the token id 3 in the tokenized sentences.\n",
    "# bpe tokenizer should not have any 'unk' tokens because it is a byte level tokenizer.\n",
    "spacy_en_unk_token_count = 0\n",
    "for data_point in translation_dataset_check_lost_pairs:\n",
    "    spacy_en_unk_token_count += spacy_english_tokenizer.encode(data_point['src']).count(3) \n",
    "print(\"Number of 'unk' tokens in spacy english tokenizer: \", spacy_en_unk_token_count)\n",
    "print(\"-\" * 150)\n",
    "\n",
    "spacy_te_unk_token_count = 0\n",
    "for data_point in translation_dataset_check_lost_pairs:\n",
    "    spacy_te_unk_token_count += spacy_telugu_tokenizer.encode(data_point['tgt']).count(3)\n",
    "print(\"Number of 'unk' tokens in spacy telugu tokenizer: \", spacy_te_unk_token_count)\n",
    "print(\"-\" * 150)\n",
    "\n",
    "bpe_en_unk_token_count = 0\n",
    "for data_point in translation_dataset_check_lost_pairs:\n",
    "    bpe_en_unk_token_count += bpe_english_tokenizer.encode(data_point['src']).count(3)\n",
    "print(\"Number of 'unk' tokens in bpe english tokenizer: \", bpe_en_unk_token_count)\n",
    "print(\"-\" * 150)\n",
    "\n",
    "bpe_te_unk_token_count = 0\n",
    "for data_point in translation_dataset_check_lost_pairs:\n",
    "    bpe_te_unk_token_count += bpe_telugu_tokenizer.encode(data_point['tgt']).count(3)\n",
    "print(\"Number of 'unk' tokens in bpe telugu tokenizer: \", bpe_te_unk_token_count)\n",
    "print(\"-\" * 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of source tokens in the training set using spacy tokenizer: 2778620\n",
      "Total number of target tokens in the training set using spacy tokenizer: 2182075\n",
      "Total number of source tokens in the training set using bpe tokenizer: 2959244\n",
      "Total number of target tokens in the training set using bpe tokenizer: 10683302\n"
     ]
    }
   ],
   "source": [
    "total_spacy_en_token_count = 0\n",
    "total_spacy_te_token_count = 0\n",
    "total_bpe_en_token_count = 0\n",
    "total_bpe_te_token_count = 0\n",
    "for data_point in translation_dataset_check_lost_pairs:\n",
    "    total_spacy_en_token_count += len(spacy_english_tokenizer.encode(data_point['src']))\n",
    "    total_spacy_te_token_count += len(spacy_telugu_tokenizer.encode(data_point['tgt']))\n",
    "    total_bpe_en_token_count += len(bpe_english_tokenizer.encode(data_point['src']))\n",
    "    total_bpe_te_token_count += len(bpe_telugu_tokenizer.encode(data_point['tgt']))\n",
    "\n",
    "\n",
    "print(f\"Total number of source tokens in the training set using spacy tokenizer: {total_spacy_en_token_count}\")\n",
    "print(f\"Total number of target tokens in the training set using spacy tokenizer: {total_spacy_te_token_count}\")\n",
    "print(f\"Total number of source tokens in the training set using bpe tokenizer: {total_bpe_en_token_count}\")\n",
    "print(f\"Total number of target tokens in the training set using bpe tokenizer: {total_bpe_te_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually observe some data output by the dataloader to confirm that batches are being formed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(src_batch sequence length:  28 , tgt_batch sequence length:  26\n",
      "(src_batch sequence length:  9 , tgt_batch sequence length:  7\n",
      "(src_batch sequence length:  7 , tgt_batch sequence length:  9\n",
      "(src_batch sequence length:  11 , tgt_batch sequence length:  10\n",
      "(src_batch sequence length:  37 , tgt_batch sequence length:  29\n",
      "(src_batch sequence length:  13 , tgt_batch sequence length:  14\n",
      "(src_batch sequence length:  35 , tgt_batch sequence length:  31\n",
      "(src_batch sequence length:  18 , tgt_batch sequence length:  15\n",
      "(src_batch sequence length:  22 , tgt_batch sequence length:  22\n",
      "(src_batch sequence length:  12 , tgt_batch sequence length:  14\n",
      "(src_batch sequence length:  12 , tgt_batch sequence length:  12\n",
      "(src_batch sequence length:  8 , tgt_batch sequence length:  9\n",
      "(src_batch sequence length:  42 , tgt_batch sequence length:  40\n",
      "(src_batch sequence length:  13 , tgt_batch sequence length:  16\n",
      "(src_batch sequence length:  22 , tgt_batch sequence length:  19\n",
      "(src_batch sequence length:  36 , tgt_batch sequence length:  35\n",
      "(src_batch sequence length:  32 , tgt_batch sequence length:  38\n",
      "(src_batch sequence length:  28 , tgt_batch sequence length:  25\n",
      "(src_batch sequence length:  19 , tgt_batch sequence length:  17\n",
      "(src_batch sequence length:  22 , tgt_batch sequence length:  22\n",
      "(src_batch sequence length:  12 , tgt_batch sequence length:  11\n"
     ]
    }
   ],
   "source": [
    "# Lets just iterate through a few batches observe the source and target batch sequence lengths.\n",
    "for idx, (src_batch, tgt_batch) in enumerate(train_dataloader_check_lost_pairs):\n",
    "    if idx > 20:\n",
    "        break\n",
    "    print(\"(src_batch sequence length: \", src_batch.shape[1], \", tgt_batch sequence length: \", tgt_batch.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(src_batch: \n",
      " tensor([[  481,  1209,   204,  ...,     2,     2,     2],\n",
      "        [   12,   500,    22,  ...,     2,     2,     2],\n",
      "        [  727,   184,     6,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [  347, 11390,    28,  ...,     2,     2,     2],\n",
      "        [   32, 27954,     9,  ...,     2,     2,     2],\n",
      "        [ 3321, 32480,  4022,  ...,     2,     2,     2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "(tgt_batch: \n",
      " tensor([[    0,   372, 18410,  ...,     2,     2,     2],\n",
      "        [    0,    23,     6,  ...,     2,     2,     2],\n",
      "        [    0,   502,  6982,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [    0,  6024,     3,  ...,     2,     2,     2],\n",
      "        [    0,   614,     3,  ...,     2,     2,     2],\n",
      "        [    0,  9999,  2753,  ...,     2,     2,     2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "(src_batch: \n",
      " tensor([[   63,  2589,    21,  ...,     2,     2,     2],\n",
      "        [40216,   134,    21,  ...,     6,  1496,     4],\n",
      "        [    3,   101,    16,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [ 1263,    31,    18,  ...,     2,     2,     2],\n",
      "        [  781,    39,    48,  ...,     2,     2,     2],\n",
      "        [    4,   754,    70,  ...,  8013,     4,     2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "(tgt_batch: \n",
      " tensor([[    0,  1217, 43783,  ...,     2,     2,     2],\n",
      "        [    0,     3,     3,  ...,    20,     4,     1],\n",
      "        [    0,    94,  6775,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [    0,  2058,   221,  ...,     2,     2,     2],\n",
      "        [    0,  1008,  5311,  ...,     2,     2,     2],\n",
      "        [    0,  1498,  9316,  ...,     2,     2,     2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "(src_batch: \n",
      " tensor([[   63,    24,    67, 15434,  7250,     4,     2,     2,     2],\n",
      "        [ 6315,    11,    29,    13,  1621,     4,     2,     2,     2],\n",
      "        [ 4222,  4711,     8,  8018,  4602,     4,     2,     2,     2],\n",
      "        [   33,    18,    29,   284,   240,   541,     4,     2,     2],\n",
      "        [  401,   229,    17,  4912,     4,     2,     2,     2,     2],\n",
      "        [   63,    89,    62,  8114,    21,     4,     2,     2,     2],\n",
      "        [ 1422,    11,   194,   147,    77,     2,     2,     2,     2],\n",
      "        [   41,    16,   293,  4170,    42,     4,     2,     2,     2],\n",
      "        [   87,   164,    56,    26,     4,     2,     2,     2,     2],\n",
      "        [   59,    11,     5,  1439,    37,    19,   474,    20,     2],\n",
      "        [   32,    88,   182,  3626,   330,     4,     2,     2,     2],\n",
      "        [    3,    57,    61,   823,     4,     2,     2,     2,     2],\n",
      "        [  289,   530,   499,     5,   188,     4,     2,     2,     2],\n",
      "        [   52,    99,    13,   133,   375,     4,     2,     2,     2],\n",
      "        [  181,    24,    46,  7597,  1679,     2,     2,     2,     2],\n",
      "        [   58,    17,   506,  1233,    19,     5,    34,     4,     2],\n",
      "        [  480,   228, 31053,    60,     2,     2,     2,     2,     2],\n",
      "        [20316,    17,  1011,    10,  6643,     4,     2,     2,     2],\n",
      "        [  129,   311,  1803,    83,     4,     2,     2,     2,     2],\n",
      "        [ 7128,    26,    11,   112,  1089,     4,     2,     2,     2],\n",
      "        [   58,    44,    13,   146,    33,   164,     4,     2,     2],\n",
      "        [   12,   196,    11,    10,  2192,     4,     2,     2,     2],\n",
      "        [   58,    38,  2544,    19,    21,     4,     2,     2,     2],\n",
      "        [  129,   164,   107,    31,    11,    77,     2,     2,     2],\n",
      "        [  298,    66,   763,   125,   464,     4,     2,     2,     2],\n",
      "        [ 1628,   172,    11,    13,   479,     4,     2,     2,     2],\n",
      "        [  625,   329, 24969,    51,     4,     2,     2,     2,     2],\n",
      "        [ 6837,    10,    43,    35,    51,     4,     2,     2,     2],\n",
      "        [   59,    46,   185,     9,  1563,     2,     2,     2,     2],\n",
      "        [  106,    11,   221,   768,   177,     4,     2,     2,     2],\n",
      "        [  100,     6,    31,    11,  1219,  9531,     4,     2,     2],\n",
      "        [   41,    11,  1279,    13,   458,   400,     4,     2,     2],\n",
      "        [   32,  1301,     9,    23,  1067,     4,     2,     2,     2],\n",
      "        [   49,    11,    13,  1116,  2746,     4,     2,     2,     2],\n",
      "        [ 2277,  2692,  4741,    15,    31,     4,     2,     2,     2],\n",
      "        [  302,    88,    26,  5964,    20,     2,     2,     2,     2],\n",
      "        [ 1005,    13,  2317,     7,  4015,    20,     2,     2,     2],\n",
      "        [ 1183,  3808,     8,    51,     4,     2,     2,     2,     2],\n",
      "        [  366,    24,   297,   671,     4,     2,     2,     2,     2],\n",
      "        [   58,    11,    13,   185,     9,   164,     4,     2,     2],\n",
      "        [ 4016,  5733,   368,     5,   105,     4,     2,     2,     2],\n",
      "        [ 2478,  1607,    50,  2761,    30,  2994,    47,     2,     2],\n",
      "        [  668,  1127,     9,    23,    76,     4,     2,     2,     2],\n",
      "        [   50, 19968,    47,  1050,    75, 14692,  1386,     4,     2],\n",
      "        [   58,    18,   288,  2494,     4,     2,     2,     2,     2],\n",
      "        [ 1262,    27,  2155,    19,    26,     4,     2,     2,     2],\n",
      "        [   41,    44,     5,   983,     7, 22315,     4,     2,     2],\n",
      "        [   59,    17,    99,    97,  5662,    20,     2,     2,     2],\n",
      "        [   87,  1829,   200,    19,  1610,     4,     2,     2,     2],\n",
      "        [   58,   593,     9,    23,  2789,     4,     2,     2,     2],\n",
      "        [  152,    46,  1563,    31,    56,    20,     2,     2,     2],\n",
      "        [ 2105,  2607,   103,  2992,   730,     2,     2,     2,     2],\n",
      "        [   52,     6,   343,   226,    29,    23,   148,     4,     2],\n",
      "        [ 1080,   171,    31,   356,     4,     2,     2,     2,     2],\n",
      "        [  401,   122,   111,     4,     2,     2,     2,     2,     2],\n",
      "        [   14,    14,    14,    41,    11,    29,  1579,     4,    14],\n",
      "        [   41,   149,    62,    24,   143,  1239,     4,     2,     2],\n",
      "        [   58,    11,    13,   444,    76,     4,     2,     2,     2],\n",
      "        [   59,    89,    48,   285,    10,    94,    20,     2,     2],\n",
      "        [  408,    17, 27348,    25,    26,     4,     2,     2,     2],\n",
      "        [28888,    15,    56, 12482,    20,     2,     2,     2,     2],\n",
      "        [   41,    18,    13, 29699,   274,     4,     2,     2,     2],\n",
      "        [   63,    27,    23,   206,   191,     4,     2,     2,     2],\n",
      "        [   52,  1906,    48, 16935,     4,     2,     2,     2,     2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "(tgt_batch: \n",
      " tensor([[    0, 22690,     3,   747,   798,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,     3, 43213,   185,  2207,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,  4756, 15281,     5,  9385, 24251,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,  2239,    10,   594,     4,     1,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,    86,   119,  1338,  3091,    83,     4,     1,     2,     2,\n",
      "             2],\n",
      "        [    0,  1676,    51,   537,  9026,   594,     4,     1,     2,     2,\n",
      "             2],\n",
      "        [    0,   533,     3,  2803,     4,    34,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,   439,   523,    10,  7199,    20,     4,     1,     2,     2,\n",
      "             2],\n",
      "        [    0,   155,    87,     6,   482, 13434,     4,     1,     2,     2,\n",
      "             2],\n",
      "        [    0,   148,   163,     7,     1,     2,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,    80,   180,  5849,   606,    42,     4,     1,     2,     2,\n",
      "             2],\n",
      "        [    0,     3,  1574,   175,   662,   128,     4,     1,     2,     2,\n",
      "             2],\n",
      "        [    0,     3,   945,  5043,     3,     1,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,    26,  4246,  2215,   162,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,    41,   122,     3,   723,     7,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,  1331, 11097,     4,     1,     2,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,  2503,  4474,   842,    54,    27,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,     3,     3,  2262,   162,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,    41,   195, 25365,     3,    49,     4,     1,     2,     2,\n",
      "             2],\n",
      "        [    0,   397,     3,    25,  1013,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,    80,   482,   433,    17,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,  1411,  3670,  1549,   474,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,   202, 29720,     3,  1077,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0, 10867, 19107,    85,  1586,    34,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,  5408,   359,  1672, 27028,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,   698,     5,    42,    59,  1920,     4,     1,     2,     2,\n",
      "             2],\n",
      "        [    0,  3566,   926,   910, 12129,    19,     4,     1,     2,     2,\n",
      "             2],\n",
      "        [    0,   773,     3,   259,    19,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,    41, 26334,   115,   268,     1,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,   529,    59,  6062, 30633,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,    23,   352, 26153,    62,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,     3,   121,   414,     4,     1,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,    93,   227,  8169,     3,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,    14,    93,  2087, 11493,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,     3,   222,   108,  1071,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,  1243,     6,  2750,   327,     4,     7,     1,     2,     2,\n",
      "             2],\n",
      "        [    0,  2894, 20507,     3,     7,     1,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,  1055,    97,     5, 32926,   582,    19,     4,     1,     2,\n",
      "             2],\n",
      "        [    0,   123,  2864, 46959, 11782,   621,     4,     1,     2,     2,\n",
      "             2],\n",
      "        [    0,   223,  2068,   753,     4,     1,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,   202,  3560,  2256,  9094,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,   507,   295,  4701,    15,  2418,  1145,    13,     1,     2,\n",
      "             2],\n",
      "        [    0,   510,   140,  1213, 13757,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,    15, 13321,    13,   135, 29756,  1988,     1,     2,     2,\n",
      "             2],\n",
      "        [    0,   232,    86, 10163,  3151,   114,     4,     1,     2,     2,\n",
      "             2],\n",
      "        [    0,   202,  1192,   321, 20340,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,    14,  3949,   713,  1593,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,     3,  1858,    61, 36243,     7,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0, 23554,  1343,   892, 19098,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,   100,  1501,  2400,     3,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,   301, 27644,  9330, 30771,     7,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,  5444,  3045,  1693,     3,     1,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,   300,   321,     3,     4,     1,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,   792,   852,    30,  1253,  9185,     4,     1,     2,     2,\n",
      "             2],\n",
      "        [    0,    86,    50,   141,    77,  5150,    38,     4,     1,     2,\n",
      "             2],\n",
      "        [    0,     8,     8,     8,   135,     3,   441, 25429,     4,     8,\n",
      "             1],\n",
      "        [    0,     3,   531,  1193,   534,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,   232,    16,  3880,    17,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0, 10006,  1517,     3,     7,     1,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,    39,   183, 35949, 33205,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,    87,   727,     3,   367,  3514,     7,     1,     2,     2,\n",
      "             2],\n",
      "        [    0, 12682,     3,   148,  1310,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,   206,   330,   585,  2321,     4,     1,     2,     2,     2,\n",
      "             2],\n",
      "        [    0,    26,  1382,  1916,  2692,     3,     4,     1,     2,     2,\n",
      "             2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "(src_batch: \n",
      " tensor([[   33,    24,    62,   305,     5,    34,   213,     4,     2,     2],\n",
      "        [  100,     6,    33,    57,    62,   886,    21,     4,     2,     2],\n",
      "        [ 1412,   113,    23,   420,  2215,     4,     2,     2,     2,     2],\n",
      "        [  298,    17,    56,   596,   353,     4,     2,     2,     2,     2],\n",
      "        [   58,    11,    67,  1345,     7,  6025,     4,     2,     2,     2],\n",
      "        [   52,    56,    21,    88,    10,     5,   649,     4,     2,     2],\n",
      "        [   12,   530,    18,  2794,     5,   658,     4,     2,     2,     2],\n",
      "        [   49,    11,   145,    13,    34,     4,     2,     2,     2,     2],\n",
      "        [   32,   325,   312,   291,  5872,     4,     2,     2,     2,     2],\n",
      "        [ 9520,    10, 11739,  4907,  1575,   165,     4,     2,     2,     2],\n",
      "        [    3, 48261,    44,  7496,    11,   133,     4,     2,     2,     2],\n",
      "        [ 2870,  7291,    27,    23,   590,    76,     4,     2,     2,     2],\n",
      "        [   12,   105,    73,   826,    51,     4,     2,     2,     2,     2],\n",
      "        [   41,    11,    13,   112,  5328,  1198,     4,     2,     2,     2],\n",
      "        [  359, 10193,   596,     3,     4,     2,     2,     2,     2,     2],\n",
      "        [11215,   715,   311,  1576,     4,     2,     2,     2,     2,     2],\n",
      "        [ 2288,    17,   213,     9,  4406,     4,     2,     2,     2,     2],\n",
      "        [   59,    11,   978,    10,    26,   131,    20,     2,     2,     2],\n",
      "        [   12,   849,    53,    13,   270,     4,     2,     2,     2,     2],\n",
      "        [   32,  1127,     9,  1244,     5,  1989,     4,     2,     2,     2],\n",
      "        [  104,    11,     6,     7,   963,     6,  2778, 30656,     4,     2],\n",
      "        [   12,  6600,    11,   808,   124,    25,   124,     4,     2,     2],\n",
      "        [  129,    27,    24,   125,   458,  1091,     4,     2,     2,     2],\n",
      "        [  289,  1648,    42,   427,  2012,     4,     2,     2,     2,     2],\n",
      "        [ 1309,    72,     7,    68,    89,    29,     4,     2,     2,     2],\n",
      "        [   41,    11,    13,   311,  1276,    85,     4,     2,     2,     2],\n",
      "        [  201,   132,    17, 10140,  2981,   671,     4,     2,     2,     2],\n",
      "        [   52,    21,    88,    67,  1857,     4,     2,     2,     2,     2],\n",
      "        [   69,    16,  9706,     5,  1132,     4,     2,     2,     2,     2],\n",
      "        [ 2105,     6,  2360,   827,    35,    56,    22,    80,   324,     2],\n",
      "        [   12,   105,    16,   680,   708,     4,     2,     2,     2,     2],\n",
      "        [  772,  4352,   103,     3,  4948, 12374,     2,     2,     2,     2],\n",
      "        [ 1607,  1103,     9,  6720,   716,     2,     2,     2,     2,     2],\n",
      "        [   63,   205,   180,  1300,   362,     2,     2,     2,     2,     2],\n",
      "        [ 5391,  4963,    30, 11062,   323, 41102,   323,  9966,     2,     2],\n",
      "        [   32,    53,    39,    26,    10,   189,     4,     2,     2,     2],\n",
      "        [  106,    11,   107,    31,   762,     4,     2,     2,     2,     2],\n",
      "        [   14,    14,    14,   851,   244,   801,    38,  2550,     4,    14],\n",
      "        [ 1836,   122,   101,    18,  1564,     4,     2,     2,     2,     2],\n",
      "        [   33,    66, 19909,   182,   524,  4235,     4,     2,     2,     2],\n",
      "        [   32,  1078,  1112,   543,  2413,     4,     2,     2,     2,     2],\n",
      "        [   12,   729,  1268,    16,   389,     3,     4,     2,     2,     2],\n",
      "        [   32,   386,   552,     5,   341,  1819,     4,     2,     2,     2],\n",
      "        [  100,     6,   498,    11,   806,     4,     2,     2,     2,     2],\n",
      "        [   63,    38, 11202, 17072,     8,  7973,     4,     2,     2,     2],\n",
      "        [  106,    18,    54,  5328,  1028,     4,     2,     2,     2,     2],\n",
      "        [   14,    33,    57,   155,    57,  2735,     4,    14,    14,    14],\n",
      "        [   33,    57,    62,   164,   107,     5,   411,    11,     4,     2],\n",
      "        [   43,  1933,     9,   486,  3220,     2,     2,     2,     2,     2],\n",
      "        [    3,    66,   203,     9,  9643,   592,     2,     2,     2,     2],\n",
      "        [   33,    27,   158,   919,   116,     5,   335,     4,     2,     2],\n",
      "        [10431, 24165,  9663,  4380,     3,     4,     2,     2,     2,     2],\n",
      "        [   12,   720,    18,  1162,   122,     4,     2,     2,     2,     2],\n",
      "        [   12,  1600,   352,   489,     4,     2,     2,     2,     2,     2],\n",
      "        [   32,   564,     8,    16,    13,   426,     4,     2,     2,     2],\n",
      "        [  558,   122,    38,  1979,     4,     2,     2,     2,     2,     2],\n",
      "        [ 1767,   183,  1348,    15,  5641,   287,     2,     2,     2,     2],\n",
      "        [15178,   441,    24,   567,    82,     4,     2,     2,     2,     2],\n",
      "        [30496,  1981,   203,     9,   855,     2,     2,     2,     2,     2],\n",
      "        [  139,    13,   112,  1089,   551,     4,     2,     2,     2,     2],\n",
      "        [ 1242,   609,  4522,    84,     5,   199,     4,     2,     2,     2],\n",
      "        [   51,   412,   122,     4,     2,     2,     2,     2,     2,     2],\n",
      "        [  179,  1311,    17,   102,  1084,     4,     2,     2,     2,     2],\n",
      "        [   63,    17,   792,     9,   899,     4,     2,     2,     2,     2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "(tgt_batch: \n",
      " tensor([[    0,    11,   320,    99,  3083,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,   300,   120,     3,    33, 12187,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,   562,     3,  3306, 17897,   298,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,  1387,   260,   260,     3, 25697,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0, 37397,   134,  8232,    32,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,    26,    87,     3,     3,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,  6567,   566,  6479,  1667,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,   394,   177,   296,    31,    14,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,    11,    55,  2370,  6723,  6082,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0, 31200, 12359,  3125,  2809,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,  7588,     3,  8432,    25,  2084,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0, 11274,   117,   529, 21035,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,  1204,  3027,    98,    48,    19,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,    14,    25,  8031,   747,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,     3,  6314, 31806,     3, 26734, 11325,     4,     1,     2,\n",
      "             2,     2,     2],\n",
      "        [    0, 22816,   434, 46640, 14735, 34466,  3264,     4,     1,     2,\n",
      "             2,     2,     2],\n",
      "        [    0, 47734,   169,    99,     3,    17,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,   142,   971,     6,   267,     7,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,    11,  4560,    16,  1445, 14847,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,  2213, 11727,     3,  1869,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,  6089,    42,    25,     3,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,  2939,     3,  2044,     4,     1,     2,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,    41,   422,  4463,  4723,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,   967,  2532,   123,     3, 18002,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,    23,    50,  1233,   594,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,     3, 10506,    31,    14,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,  1387,    25,  7974,   742,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,    23, 11222,     3,    85,    96,    43,     4,     1,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,  1989,    67,    10,  3065,     3,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,  5444,     5,  2397, 49187,  2089,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,    39,   934,   602,  3901, 10634,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,     3, 32359,  3004,  2804,     1,     2,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0, 36316,  7048,  1032,  1602,  7589,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,   232,   153,  1473,   143,  2504,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,  1629,     9, 22081,  1584,    15,     3,     3,     3,    13,\n",
      "          5259,     4,     1],\n",
      "        [    0,     6,  4666,  2052,   393,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,   203, 34963,    59,   384,  6072,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,    44,   170,   119,  1568, 28193, 15288,     4,     1,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,  1679,   141, 36443,  5235,   969,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,   170,  3211,  9692,     3,     1,     2,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,   963,  3520,  8277,  2751,   271,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,  3247,   149,     3,   179,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,   151,     3, 18217,    46,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,    23,  6349,    96,    16,   761,   333,     4,     1,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,    51,   944,     3,     5, 18112,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,    14,    16,  5742,  1642,    17,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,     3,  3514, 40787,    32,    44,    24,   706,     4,     1,\n",
      "             2,     2,     2],\n",
      "        [    0,   411,     3,     3,     4,     1,     2,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0, 12000,  1730,   124,  2074,  1689,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,     3,     3, 28485,     3,     1,     2,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0, 45972, 22408,    29,    62,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0, 27024,  3383,  4380,  7128,     3,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,  1067,   107,  3866,   825,     3,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,     6,   205,  2290,  2224,   750,   757,     4,     1,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,  2349,  8440, 28872,  9102,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,   449,   512,   107,   712, 24923,    46,     4,     1,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,     3,  1856,     3,  1167,     1,     2,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0, 15037,   380,  5406,     3, 12034,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,  3020,  1384,     3,  1489, 44119,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,    25, 19362,     6, 24630,     3,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,   182,   277,  3154,     3,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,   334,   340,  1428,   319, 16415,  1507,    19,     4,     1,\n",
      "             2,     2,     2],\n",
      "        [    0,    12,    78, 15348,  1015,   128,     4,     1,     2,     2,\n",
      "             2,     2,     2],\n",
      "        [    0,   245,  4859,   384,  2618,  1129,     4,     1,     2,     2,\n",
      "             2,     2,     2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "(src_batch: \n",
      " tensor([[  12, 2464,  896,  ...,    2,    2,    2],\n",
      "        [ 465,  228,   16,  ...,    2,    2,    2],\n",
      "        [ 760,    6, 3686,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [  12, 2118,  438,  ...,    2,    2,    2],\n",
      "        [ 154,   30,  162,  ...,    2,    2,    2],\n",
      "        [1284,   11,   54,  ...,    2,    2,    2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "(tgt_batch: \n",
      " tensor([[   0,  190,    5,  ...,    1,    2,    2],\n",
      "        [   0, 2461, 3810,  ...,    2,    2,    2],\n",
      "        [   0,  140,    5,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [   0, 7700,  328,  ...,    2,    2,    2],\n",
      "        [   0,  126,    3,  ...,    4,    1,    2],\n",
      "        [   0, 4667,    5,  ..., 3203,    4,    1]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "(src_batch: \n",
      " tensor([[   12,   989,    21,  ...,     2,     2,     2],\n",
      "        [ 1087,     6,   121,  ...,     2,     2,     2],\n",
      "        [   70,    13,   596,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [   25,  7145,  1180,  ...,     2,     2,     2],\n",
      "        [38200,     6,     5,  ...,     2,     2,     2],\n",
      "        [  707,   236,     5,  ...,     2,     2,     2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "(tgt_batch: \n",
      " tensor([[    0,  1258,    71,  ...,     2,     2,     2],\n",
      "        [    0,     6,   205,  ...,     2,     2,     2],\n",
      "        [    0,    16,  9035,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [    0, 18877,     3,  ...,     2,     2,     2],\n",
      "        [    0,   373,   448,  ...,     2,     2,     2],\n",
      "        [    0,   188,  2195,  ...,     2,     2,     2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lets just iterate through a few batches observe the source and target batches.\n",
    "for idx, (src_batch, tgt_batch) in enumerate(train_dataloader_check_lost_pairs):\n",
    "    if idx > 5:\n",
    "        break\n",
    "    print(\"(src_batch: \\n\", src_batch)\n",
    "    print(\"-\" * 150)\n",
    "    print(\"(tgt_batch: \\n\", tgt_batch)\n",
    "    print(\"-\" * 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the saved model state dict to see that all the parameters are being saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_implementation.data_processing.data_preparation.data_helpers import get_tokenizers\n",
    "from model_implementation.model_building.machine_translation_model import MachineTranslationModel\n",
    "from model_implementation.utils.constants import ( \n",
    "    DROPOUT_PROB, D_FEED_FORWARD, D_MODEL, FULL_EN_TE_DATASET_PATH, \n",
    "    MAX_INPUT_SEQUENCE_LENGTH, NUM_HEADS, NUM_LAYERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenizer, telugu_tokenizer = get_tokenizers(dataset_relative_path=FULL_EN_TE_DATASET_PATH, tokenizer_type=\"bpe\", retrain_tokenizers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_vocab_size:  30000\n",
      "tgt_vocab_size:  30000\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size=english_tokenizer.get_vocab_size() \n",
    "tgt_vocab_size=telugu_tokenizer.get_vocab_size()\n",
    "print(\"src_vocab_size: \", src_vocab_size)\n",
    "print(\"tgt_vocab_size: \", tgt_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_model = MachineTranslationModel(d_model=D_MODEL, \n",
    "                                            d_feed_forward=D_FEED_FORWARD,\n",
    "                                            dropout_prob=DROPOUT_PROB, \n",
    "                                            num_heads=NUM_HEADS, \n",
    "                                            src_vocab_size=src_vocab_size, \n",
    "                                            tgt_vocab_size=tgt_vocab_size, \n",
    "                                            num_layers=NUM_LAYERS, \n",
    "                                            max_seq_len=MAX_INPUT_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(translation_model.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_embedding.look_up_table.weight   torch.Size([30000, 512])\n",
      "tgt_embedding.look_up_table.weight   torch.Size([30000, 512])\n",
      "positional_encoding.positional_encoding   torch.Size([1, 150, 512])\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.0.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.0.bias   torch.Size([512])\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.1.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.1.bias   torch.Size([512])\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.2.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.2.bias   torch.Size([512])\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.3.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.3.bias   torch.Size([512])\n",
      "encoder.encoder_layers.0.feed_forward.linear_layer_1.weight   torch.Size([2048, 512])\n",
      "encoder.encoder_layers.0.feed_forward.linear_layer_1.bias   torch.Size([2048])\n",
      "encoder.encoder_layers.0.feed_forward.linear_layer_2.weight   torch.Size([512, 2048])\n",
      "encoder.encoder_layers.0.feed_forward.linear_layer_2.bias   torch.Size([512])\n",
      "encoder.encoder_layers.0.sublayer_wrappers.0.layer_norm.weight   torch.Size([512])\n",
      "encoder.encoder_layers.0.sublayer_wrappers.0.layer_norm.bias   torch.Size([512])\n",
      "encoder.encoder_layers.0.sublayer_wrappers.1.layer_norm.weight   torch.Size([512])\n",
      "encoder.encoder_layers.0.sublayer_wrappers.1.layer_norm.bias   torch.Size([512])\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.0.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.0.bias   torch.Size([512])\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.1.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.1.bias   torch.Size([512])\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.2.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.2.bias   torch.Size([512])\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.3.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.3.bias   torch.Size([512])\n",
      "encoder.encoder_layers.1.feed_forward.linear_layer_1.weight   torch.Size([2048, 512])\n",
      "encoder.encoder_layers.1.feed_forward.linear_layer_1.bias   torch.Size([2048])\n",
      "encoder.encoder_layers.1.feed_forward.linear_layer_2.weight   torch.Size([512, 2048])\n",
      "encoder.encoder_layers.1.feed_forward.linear_layer_2.bias   torch.Size([512])\n",
      "encoder.encoder_layers.1.sublayer_wrappers.0.layer_norm.weight   torch.Size([512])\n",
      "encoder.encoder_layers.1.sublayer_wrappers.0.layer_norm.bias   torch.Size([512])\n",
      "encoder.encoder_layers.1.sublayer_wrappers.1.layer_norm.weight   torch.Size([512])\n",
      "encoder.encoder_layers.1.sublayer_wrappers.1.layer_norm.bias   torch.Size([512])\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.0.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.0.bias   torch.Size([512])\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.1.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.1.bias   torch.Size([512])\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.2.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.2.bias   torch.Size([512])\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.3.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.3.bias   torch.Size([512])\n",
      "encoder.encoder_layers.2.feed_forward.linear_layer_1.weight   torch.Size([2048, 512])\n",
      "encoder.encoder_layers.2.feed_forward.linear_layer_1.bias   torch.Size([2048])\n",
      "encoder.encoder_layers.2.feed_forward.linear_layer_2.weight   torch.Size([512, 2048])\n",
      "encoder.encoder_layers.2.feed_forward.linear_layer_2.bias   torch.Size([512])\n",
      "encoder.encoder_layers.2.sublayer_wrappers.0.layer_norm.weight   torch.Size([512])\n",
      "encoder.encoder_layers.2.sublayer_wrappers.0.layer_norm.bias   torch.Size([512])\n",
      "encoder.encoder_layers.2.sublayer_wrappers.1.layer_norm.weight   torch.Size([512])\n",
      "encoder.encoder_layers.2.sublayer_wrappers.1.layer_norm.bias   torch.Size([512])\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.0.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.0.bias   torch.Size([512])\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.1.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.1.bias   torch.Size([512])\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.2.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.2.bias   torch.Size([512])\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.3.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.3.bias   torch.Size([512])\n",
      "encoder.encoder_layers.3.feed_forward.linear_layer_1.weight   torch.Size([2048, 512])\n",
      "encoder.encoder_layers.3.feed_forward.linear_layer_1.bias   torch.Size([2048])\n",
      "encoder.encoder_layers.3.feed_forward.linear_layer_2.weight   torch.Size([512, 2048])\n",
      "encoder.encoder_layers.3.feed_forward.linear_layer_2.bias   torch.Size([512])\n",
      "encoder.encoder_layers.3.sublayer_wrappers.0.layer_norm.weight   torch.Size([512])\n",
      "encoder.encoder_layers.3.sublayer_wrappers.0.layer_norm.bias   torch.Size([512])\n",
      "encoder.encoder_layers.3.sublayer_wrappers.1.layer_norm.weight   torch.Size([512])\n",
      "encoder.encoder_layers.3.sublayer_wrappers.1.layer_norm.bias   torch.Size([512])\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.0.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.0.bias   torch.Size([512])\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.1.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.1.bias   torch.Size([512])\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.2.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.2.bias   torch.Size([512])\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.3.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.3.bias   torch.Size([512])\n",
      "encoder.encoder_layers.4.feed_forward.linear_layer_1.weight   torch.Size([2048, 512])\n",
      "encoder.encoder_layers.4.feed_forward.linear_layer_1.bias   torch.Size([2048])\n",
      "encoder.encoder_layers.4.feed_forward.linear_layer_2.weight   torch.Size([512, 2048])\n",
      "encoder.encoder_layers.4.feed_forward.linear_layer_2.bias   torch.Size([512])\n",
      "encoder.encoder_layers.4.sublayer_wrappers.0.layer_norm.weight   torch.Size([512])\n",
      "encoder.encoder_layers.4.sublayer_wrappers.0.layer_norm.bias   torch.Size([512])\n",
      "encoder.encoder_layers.4.sublayer_wrappers.1.layer_norm.weight   torch.Size([512])\n",
      "encoder.encoder_layers.4.sublayer_wrappers.1.layer_norm.bias   torch.Size([512])\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.0.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.0.bias   torch.Size([512])\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.1.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.1.bias   torch.Size([512])\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.2.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.2.bias   torch.Size([512])\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.3.weight   torch.Size([512, 512])\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.3.bias   torch.Size([512])\n",
      "encoder.encoder_layers.5.feed_forward.linear_layer_1.weight   torch.Size([2048, 512])\n",
      "encoder.encoder_layers.5.feed_forward.linear_layer_1.bias   torch.Size([2048])\n",
      "encoder.encoder_layers.5.feed_forward.linear_layer_2.weight   torch.Size([512, 2048])\n",
      "encoder.encoder_layers.5.feed_forward.linear_layer_2.bias   torch.Size([512])\n",
      "encoder.encoder_layers.5.sublayer_wrappers.0.layer_norm.weight   torch.Size([512])\n",
      "encoder.encoder_layers.5.sublayer_wrappers.0.layer_norm.bias   torch.Size([512])\n",
      "encoder.encoder_layers.5.sublayer_wrappers.1.layer_norm.weight   torch.Size([512])\n",
      "encoder.encoder_layers.5.sublayer_wrappers.1.layer_norm.bias   torch.Size([512])\n",
      "encoder.layer_norm.weight   torch.Size([512])\n",
      "encoder.layer_norm.bias   torch.Size([512])\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.0.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.0.bias   torch.Size([512])\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.1.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.1.bias   torch.Size([512])\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.2.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.2.bias   torch.Size([512])\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.3.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.3.bias   torch.Size([512])\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.0.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.0.bias   torch.Size([512])\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.1.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.1.bias   torch.Size([512])\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.2.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.2.bias   torch.Size([512])\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.3.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.3.bias   torch.Size([512])\n",
      "decoder.decoder_layers.0.feed_forward.linear_layer_1.weight   torch.Size([2048, 512])\n",
      "decoder.decoder_layers.0.feed_forward.linear_layer_1.bias   torch.Size([2048])\n",
      "decoder.decoder_layers.0.feed_forward.linear_layer_2.weight   torch.Size([512, 2048])\n",
      "decoder.decoder_layers.0.feed_forward.linear_layer_2.bias   torch.Size([512])\n",
      "decoder.decoder_layers.0.sublayer_wrappers.0.layer_norm.weight   torch.Size([512])\n",
      "decoder.decoder_layers.0.sublayer_wrappers.0.layer_norm.bias   torch.Size([512])\n",
      "decoder.decoder_layers.0.sublayer_wrappers.1.layer_norm.weight   torch.Size([512])\n",
      "decoder.decoder_layers.0.sublayer_wrappers.1.layer_norm.bias   torch.Size([512])\n",
      "decoder.decoder_layers.0.sublayer_wrappers.2.layer_norm.weight   torch.Size([512])\n",
      "decoder.decoder_layers.0.sublayer_wrappers.2.layer_norm.bias   torch.Size([512])\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.0.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.0.bias   torch.Size([512])\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.1.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.1.bias   torch.Size([512])\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.2.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.2.bias   torch.Size([512])\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.3.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.3.bias   torch.Size([512])\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.0.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.0.bias   torch.Size([512])\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.1.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.1.bias   torch.Size([512])\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.2.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.2.bias   torch.Size([512])\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.3.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.3.bias   torch.Size([512])\n",
      "decoder.decoder_layers.1.feed_forward.linear_layer_1.weight   torch.Size([2048, 512])\n",
      "decoder.decoder_layers.1.feed_forward.linear_layer_1.bias   torch.Size([2048])\n",
      "decoder.decoder_layers.1.feed_forward.linear_layer_2.weight   torch.Size([512, 2048])\n",
      "decoder.decoder_layers.1.feed_forward.linear_layer_2.bias   torch.Size([512])\n",
      "decoder.decoder_layers.1.sublayer_wrappers.0.layer_norm.weight   torch.Size([512])\n",
      "decoder.decoder_layers.1.sublayer_wrappers.0.layer_norm.bias   torch.Size([512])\n",
      "decoder.decoder_layers.1.sublayer_wrappers.1.layer_norm.weight   torch.Size([512])\n",
      "decoder.decoder_layers.1.sublayer_wrappers.1.layer_norm.bias   torch.Size([512])\n",
      "decoder.decoder_layers.1.sublayer_wrappers.2.layer_norm.weight   torch.Size([512])\n",
      "decoder.decoder_layers.1.sublayer_wrappers.2.layer_norm.bias   torch.Size([512])\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.0.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.0.bias   torch.Size([512])\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.1.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.1.bias   torch.Size([512])\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.2.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.2.bias   torch.Size([512])\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.3.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.3.bias   torch.Size([512])\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.0.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.0.bias   torch.Size([512])\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.1.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.1.bias   torch.Size([512])\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.2.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.2.bias   torch.Size([512])\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.3.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.3.bias   torch.Size([512])\n",
      "decoder.decoder_layers.2.feed_forward.linear_layer_1.weight   torch.Size([2048, 512])\n",
      "decoder.decoder_layers.2.feed_forward.linear_layer_1.bias   torch.Size([2048])\n",
      "decoder.decoder_layers.2.feed_forward.linear_layer_2.weight   torch.Size([512, 2048])\n",
      "decoder.decoder_layers.2.feed_forward.linear_layer_2.bias   torch.Size([512])\n",
      "decoder.decoder_layers.2.sublayer_wrappers.0.layer_norm.weight   torch.Size([512])\n",
      "decoder.decoder_layers.2.sublayer_wrappers.0.layer_norm.bias   torch.Size([512])\n",
      "decoder.decoder_layers.2.sublayer_wrappers.1.layer_norm.weight   torch.Size([512])\n",
      "decoder.decoder_layers.2.sublayer_wrappers.1.layer_norm.bias   torch.Size([512])\n",
      "decoder.decoder_layers.2.sublayer_wrappers.2.layer_norm.weight   torch.Size([512])\n",
      "decoder.decoder_layers.2.sublayer_wrappers.2.layer_norm.bias   torch.Size([512])\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.0.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.0.bias   torch.Size([512])\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.1.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.1.bias   torch.Size([512])\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.2.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.2.bias   torch.Size([512])\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.3.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.3.bias   torch.Size([512])\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.0.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.0.bias   torch.Size([512])\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.1.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.1.bias   torch.Size([512])\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.2.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.2.bias   torch.Size([512])\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.3.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.3.bias   torch.Size([512])\n",
      "decoder.decoder_layers.3.feed_forward.linear_layer_1.weight   torch.Size([2048, 512])\n",
      "decoder.decoder_layers.3.feed_forward.linear_layer_1.bias   torch.Size([2048])\n",
      "decoder.decoder_layers.3.feed_forward.linear_layer_2.weight   torch.Size([512, 2048])\n",
      "decoder.decoder_layers.3.feed_forward.linear_layer_2.bias   torch.Size([512])\n",
      "decoder.decoder_layers.3.sublayer_wrappers.0.layer_norm.weight   torch.Size([512])\n",
      "decoder.decoder_layers.3.sublayer_wrappers.0.layer_norm.bias   torch.Size([512])\n",
      "decoder.decoder_layers.3.sublayer_wrappers.1.layer_norm.weight   torch.Size([512])\n",
      "decoder.decoder_layers.3.sublayer_wrappers.1.layer_norm.bias   torch.Size([512])\n",
      "decoder.decoder_layers.3.sublayer_wrappers.2.layer_norm.weight   torch.Size([512])\n",
      "decoder.decoder_layers.3.sublayer_wrappers.2.layer_norm.bias   torch.Size([512])\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.0.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.0.bias   torch.Size([512])\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.1.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.1.bias   torch.Size([512])\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.2.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.2.bias   torch.Size([512])\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.3.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.3.bias   torch.Size([512])\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.0.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.0.bias   torch.Size([512])\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.1.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.1.bias   torch.Size([512])\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.2.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.2.bias   torch.Size([512])\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.3.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.3.bias   torch.Size([512])\n",
      "decoder.decoder_layers.4.feed_forward.linear_layer_1.weight   torch.Size([2048, 512])\n",
      "decoder.decoder_layers.4.feed_forward.linear_layer_1.bias   torch.Size([2048])\n",
      "decoder.decoder_layers.4.feed_forward.linear_layer_2.weight   torch.Size([512, 2048])\n",
      "decoder.decoder_layers.4.feed_forward.linear_layer_2.bias   torch.Size([512])\n",
      "decoder.decoder_layers.4.sublayer_wrappers.0.layer_norm.weight   torch.Size([512])\n",
      "decoder.decoder_layers.4.sublayer_wrappers.0.layer_norm.bias   torch.Size([512])\n",
      "decoder.decoder_layers.4.sublayer_wrappers.1.layer_norm.weight   torch.Size([512])\n",
      "decoder.decoder_layers.4.sublayer_wrappers.1.layer_norm.bias   torch.Size([512])\n",
      "decoder.decoder_layers.4.sublayer_wrappers.2.layer_norm.weight   torch.Size([512])\n",
      "decoder.decoder_layers.4.sublayer_wrappers.2.layer_norm.bias   torch.Size([512])\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.0.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.0.bias   torch.Size([512])\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.1.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.1.bias   torch.Size([512])\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.2.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.2.bias   torch.Size([512])\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.3.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.3.bias   torch.Size([512])\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.0.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.0.bias   torch.Size([512])\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.1.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.1.bias   torch.Size([512])\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.2.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.2.bias   torch.Size([512])\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.3.weight   torch.Size([512, 512])\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.3.bias   torch.Size([512])\n",
      "decoder.decoder_layers.5.feed_forward.linear_layer_1.weight   torch.Size([2048, 512])\n",
      "decoder.decoder_layers.5.feed_forward.linear_layer_1.bias   torch.Size([2048])\n",
      "decoder.decoder_layers.5.feed_forward.linear_layer_2.weight   torch.Size([512, 2048])\n",
      "decoder.decoder_layers.5.feed_forward.linear_layer_2.bias   torch.Size([512])\n",
      "decoder.decoder_layers.5.sublayer_wrappers.0.layer_norm.weight   torch.Size([512])\n",
      "decoder.decoder_layers.5.sublayer_wrappers.0.layer_norm.bias   torch.Size([512])\n",
      "decoder.decoder_layers.5.sublayer_wrappers.1.layer_norm.weight   torch.Size([512])\n",
      "decoder.decoder_layers.5.sublayer_wrappers.1.layer_norm.bias   torch.Size([512])\n",
      "decoder.decoder_layers.5.sublayer_wrappers.2.layer_norm.weight   torch.Size([512])\n",
      "decoder.decoder_layers.5.sublayer_wrappers.2.layer_norm.bias   torch.Size([512])\n",
      "decoder.layer_norm.weight   torch.Size([512])\n",
      "decoder.layer_norm.bias   torch.Size([512])\n",
      "token_predictor.linear.weight   torch.Size([30000, 512])\n",
      "token_predictor.linear.bias   torch.Size([30000])\n"
     ]
    }
   ],
   "source": [
    "for key, value in translation_model.state_dict().items():\n",
    "    print(key, \" \", value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the correctness of Inference via Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_implementation.data_processing.data_preparation.data_helpers import get_tokenizers\n",
    "from model_implementation.model_building.machine_translation_model import MachineTranslationModel\n",
    "from model_implementation.model_inference.translator import translate\n",
    "from model_implementation.utils.constants import ( \n",
    "    DROPOUT_PROB, D_FEED_FORWARD, D_MODEL, FULL_EN_TE_DATASET_PATH, MAX_INPUT_SEQUENCE_LENGTH, NUM_HEADS, NUM_LAYERS\n",
    ")\n",
    "from model_implementation.utils.helpers import load_model_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_translation_model_from_disk(model_name: str, \n",
    "                                     src_vocab_size: int, \n",
    "                                     tgt_vocab_size: int, \n",
    "                                     checkpoint_prefix: str=\"\") -> MachineTranslationModel:\n",
    "    \"\"\"Loads the trained translation model from disk.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model to load from disk.\n",
    "\n",
    "    Returns:\n",
    "        MachineTranslationModel: Returns the trained machine translation model.\n",
    "    \"\"\"\n",
    "    translation_model = MachineTranslationModel(d_model=D_MODEL, \n",
    "                                                d_feed_forward=D_FEED_FORWARD,\n",
    "                                                dropout_prob=DROPOUT_PROB, \n",
    "                                                num_heads=NUM_HEADS, \n",
    "                                                src_vocab_size=src_vocab_size, \n",
    "                                                tgt_vocab_size=tgt_vocab_size, \n",
    "                                                num_layers=NUM_LAYERS, \n",
    "                                                max_seq_len=MAX_INPUT_SEQUENCE_LENGTH)\n",
    "    load_model_from_disk(model=translation_model, model_name=model_name, checkpoint_prefix=checkpoint_prefix)\n",
    "    return translation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MachineTranslationModel(\n",
       "  (src_embedding): Embeddings(\n",
       "    (look_up_table): Embedding(30000, 512)\n",
       "  )\n",
       "  (tgt_embedding): Embeddings(\n",
       "    (look_up_table): Embedding(30000, 512)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (self_attention): MultiHeadedAttention(\n",
       "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "          (linear_layers): ModuleList(\n",
       "            (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): FeedForwardNN(\n",
       "          (linear_layer_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear_layer_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer_wrappers): ModuleList(\n",
       "          (0-1): 2 x SubLayerWrapper(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-5): 6 x DecoderLayer(\n",
       "        (self_attention): MultiHeadedAttention(\n",
       "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "          (linear_layers): ModuleList(\n",
       "            (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (src_attention): MultiHeadedAttention(\n",
       "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "          (linear_layers): ModuleList(\n",
       "            (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): FeedForwardNN(\n",
       "          (linear_layer_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear_layer_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer_wrappers): ModuleList(\n",
       "          (0-2): 3 x SubLayerWrapper(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (token_predictor): TokenPredictor(\n",
       "    (linear): Linear(in_features=512, out_features=30000, bias=True)\n",
       "    (log_softmax): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_model = load_translation_model_from_disk(model_name=\"bpe_30k_en-te-model\", \n",
    "                                                     checkpoint_prefix=\"test_run_5_epoch_9\", \n",
    "                                                     src_vocab_size=30000, \n",
    "                                                     tgt_vocab_size=30000)\n",
    "translation_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenizer, telugu_tokenizer = get_tokenizers(dataset_relative_path=FULL_EN_TE_DATASET_PATH, \n",
    "                                                     tokenizer_type=\"bpe\", \n",
    "                                                     retrain_tokenizers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sentences_set_1 = [\"I am a Software Engineer at Google.\", \n",
    "                       \"How do I learn Machine Learing and start working on awesome ideas?\", \n",
    "                       \"Lets do a Masters in Data Science at good university.\", \n",
    "                       \"I watched The Boys tv show last week. It was awesome\"]\n",
    "\n",
    "src_sentences_set_2 = [\"Have you heard about Foie gras?\",\n",
    "                       \"I never thought of acting in films.\",\n",
    "                       \"Installed Software\",\n",
    "                       \"A case has been registered under Sections 302 and 376, IPC.\",\n",
    "                       \"Of this, 10 people succumbed to the injuries.\"]\n",
    "\n",
    "src_sentences_set_3 = [\"Have you heard about Foie gras?\",\n",
    "                       \"I never thought of acting in films.\",\n",
    "                       \"A case has been registered under Sections 302 and 376, IPC.\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_sentences = translate(translation_model=translation_model, \n",
    "                                 src_tokenizer=english_tokenizer, \n",
    "                                 tgt_tokenizer=telugu_tokenizer, \n",
    "                                 src_sentences=src_sentences_set_3, \n",
    "                                 beam_size=3,\n",
    "                                 search_type=\"beam\",\n",
    "                                 device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ఎందుకు ప్రారంభించారు?', 'అయితే ఈ సినిమాకి సంబంధించిన వివరాలు వెల్లడించలేదు.', 'పోలీసులు కేసు నమోదు చేశారు.']\n"
     ]
    }
   ],
   "source": [
    "print(translated_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug by running the entire training script on debug dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
