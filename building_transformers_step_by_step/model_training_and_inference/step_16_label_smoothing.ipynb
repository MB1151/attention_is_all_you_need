{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) What is label smoothing and how to use it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources to understand label smoothing:\n",
    "#\n",
    "# 1) https://www.youtube.com/watch?v=wmUiOAra_-M\n",
    "# 2) https://towardsdatascience.com/label-smoothing-make-your-model-less-over-confident-b12ea6f81a9a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL SMOOTHING:\n",
    "#   \n",
    "# Label smoothing is a regularization technique that is used to prevent the model from becoming too \n",
    "# confident about its predictions. We just add a small amount of noise to the labels.\n",
    "# In the case of classification, the target is a one-hot vector and looks like [0, 0, 1, 0, 0, 0] i.e.,\n",
    "# the probability of the correct class is 1 and the probability of all other classes is 0.\n",
    "# Label smoothing replaces the 1 with (1 - epsilon) and all other classes with (epsilon / (num_classes - 1)).\n",
    "# For example, if epsilon = 0.1 and num_classes = 6, then the target after Label Smoothing applied \n",
    "# will be [0.02, 0.02, 0.9, 0.02, 0.02, 0.02].\n",
    "#\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "# LABEL SMOOTHING IN TRANSFORMERS:\n",
    "#\n",
    "# In the target output for transformers, we also have a padding token. We don't want to apply label\n",
    "# smoothing to the padding token. So, we will only apply label smoothing to the non-padding tokens.\n",
    "# Lets say the target for a specific token is [0, 0, 0, 0, 1, 0] and the padding token is 2. In \n",
    "# other words, the expected output is 4 (some token in the 4th position in the vocabulary). Now, when\n",
    "# we apply Label Smoothing to this target, we will not share any of the 'smoothing' to the padding token.\n",
    "# We let the probability of the padding token to be 0 and the probability of the correct token to be\n",
    "# (1 - epsilon). The probability of all other tokens will be (epsilon / (num_classes - 2)). Note that we\n",
    "# subtract 2 because we are not considering the padding token and the correct token i.e., ignoring \n",
    "# two classes. In this case, the target after Label Smoothing applied will be \n",
    "# [0.025, 0.025, 0.0, 0.025, 0.9, 0.025]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "from typing import Optional\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the padding token or the class label for the padding token.\n",
    "padding_idx = 2\n",
    "# Amount of probability to be shared among the tokens excluding correct token and padding tokens.\n",
    "smoothing = 0.1\n",
    "# Amount of probability shared with the correct token.\n",
    "confidence = 1 - smoothing\n",
    "# Number of classes in the classification problem. It is the size of the vocabulary in transformers.\n",
    "# It includes the padding token.\n",
    "num_classes = 6\n",
    "# Number of sentences in the batch.\n",
    "batch_size = 2\n",
    "# Number of tokens in each sentence.\n",
    "seq_len = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 8])\n",
      "targets: \n",
      " tensor([[0, 3, 4, 5, 5, 1, 2, 2],\n",
      "        [1, 5, 3, 3, 4, 0, 0, 2]])\n"
     ]
    }
   ],
   "source": [
    "# Creating the target labels for 2 sentences each with 8 tokens and 6 possible classes for prediction \n",
    "# (including the padding token). This is compared to the output of the linear layer (TokenPredictor layer) \n",
    "# after the Decoder in the transformer.\n",
    "#\n",
    "# The class labels are the index of the correct class in the vocabulary.\n",
    "# targets[0][0] = 0 --> The correct class for the 0th token in sentence 1 is 0 i.e., some token in the 0th position in the target vocabulary.\n",
    "# targets[0][1] = 3 --> The correct class for the 1st token in sentence 1 is 3 i.e., some token in the 3th position in the target vocabulary.\n",
    "# ...\n",
    "# targets[0][7] = 2 --> The correct class for the 7th token in sentence 1 is 2 i.e., the padding token.\n",
    "# targets[1][0] = 1 --> The correct class for the 0th token in sentence 2 is 1 i.e., some token in the 5th position in the target vocabulary.\n",
    "# ...\n",
    "targets = torch.tensor(data=[[0, 3, 4, 5, 5, 1, 2, 2], [1, 5, 3, 3, 4, 0, 0, 2]], dtype=torch.int64)\n",
    "print(\"shape: \", targets.shape)\n",
    "print(\"targets: \\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 8, 6])\n",
      "smoothed_probs: \n",
      " tensor([[[0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the predicted probabilities to get the same shape as the predicted_probs.\n",
    "# We will use this copy to create the smoothed probabilities of the corresponding targets. \n",
    "smoothed_probs = torch.zeros(size=(batch_size, seq_len, num_classes), dtype=torch.float32)\n",
    "print(\"shape: \", smoothed_probs.shape)\n",
    "print(\"smoothed_probs: \\n\", smoothed_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 8, 6])\n",
      "smoothed_probs: \n",
      " tensor([[[0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250]],\n",
      "\n",
      "        [[0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250]]])\n"
     ]
    }
   ],
   "source": [
    "# We need to share the smoothing probability to the tokens (excluding correct token and padding token).\n",
    "# Lets first fill the entire tensor with the smoothing probability. We will deal with the correct token\n",
    "# and padding token probabilities later.\n",
    "smoothed_probs = smoothed_probs.fill_(value=smoothing / (num_classes - 2))\n",
    "print(\"shape: \", smoothed_probs.shape)\n",
    "print(\"smoothed_probs: \\n\", smoothed_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 8, 1])\n",
      "unsqueezed_targets: \n",
      " tensor([[[0],\n",
      "         [3],\n",
      "         [4],\n",
      "         [5],\n",
      "         [5],\n",
      "         [1],\n",
      "         [2],\n",
      "         [2]],\n",
      "\n",
      "        [[1],\n",
      "         [5],\n",
      "         [3],\n",
      "         [3],\n",
      "         [4],\n",
      "         [0],\n",
      "         [0],\n",
      "         [2]]])\n"
     ]
    }
   ],
   "source": [
    "# The targets tensor [[0, 3, 4, 5, 5, 1, 2, 2], [1, 5, 3, 3, 4, 0, 0, 2]] contains the correct class labels \n",
    "# (correspond to the tokens in the vocabulary) for each of the 8 tokens in each of the 2 sentences in the batch. \n",
    "# We will use the targets tensor to find the appropriate positions in the smoothed_probs tensor to update it\n",
    "# with the confidence probabilities. We will need to bring the targets tensor to contain the same number of\n",
    "# dimensions as the smoothed_probs tensor to use it with the scatter_ function in the next step to do this.\n",
    "unsqueezed_targets = targets.unsqueeze(dim=-1)\n",
    "print(\"shape: \", unsqueezed_targets.shape)\n",
    "print(\"unsqueezed_targets: \\n\", unsqueezed_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 8, 6])\n",
      "smoothed_probs: \n",
      " tensor([[[0.9000, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.9000, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.9000, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.9000],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.9000],\n",
      "         [0.0250, 0.9000, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.9000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.9000, 0.0250, 0.0250, 0.0250]],\n",
      "\n",
      "        [[0.0250, 0.9000, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.9000],\n",
      "         [0.0250, 0.0250, 0.0250, 0.9000, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.9000, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0250, 0.0250, 0.9000, 0.0250],\n",
      "         [0.9000, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.9000, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.9000, 0.0250, 0.0250, 0.0250]]])\n"
     ]
    }
   ],
   "source": [
    "# The smoothed_probs tensor need to be filled with the confidence probability at the positions that \n",
    "# correspond to the correct class labels (expected output tokens). \n",
    "#\n",
    "# From above, the correct class labels for sentence 1 in the batch are [0, 3, 4, 5, 5, 1, 2, 2].\n",
    "# - This means the right token for the zeroth token is in the 0th position (targets[0][0]) in the vocabulary. \n",
    "#   So, the 0th position in the smoothed_probs tensor should be filled with the confidence probability (0.9).\n",
    "# - Similarly, the right word for the first token is in the 3rd position (targets[0][1]) in the vocabulary. \n",
    "#   So, the 3rd position in the smoothed_probs tensor should be filled with the confidence probability (0.9).\n",
    "# - ... for all other tokens.\n",
    "# \n",
    "# The unsqueezed_targets tensor will act as the index tensor to replace the probabilities in the \n",
    "# smoothed_probs tensor. We will use the 'scatter_' function to accomplish this. \n",
    "# Refer to 'understanding_tensor_manipulations_part_4.ipynb' to understand torch.scatter_ function.\n",
    "smoothed_probs.scatter_(dim=-1, index=unsqueezed_targets, value=confidence)\n",
    "print(\"shape: \", smoothed_probs.shape)\n",
    "print(\"smoothed_probs: \\n\", smoothed_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 8, 6])\n",
      "smoothed_probs: \n",
      " tensor([[[0.9000, 0.0250, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.9000, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.9000, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.0250, 0.9000],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.0250, 0.9000],\n",
      "         [0.0250, 0.9000, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.0250, 0.0250]],\n",
      "\n",
      "        [[0.0250, 0.9000, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.0250, 0.9000],\n",
      "         [0.0250, 0.0250, 0.0000, 0.9000, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.9000, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.9000, 0.0250],\n",
      "         [0.9000, 0.0250, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.9000, 0.0250, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.0250, 0.0250]]])\n"
     ]
    }
   ],
   "source": [
    "# The padding token should not be predicted at all by the model. So, the probability associated with the\n",
    "# class label that correspond to the padding token within each target token distribution should be 0. So, \n",
    "# we set the probability at index 2 (padding_idx) to 0.\n",
    "smoothed_probs[:, :, padding_idx] = 0\n",
    "print(\"shape: \", smoothed_probs.shape)\n",
    "print(\"smoothed_probs: \\n\", smoothed_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 8, 6])\n",
      "mask: \n",
      " tensor([[[False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "# The target tensor is appended with the padding tokens at the end. These are just dummy tokens added to bring \n",
    "# all the sentences in the batch to the same length. We don't want the model to consider these tokens at all \n",
    "# in the loss calculation. So, we set the probabilities of the entire rows corresponding to the padding tokens\n",
    "# to 0.\n",
    "# \n",
    "# We repeat the target tensor 6 times (num_classes) along the last dimension to create a tensor of same shape\n",
    "# as the smoothed_probs tensor. We then use this tensor to create a mask tensor that is True for the rows \n",
    "# corresponding to the padding tokens and False for all other tokens.\n",
    "#\n",
    "# mask[0][6] and mask[0][7] are True because the 6th and 7th tokens in the 0th sentence are padding tokens.\n",
    "# mask[1][7] is True because the 7th token in the 1st sentence is a padding token.\n",
    "mask = unsqueezed_targets.repeat(1, 1, num_classes) == padding_idx\n",
    "print(\"shape: \", mask.shape)\n",
    "print(\"mask: \\n\", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 8, 6])\n",
      "smoothed_probs: \n",
      " tensor([[[0.9000, 0.0250, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.9000, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.9000, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.0250, 0.9000],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.0250, 0.9000],\n",
      "         [0.0250, 0.9000, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0250, 0.9000, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.0250, 0.9000],\n",
      "         [0.0250, 0.0250, 0.0000, 0.9000, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.9000, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.9000, 0.0250],\n",
      "         [0.9000, 0.0250, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.9000, 0.0250, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# We now use the mask to set the probabilities of the entire rows corresponding to the padding tokens to 0.\n",
    "# smoothed_probs[0][6] = 0.0\n",
    "# smoothed_probs[0][7] = 0.0\n",
    "# smoothed_probs[1][7] = 0.0\n",
    "# This will be used in the next step to calculate the loss.\n",
    "smoothed_probs = smoothed_probs.masked_fill(mask=mask, value=0.0)\n",
    "print(\"shape: \", smoothed_probs.shape)\n",
    "print(\"smoothed_probs: \\n\", smoothed_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the above steps into a module to be used in the transformer implementation.\n",
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size: int, padding_idx: int, smoothing: float=0.1):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        # Number of classes in the classification problem. It is the size of the vocabulary in transformers.\n",
    "        self.vocab_size = tgt_vocab_size\n",
    "        # Index of the padding token or the class label for the padding token. Usually set to 2.\n",
    "        self.padding_idx = padding_idx\n",
    "        # Amount of probability to be shared among the tokens excluding correct token and padding tokens.\n",
    "        self.smoothing = smoothing\n",
    "        # Amount of probability shared with the correct token.\n",
    "        self.confidence = 1 - smoothing\n",
    "    \n",
    "    def forward(self, targets: Tensor) -> Tensor:\n",
    "        \"\"\"Calculates the smoothed probabilities for each of the target tokens within each sentence.\n",
    "\n",
    "        Args:\n",
    "            targets (Tensor): The target tensor containing the correct class labels (expected token indices from the \n",
    "                              vocab) for each token in the batch. An example target tensor for a batch of 2 sentences\n",
    "                              each with 8 tokens and 6 possible classes for prediction (including the padding token)\n",
    "                              would be: [[0, 3, 4, 5, 5, 1, 2, 2], [1, 5, 3, 3, 4, 0, 0, 2]]\n",
    "                              SHAPE: [batch_size, tgt_seq_len - 1]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: A smoothed probability distribution (1D tensor) for each target token in the batch.\n",
    "                    SHAPE: [batch_size, tgt_seq_len - 1, vocab_size]                    \n",
    "        \"\"\"\n",
    "        # The above description showing the shape as (tgt_seq_len - 1) is because the first token is removed from the\n",
    "        # target tensor while calculating the loss. 'tgt_seq_len' variable here is the number of tokens in each \n",
    "        # target sequence in the batch before we removed the first token to form the expected decoder output. \n",
    "        # Don't get confused with the variable naming. Just ignore this explanation if it is confusing.\n",
    "        batch_size, tgt_seq_len = targets.shape\n",
    "        # Creating a tensor that will hold the smoothed probabilities for each target token in all the sentences.\n",
    "        smoothed_probs = torch.zeros(size=(batch_size, tgt_seq_len, self.vocab_size), dtype=torch.float32)\n",
    "        # Filling the entire tensor with the smoothing probability. We will deal with the probabilities of the\n",
    "        # correct token and padding token later.\n",
    "        smoothed_probs = smoothed_probs.fill_(value=self.smoothing / (self.vocab_size - 2))\n",
    "        # Bringing the targets tensor to contain the same number of dimensions as the smoothed_probs tensor to \n",
    "        # use it with the 'scatter_' function. This is to replace the probabilities in the smoothed_probs tensor \n",
    "        # for the padding token and the correct token in the following steps.\n",
    "        unsqueezed_targets = targets.unsqueeze(dim=-1)\n",
    "        # Replacing the probabilities in the smoothed_probs tensor with the confidence probability at the \n",
    "        # positions that correspond to the correct class labels (expected output tokens in the target).\n",
    "        smoothed_probs.scatter_(dim=-1, index=unsqueezed_targets, value=self.confidence)\n",
    "        # The padding token should not be predicted at all by the model. So, the probability associated with the\n",
    "        # class label that correspond to the padding token within each target token distribution should be 0. \n",
    "        smoothed_probs[:, :, self.padding_idx] = 0\n",
    "        # The target tensor is appended with the padding tokens at the end. These are just dummy tokens added to bring \n",
    "        # all the sentences in the batch to the same length. We don't want the model to consider these tokens at all \n",
    "        # in the loss calculation. So, we set the probabilities of the entire rows corresponding to the padding tokens\n",
    "        # to 0. More about why this setup works is explained in the next notebook 'step_17_loss_computation.ipynb'.\n",
    "        mask = unsqueezed_targets.repeat(1, 1, self.vocab_size) == self.padding_idx\n",
    "        return smoothed_probs.masked_fill(mask=mask, value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 8])\n",
      "transformer_targets: \n",
      " tensor([[0, 3, 4, 5, 5, 1, 2, 2],\n",
      "        [1, 5, 3, 3, 4, 0, 0, 2]])\n"
     ]
    }
   ],
   "source": [
    "transformer_targets = torch.tensor(data=[[0, 3, 4, 5, 5, 1, 2, 2], [1, 5, 3, 3, 4, 0, 0, 2]], dtype=torch.int64)\n",
    "print(\"shape: \", transformer_targets.shape)\n",
    "print(\"transformer_targets: \\n\", transformer_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelSmoothing()\n"
     ]
    }
   ],
   "source": [
    "label_smoothing = LabelSmoothing(tgt_vocab_size=num_classes, padding_idx=padding_idx, smoothing=smoothing)\n",
    "print(label_smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 8, 6])\n",
      "smoothed_probabilties: \n",
      " tensor([[[0.9000, 0.0250, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.9000, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.9000, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.0250, 0.9000],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.0250, 0.9000],\n",
      "         [0.0250, 0.9000, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0250, 0.9000, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.0250, 0.9000],\n",
      "         [0.0250, 0.0250, 0.0000, 0.9000, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.9000, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.9000, 0.0250],\n",
      "         [0.9000, 0.0250, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.9000, 0.0250, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "transformer_smoothed_probabilties = label_smoothing(targets=transformer_targets)\n",
    "print(\"shape: \", transformer_smoothed_probabilties.shape)\n",
    "print(\"smoothed_probabilties: \\n\", transformer_smoothed_probabilties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
