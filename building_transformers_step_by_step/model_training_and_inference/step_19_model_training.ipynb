{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn\n",
    "# \n",
    "# 1) How to train the MachineTranslation transformer model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datasets\n",
    "import math\n",
    "import spacy\n",
    "import torch\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from torch import nn, Tensor\n",
    "from typing import Optional, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Generator, Callable\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, we are going to combine the components from all the previous notebooks to train the MachineTranslation \n",
    "# transformer model. So, it's going to be a long notebook. You can skip the parts that are copied from the previous\n",
    "# notebooks and directly go to the training part. Also, in this notebook, we will use dummy data and avoid all the data\n",
    "# processing to keep it relatively smaller. We will use the in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id of the padding token.\n",
    "PAD_TOKEN_ID = 2\n",
    "# Id of the end of sentence token.\n",
    "EOS_ID = 1\n",
    "# Probability assigned to all other tokens (excluding correct token) while applying label smoothing.\n",
    "SMOOTHING_PROB = 0.1\n",
    "# Number of steps for which the learning rate increases linearly during training.\n",
    "NUM_WARMUP_STEPS = 400\n",
    "# Size of the word embeddings and all other vectors in the model.\n",
    "D_MODEL = 512\n",
    "# Number of tokens in the source (English) vocabulary.\n",
    "SRC_VOCAB_SIZE = 40000\n",
    "# Number of tokens in the target (Telugu) vocabulary.\n",
    "TGT_VOCAB_SIZE = 40000\n",
    "# Number of neurons in the hidden layer within the Feed Forward neural network of the transformer model.\n",
    "D_FEED_FORWARD = 2048\n",
    "# Number of heads in the attention layer of the transformer model.\n",
    "NUM_HEADS = 8\n",
    "# Maximum length of the sequence that is ever input to the model. Model will break if a larger sequence\n",
    "# is provided as input.\n",
    "MAX_SEQ_LEN = 250\n",
    "# Probability with which the neurons (or data) is dropped.\n",
    "DROPOUT_PROB = 0.1\n",
    "# Number of EncoderLayers in the encoder and number of DecoderLayers in the decoder.\n",
    "NUM_LAYERS = 6\n",
    "# Learning rate at the start of the model training. This is adjusted periodically during model training.\n",
    "INITIAL_LEARNING_RATE = 0.01\n",
    "# Hyperparameter to calculate the m1 moment in the optimizer. This roughly corresponds to averaging over the\n",
    "# last 10 (1/(1-beta_1)) sets of gradients. This comes from 'Gradient Descent with Momentum' algorithm.\n",
    "BETA_1 = 0.9\n",
    "# Hyperparameter to calculate the m1 moment in the optimizer. This roughly corresponds to averaging over the\n",
    "# last 50 (1/(1-beta_2)) sets of gradients. This comes from 'RMS prop' algorithm.\n",
    "BETA_2 = 0.98\n",
    "# Small value to avoid division by zero in the optimizer.\n",
    "EPSILON = 1e-8\n",
    "# Number of epochs to train the model on.\n",
    "NUM_EPOCHS = 1\n",
    "# Directory to save the model after every check point.\n",
    "MODEL_CHECK_POINTS_PATH = \"../../Data/trained_model_checkpoints\"\n",
    "# Path to the datasets.\n",
    "AI4_BHARAT_DATA_PATH = \"../../Data/AI4Bharat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVERYTHING IN THIS CELL HAS BEEN EXPLAINED IN DETAIL IN THE PREVIOUS NOTEBOOKS. PLEASE REFER TO THE EARLIER\n",
    "# NOTEBOOKS TO UNDERSTAND THE CODE IN THIS CELL. YOU CAN SKIP (JUST RUN IT BLINDLY) THIS CELL AND MOVE TO THE \n",
    "# NEXT CELL DIRECTLY IN THIS NOTEBOOK. \n",
    "\n",
    "# Refer to 'step_8_word_embeddings.ipynb' notebook to learn more about the Embeddings class.\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        \"\"\"Creates the embedding layer that serves as a look-up table for the tokens in the transformer model.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary i.e., number of distinct tokens in the vocabulary.\n",
    "            embedding_dim (int): The size of the embedding vector to be generated for each token.\n",
    "        \"\"\"\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.look_up_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    # The input is be a '2D' tensor where each '1D' tensor within the '2D' tensor is the list\n",
    "    # of indices corresponding to the tokens in the vocab.\n",
    "    # [[0, 123, 3455, 4556, 7, 1, 2, 2], [0, 56, 98, 6234, 909, 56, 1, 2]]\n",
    "    # 0 - <SOS>, 1 - <eos>, 2 - <pad>\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"Converts the input tensor of token indices to their corresponding embedding vectors.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): The input tensor of token indices.\n",
    "                            shape: [batch_size, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The tensor of embedding vectors for the corresponding input tokens.\n",
    "                    shape: [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        # There is no reasoning as to why the original 'attention_is_all_you_need' paper scaled the\n",
    "        # embeddings using 'math.sqrt(embedding_dim)'. A few blogs attempted to explain this reasoning,\n",
    "        # but I haven't found anything with solid reasoning.\n",
    "        return self.look_up_table(input) * math.sqrt(self.embedding_dim)\n",
    "\n",
    "\n",
    "# Refer to 'step_10_positional_encoding.ipynb' notebook to learn more about the PositionalEncoding class.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # d_model is the same as encoding_size.\n",
    "    def __init__(self, encoding_size: int, dropout_prob: float, max_len: int = 5000):\n",
    "        \"\"\"Creates the positional encodings.\n",
    "\n",
    "        Args:\n",
    "            encoding_size (int): Size of the positional encoding vector that represents the position of the token.\n",
    "            dropout_prob (float): Probability of an element to be zeroed or dropped.\n",
    "            max_len (int, optional): Largest position for which the positional encoding vector is generated. Defaults to 5000.\n",
    "                                     By default, it generates positional encodings for the first 5000 positions.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Refer to step_8_drop_out.ipynb notebook (link to the notebook) to understand more about dropout.\n",
    "        self.dropout = nn.Dropout(p=dropout_prob, inplace=False)\n",
    "        # Compute the positional encodings in log space.\n",
    "        positional_encoding = torch.zeros(size=(max_len, encoding_size), dtype=torch.float)\n",
    "        positional_encoding_numerators = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        numerators_in_exponent = torch.arange(0, encoding_size, 2, dtype=torch.float)\n",
    "        positional_encoding_denominators = torch.exp(numerators_in_exponent * (-math.log(10000.0) / encoding_size))\n",
    "        positional_encoding[:, 0::2] = torch.sin(positional_encoding_numerators * positional_encoding_denominators)\n",
    "        positional_encoding[:, 1::2] = torch.cos(positional_encoding_numerators * positional_encoding_denominators)\n",
    "        # Refer to understanding_tensor_manipulations_part_1.ipynb notebook (link to the notebook) to\n",
    "        # understand more about unsqueeze operation in pytorch.\n",
    "        # In transformer model, we receive 3D tensors as input to this module. Each 1D tensor\n",
    "        # in the last dimension is an embedding for the token. Each 2D tensor is a sentence.\n",
    "        # The entire 3D tensor is a batch of sentences. To work with 3D tensors in the forward\n",
    "        # method, we convert the positional encoding to a 3D tensor.\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        # Refer to using_modules.ipynb (link to the notebook) to understand more about buffers in pytorch.\n",
    "        # This tells the module to not update the positional encoding tensor during the training. It is \n",
    "        # not a trainable parameter but it is still part of the state of the model.\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "    \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"Adds the positional encodings to the input tensor.\n",
    "        Args:\n",
    "            input (Tensor): The input tensor containing the embeddings of the tokens.\n",
    "                            shape: [batch_size, sentence_length, d_model]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Input with the positional encodings added to it.\n",
    "                    shape: [batch_size, sentence_length, d_model]\n",
    "                    d_model is the same as encoding_size.\n",
    "        \"\"\"\n",
    "        # Refer to understanding_tensor_manipulations_part_5.ipynb notebook (link to the notebook) to \n",
    "        # understand more about broadcasting in python.\n",
    "        # The input tensor is a 3D tensor of shape (batch_size, sentence_length, encoding_size).\n",
    "        # We add (uses broadcasting) the positional encoding to the input tensor to get the final tensor.\n",
    "        # positional_encoding: (1, max_len, encoding_size) --> (1, sentence_length, encoding_size) \n",
    "        #       -- Extracts the positional encodings for the sentence_length from the positional_encoding \n",
    "        #          tensor.\n",
    "        # (batch_size, sentence_length, encoding_size) --> input\n",
    "        # (batch_size, sentence_length, encoding_size) --> Resultant tensor shape after broadcasting.\n",
    "        # requires_grad_(False) is not needed since the positional encoding is already registered\n",
    "        # as a Buffer and not a trainable parameter. It is just included for clarity.\n",
    "        input = input + self.positional_encoding[:, :input.size(1)].requires_grad_(False)\n",
    "        return self.dropout(input)\n",
    "    \n",
    "\n",
    "# Creates a copy (deepcopy) of the module and returns ModuleList containing the copies.\n",
    "def clone_module(module: nn.Module, num_clones: int) -> nn.ModuleList:\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(num_clones)])\n",
    "\n",
    "\n",
    "# Refer to 'step_11_multi_headed_attention.ipynb' notebook to understand how this function works.\n",
    "def construct_attention_heads(queries: Tensor, keys: Tensor, values: Tensor, mask: Optional[Tensor]=None, dropout_layer: Optional[nn.Module]=None) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Calculates the attention scores for each token in the sequence with every other token in the sequence.\n",
    "       Applues the mask if provided and then normalizes the scores using softmax. It then calculates the \n",
    "       attention heads for each token in the sequence.\n",
    "\n",
    "    Args:\n",
    "        queries (Tensor): [batch_size, num_heads, seq_len, d_k]\n",
    "        keys (Tensor): [batch_size, num_heads, seq_len, d_k]\n",
    "        values (Tensor): [batch_size, num_heads, seq_len, d_k]\n",
    "        mask (Optional[Tensor], optional): [batch_size, 1, seq_len, seq_len]. Defaults to None.\n",
    "        dropout_layer (Optional[nn.Module], optional): probability with which the values are dropped on dropout layer. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: Returns the attention heads and the attention scores.\n",
    "                               attention_heads: [batch_size, num_heads, seq_len, d_k]\n",
    "                               attention_scores: [batch_size, num_heads, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    # Size of the vectors for each token for each head in the sequence.\n",
    "    d_k = queries.shape[-1]\n",
    "    # Calculate the attention scores for each token in the sequence with every other token in the sequence.\n",
    "    attention_scores = torch.matmul(queries, keys.transpose(dim0=2, dim1=3)) / math.sqrt(d_k)\n",
    "    # Mask the attention scores if a mask is provided. Mask is used in two different ways:\n",
    "    # 1) To prevent the model from attending to the padding tokens --> This applies for both src and tgt sentences.\n",
    "    # 2) To prevent the model from attending to the future tokens in the sequence --> This applies only for tgt sentences.\n",
    "    if mask is not None:\n",
    "        # Please do not set the masked values to float('-inf') as it sometimes (not in everycase) causes softmax to return nan.\n",
    "        attention_scores = attention_scores.masked_fill(mask == False, float('-1e9'))\n",
    "    # Normalize the attention scores using softmax.\n",
    "    attention_scores = attention_scores.softmax(dim=-1)\n",
    "    # Apply dropout regularization to prevent overfitting problems.\n",
    "    if dropout_layer is not None:\n",
    "        dropout_layer(attention_scores)\n",
    "    # Calculate the attention heads for each token in the sequence. The head for each token is calculated by\n",
    "    # taking the weighted average (averaged by attention scores) of the values for all the tokens in the \n",
    "    # sequence for the token of interest.\n",
    "    attention_heads = torch.matmul(attention_scores, values)\n",
    "    return attention_heads, attention_scores\n",
    "\n",
    "\n",
    "# Refer to 'step_11_multi_headed_attention.ipynb' notebook to understand how this class works.\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_model: int, dropout_prob: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads.\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        # We use dropout to prevent overfitting.\n",
    "        self.dropout_layer = nn.Dropout(p=dropout_prob)\n",
    "        # Creating the linear layers that generate queries, keys and values for each token in the sequence.\n",
    "        # Also, creating an additional linear layer to generate the output of the Multi-Headed Attention from concatenated attention heads.\n",
    "        self.linear_layers = clone_module(module=nn.Linear(in_features=d_model, out_features=d_model), num_clones=4)\n",
    "\n",
    "\n",
    "    def forward(self, query_input: Tensor, key_input: Tensor, value_input: Tensor, mask: Optional[Tensor]=None) -> Tensor:\n",
    "        \"\"\"Forward pass of the Multi-Headed Attention layer. \n",
    "\n",
    "        Args:\n",
    "            query (Tensor): Input to be used for query creation.\n",
    "                            query_input: [batch_size, seq_len, d_model]\n",
    "            key (Tensor): Input to be used for key creation.\n",
    "                          key_input  : [batch_size, seq_len, d_model]\n",
    "            value (Tensor): Input to be used for value creation.\n",
    "                            value_input: [batch_size, seq_len, d_model]\n",
    "            mask (Tensor): Mask to be applied to the attention scores. Default is None. Same mask will \n",
    "                           be applied to all the heads in the Multi-Headed Attention layer.\n",
    "                           mask: [batch_size, 1, seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Mutli-Headed Attention Output: Output of the Multi-Headed Attention layer. Generates one output vector \n",
    "                                           for each token in the sequence. Does this for each sequence in the batch.\n",
    "                                           output: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Generates the queries, keys and values for each token in the sequence.\n",
    "        # shape of queries, keys, values: [batch_size, seq_len, d_model]\n",
    "        queries, keys, values = [linear_layer(input) for linear_layer, input in zip(self.linear_layers, (query_input, key_input, value_input))]\n",
    "        batch_size = query_input.shape[0]\n",
    "        seq_len = query_input.shape[1]\n",
    "        # Separating the queries, keys and values for each head into a separate vector. The vectors for each token in all the heads\n",
    "        # are concatenated when they are created using the linear_layers above.\n",
    "        # Shape for queries, keys, values after view: [batch_size, seq_len, num_heads, d_k]\n",
    "        # Shape for queries, key, values after transpose: [batch_size, num_heads, seq_len, d_k]\n",
    "        queries, keys, values = [data.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(dim0=1, dim1=2) for data in (queries, keys, values)]\n",
    "        # Calculate the attention heads for each token in the sequence.\n",
    "        # attention_heads: [batch_size, num_heads, seq_len, d_k]\n",
    "        attention_heads, attention_scores = construct_attention_heads(queries=queries, keys=keys, values=values, mask=mask, dropout_layer=self.dropout_layer)\n",
    "        # Concatenate the attention heads for each token from all the heads.\n",
    "        # attention_heads: [batch_size, seq_len, d_model]\n",
    "        attention_heads = attention_heads.transpose(dim0=1, dim1=2).reshape(batch_size, seq_len, self.d_model)\n",
    "        # Generate the output of the Multi-Headed Attention layer.\n",
    "        return self.linear_layers[-1](attention_heads)\n",
    "    \n",
    "\n",
    "# Refer to 'step_12_feed_forward_neural_network.ipynb' notebook to understand how this class works.\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, d_model: int, d_feed_forward: int, dropout_prob: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear_layer_1 = nn.Linear(in_features=d_model, out_features=d_feed_forward)\n",
    "        self.linear_layer_2 = nn.Linear(in_features=d_feed_forward, out_features=d_model)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"Passes the input through the Feed Forward Neural Network and returns the output \n",
    "           of the neural network.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): The output of the Multi-Headed Attention layer.\n",
    "                            shape: [batch_size, seq_len, d_model]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output of the Feed Forward Neural Network.\n",
    "                    shape: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # We first expand the input to higher dimension. We apply the ReLU activation function in this layer.\n",
    "        intermediate_output = self.linear_layer_1(input).relu()\n",
    "        # Dropout layer to prevent overfitting\n",
    "        intermediate_output = self.dropout_layer(intermediate_output)\n",
    "        # We then compress the input back to its original dimension. There is no specific intuitive explanation \n",
    "        # as to why this is done. It is just shown to be working practically in neural networks in general and \n",
    "        # in this paper in particular.\n",
    "        return self.linear_layer_2(intermediate_output)\n",
    "    \n",
    "\n",
    "# Refer to 'step_14_encoder.ipynb' to understand how this class works.\n",
    "class SubLayerWrapper(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout_prob: float):\n",
    "        \"\"\"This class is a wrapper around the MultiHeadedAttention and PositionwiseFeedForward classes.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimension of the vectors used in the Attention model.\n",
    "            dropout_prob (float): probability with which nodes can be dropped.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, input: Tensor, sublayer: nn.Module) -> Tensor:\n",
    "        \"\"\"It applies the operation on the input, applies dropout, adds the input back to the transformed \n",
    "           input, does normalization and returns the output.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Input to be transformer by the sublayer.\n",
    "                            input: [batch_size, seq_len, d_model]\n",
    "            sublayer (nn.Module): sublayer could be either MultiHeadedAttention or PositionwiseFeedForward.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: Output of the sublayer transformation.\n",
    "                    output: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        return self.layer_norm(input + self.dropout(sublayer(input)))\n",
    "\n",
    "\n",
    "# Refer to 'step_14_encoder.ipynb' to understand how this class works.\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, self_attention: MultiHeadedAttention, feed_forward: FeedForwardNN, d_model: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout_prob = dropout_prob\n",
    "        # These modules are now the child modules of the EncoderLayer and will be registered as parameters of the EncoderLayer.\n",
    "        self.self_attention = self_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer_wrappers = clone_module(module=SubLayerWrapper(d_model=self.d_model, dropout_prob=self.dropout_prob), num_clones=2)\n",
    "\n",
    "    def forward(self, input: Tensor, mask: Tensor) -> Tensor:\n",
    "        \"\"\"This method is the forward pass of the EncoderLayer class.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Source sentence provided as input to the EncoderLayer. These are the embeddings of the source \n",
    "                            sentence for the first EncoderLayer.\n",
    "                            SHAPE: [batch_size, seq_len, d_model]\n",
    "            mask (Tensor): Boolean mask to be applied to the input during attention scores calculation.\n",
    "                           SHAPE: [batch_size, 1, seq_len, seq_len]\n",
    "        Returns:\n",
    "            Tensor: Output of the EncoderLayer.\n",
    "                    SHAPE: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # We are just saving the function call to the self_attention method in a variable and passing the\n",
    "        # lambda function (contained within the variable) to the sublayer_wrappers[0] to execute it when \n",
    "        # needed.\n",
    "        output = self.sublayer_wrappers[0](input, lambda input: self.self_attention(query_input=input, key_input=input, value_input=input, mask=mask))\n",
    "        return self.sublayer_wrappers[1](output, self.feed_forward)\n",
    "\n",
    "\n",
    "# Refer to 'step_14_encoder.ipynb' to understand how this class works.\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_layer: EncoderLayer, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = clone_module(module=encoder_layer, num_clones=num_layers)\n",
    "        self.layer_norm = nn.LayerNorm(encoder_layer.d_model)\n",
    "\n",
    "    def forward(self, input: Tensor, mask: Optional[Tensor]=None) -> Tensor:\n",
    "        \"\"\"This method is the forward pass of the Encoder class. The output of the current EncoderLayer is\n",
    "           passed as input to the next EncoderLayer. We have 6 identical EncoderLayers stacked on top of \n",
    "           each other. The output of the last EncoderLayer is passed through a Layer Normalization layer\n",
    "           and returned as the final output of the Encoder\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Input to the Encoder i.e., embeddings of the tokenized src sequences.\n",
    "                            input: [batch_size, seq_len, d_model]\n",
    "            mask (Optional[Tensor], optional): Boolean mask to be applied during attention scores calculation.\n",
    "                                               mask: [batch_size, 1, seq_len, seq_len]. Defaults to None.\n",
    "                            \n",
    "        Returns:\n",
    "            Tensor: Output of the Encoder i.e., encoded src sentences.\n",
    "                    output: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        output = input\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            # Pass the output of the previous EncoderLayer to the current EncoderLayer.\n",
    "            output = encoder_layer(input=output, mask=mask)\n",
    "        return self.layer_norm(output)\n",
    "\n",
    "\n",
    "# Refer to 'step_15_decoder.ipynb' to understand how this class works.\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, self_attention: MultiHeadedAttention, src_attention: MultiHeadedAttention, feed_forward: FeedForwardNN, d_model: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout_prob = dropout_prob\n",
    "        # These modules are now the child modules of the DecoderLayer and will be registered as parameters of the DecoderLayer.\n",
    "        self.self_attention = self_attention\n",
    "        self.src_attention = src_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer_wrappers = clone_module(module=SubLayerWrapper(d_model=d_model, dropout_prob=dropout_prob), num_clones=3)\n",
    "\n",
    "    def forward(self, input: Tensor, encoded_src: Tensor, tgt_mask: Tensor, src_mask: Optional[Tensor]=None) -> Tensor:\n",
    "        \"\"\"This method is the forward pass of the DecoderLayer class.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Target sentence provided as input to the DecoderLayer. These are the embeddings of the target \n",
    "                            sentence for the first DecoderLayer.\n",
    "                            SHAPE: [batch_size, seq_len, d_model]\n",
    "            encoded_src (Tensor): Encoded source sentence. This is the output of the Encoder. This is used to calculate the\n",
    "                                  source attention scores for the target sentence. \n",
    "                                  SHAPE: [batch_size, seq_len, d_model] \n",
    "            tgt_mask (Tensor): Mask to prevent the future tokens in the target sentence to attend to the previous tokens and\n",
    "                               also to prevent padding tokens from attending to any other token except other padding tokens.\n",
    "                               SHAPE: [batch_size, 1, seq_len, seq_len]\n",
    "            src_mask (Optional[Tensor], optional): Mask to prevent the the padding tokens to attend to the tokens in the tgt sentence. \n",
    "                                                   Defaults to None.\n",
    "                                                   SHAPE: [batch_size, 1, seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Returns the output of the DecoderLayer. This is the output of the Positionwise FeedForward Neural Network.\n",
    "                    SHAPE: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # First sublayer: Self-Attention on the target sentence. Hence, it uses the tgt_mask.\n",
    "        self_attention_output = self.sublayer_wrappers[0](input=input, sublayer=lambda input: self.self_attention(query_input=input, key_input=input, value_input=input, mask=tgt_mask)) \n",
    "        # To give intuition about src_attention, I have a query for a token in the target sentence. I want to know whether \n",
    "        # some token in the source sentence is important for me to predict the output for the token in the target sentence. \n",
    "        # So, I go to the source sentence and get the values for all the tokens in the source sentence. I then calculate \n",
    "        # the attention scores between the query (in tgt) and the keys (in src). I then calculate the attention heads for \n",
    "        # the token in the target sentence using the attention scores. This is what is done in the below line. Note that \n",
    "        # referring to statement 'the keys and values are from the source' doesn't mean that you get keys and values \n",
    "        # explicitly. It means we use the encoded data from the source sentence to calculate the queries and keys for \n",
    "        # this transformation.\n",
    "        # Second sublayer: Attention on the source sentence. Hence, it uses the src_mask.\n",
    "        src_attention_output = self.sublayer_wrappers[1](input=self_attention_output, sublayer=lambda self_attention_output: self.src_attention(query_input=self_attention_output, key_input=encoded_src, value_input=encoded_src, mask=src_mask))\n",
    "        # Third sublayer: Positionwise FeedForward Neural Network\n",
    "        return self.sublayer_wrappers[2](input=src_attention_output, sublayer=self.feed_forward)\n",
    "\n",
    "\n",
    "# Refer to 'step_15_decoder.ipynb' to understand how this class works.\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_layer: DecoderLayer, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.decoder_layers = clone_module(module=decoder_layer, num_clones=num_layers)\n",
    "        self.layer_norm = nn.LayerNorm(decoder_layer.d_model)\n",
    "\n",
    "    def forward(self, input: Tensor, encoded_src: Tensor, tgt_mask: Tensor, src_mask: Optional[Tensor]=None) -> Tensor:\n",
    "        \"\"\"This method is the forward pass of the Decoder class. The output of the current DecoderLayer is\n",
    "           passed as input to the next DecoderLayer. We have 6 identical DecoderLayers stacked on top of \n",
    "           each other. The output of the Encoder (last EncoderLayer) is also passed as input to the \n",
    "           first DecoderLayer. The output of the last DecoderLayer is passed through a Layer Normalization \n",
    "           layer and returned as the final output of the Decoder.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Input to the Decoder i.e., embeddings of the tokenized tgt sequences.\n",
    "                            input: [batch_size, seq_len, d_model]\n",
    "            encoded_src (Tensor): output of the encoder i.e., encoded src sequences.\n",
    "            tgt_mask (Tensor): Boolean mask to be applied during self attention scores calculation.\n",
    "                               tgt_mask: [batch_size, 1, seq_len, seq_len].\n",
    "            src_mask (Optional[Tensor], optional): Boolean mask to be applied during src attention scores calculation.\n",
    "                                                   tgt_mask: [batch_size, 1, seq_len, seq_len]. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output of the Decoder.\n",
    "                    output: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        output = input\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            # Pass the output of the previous DecoderLayer to the current DecoderLayer.\n",
    "            output = decoder_layer(input=output, encoded_src=encoded_src, tgt_mask=tgt_mask, src_mask=src_mask)\n",
    "        return self.layer_norm(output)\n",
    "\n",
    "\n",
    "# Refer to 'step_14_token_predictor.ipynb' to understand how this class works.\n",
    "class TokenPredictor(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super(TokenPredictor, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.linear = nn.Linear(in_features=d_model, out_features=vocab_size)\n",
    "        # The non-module variables are not added to the list of parameters of the model.\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, decoder_output: Tensor) -> Tensor:\n",
    "        \"\"\"The forward pass of the token predictor. Calculates the probability distribution over the \n",
    "           vocabulary. Each token vector has a corresponding probability distribution over the \n",
    "           vocabulary since we predict one token per output.\n",
    "\n",
    "        Args:\n",
    "            decoder_output (Tensor): Output of the Decoder.\n",
    "                                     shape: [batch_size, seq_len, d_model]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Log probability distribution over the vocabulary. \n",
    "                    shape: [batch_size, seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        # Project the decoder output to the vocab_size dimensional space.\n",
    "        logits = self.linear(decoder_output)\n",
    "        # Convert the logits to a probability distribution over the vocabulary. All the entires in the\n",
    "        # output tensor are negative since we are using log softmax. The log softmax is used to make\n",
    "        # the training more numerically stable. However, the maximum value is still the same as the \n",
    "        # maximum value of the original softmax output.\n",
    "        return self.log_softmax(logits)\n",
    "    \n",
    "\n",
    "# Refer to 'step_17_translation_transformer.ipynb' notebook to understand how this class works.\n",
    "class MachineTranslationTransformer(nn.Module):\n",
    "    \"\"\"Model that combines the Encoder, Decoder and the TokenPredictor to create a machine translation Transformer model.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_feed_forward: int, dropout_prob: float, num_heads: int, src_vocab_size: int, tgt_vocab_size: int, num_layers: int, max_seq_len: int):\n",
    "        \"\"\"Initializes the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): size of the embedding vectors in the model.\n",
    "            d_feed_forward (int): Number of neurons in the hidden layer of the feed forward neural network.\n",
    "            dropout_prob (float): probability with which to drop data in the transformer model.\n",
    "            num_heads (int): number of attention heads in each of the multi-head attention layers in the model.\n",
    "            src_vocab_size (int): size of the source vocabulary.\n",
    "            tgt_vocab_size (int): size of the target vocabulary.\n",
    "            num_layers (int): number of layers in the Encoder and Decoder.\n",
    "            max_seq_len (int): Maximum length of the sequence that is ever input to the model.\n",
    "        \"\"\"\n",
    "        super(MachineTranslationTransformer, self).__init__()\n",
    "        self.src_embedding = Embeddings(vocab_size=src_vocab_size, embedding_dim=d_model)\n",
    "        self.tgt_embedding = Embeddings(vocab_size=tgt_vocab_size, embedding_dim=d_model)\n",
    "        self.positional_encoding = PositionalEncoding(encoding_size=d_model, dropout_prob=dropout_prob, max_len=max_seq_len)\n",
    "        multi_headed_attention = MultiHeadedAttention(num_heads=num_heads, d_model=d_model, dropout_prob=dropout_prob)\n",
    "        feed_forward_nn = FeedForwardNN(d_model=d_model, d_feed_forward=d_feed_forward, dropout_prob=dropout_prob)\n",
    "        encoder_layer = EncoderLayer(self_attention=copy.deepcopy(multi_headed_attention), \n",
    "                                     feed_forward=copy.deepcopy(feed_forward_nn), \n",
    "                                     d_model=d_model, dropout_prob=dropout_prob)\n",
    "        decoder_layer = DecoderLayer(self_attention=copy.deepcopy(multi_headed_attention), \n",
    "                                     src_attention=copy.deepcopy(multi_headed_attention),\n",
    "                                     feed_forward=copy.deepcopy(feed_forward_nn), \n",
    "                                     d_model=d_model, dropout_prob=dropout_prob)\n",
    "        self.encoder = Encoder(encoder_layer=encoder_layer, num_layers=num_layers)\n",
    "        self.decoder = Decoder(decoder_layer=decoder_layer, num_layers=num_layers)\n",
    "        self.output_generator = TokenPredictor(d_model=d_model, vocab_size=tgt_vocab_size)\n",
    "        self.initialize_model_parameters()\n",
    "\n",
    "    def initialize_model_parameters(self):\n",
    "        \"\"\"Initializes the parameters of the model using the Xavier Uniform initialization.\"\"\"\n",
    "        for params in self.parameters():\n",
    "            # This is to ensure the only the weights are initialized and not the biases. biases usually have only\n",
    "            # one dimension and the weights have more than one dimension.\n",
    "            if params.dim() > 1:\n",
    "                nn.init.xavier_uniform_(params)\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor, tgt_mask: Tensor) -> Tensor:\n",
    "        \"\"\"The forward pass of the Transformer model. The source sentences are passed through the Encoder and the target\n",
    "           sentences are passed through the Decoder. The output of the Decoder is passed through the token predictor to\n",
    "           get the probability distribution over the target vocabulary.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Source sentences (English) containing the token ids corresponding to the indices in the src vocabulary. \n",
    "                          Example input looks like [[0, 4, 55, 67, 1, 2, 2], [0, 42, 585, 967, 19, 26, 1]]\n",
    "                          SHAPE: [batch_size, seq_len]\n",
    "            tgt (Tensor): Target sentences (Telugu) containing the token ids corresponding to the indices in the src vocabulary. \n",
    "                          Example input looks like [[0, 3, 5, 677, 81, 1, 2], [0, 7, 67, 190, 3245, 1]]\n",
    "                          SHAPE: [batch_size, seq_len]\n",
    "            src_mask (Tensor): Mask to be applied to the source sentences in each of the attention heads.\n",
    "                               src_mask: [batch_size, 1, seq_len, seq_len]\n",
    "            tgt_mask (Tensor): Mask to be applied to the target sentences in each of the attention heads.\n",
    "                               tgt_mask: [batch_size, 1, seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Log probability distribution over the tokens in the target vocabulary (Telugu vocabulary).\n",
    "                    output: [batch_size, seq_len, tgt_vocab_size]\n",
    "        \"\"\"\n",
    "        # Pass the source sentences through the encoder to get the encoded source token vectors.\n",
    "        encoded_src = self.encode(src=src, src_mask=src_mask)\n",
    "        # Pass the target sentence through the decoder to get the encoded target token vectors.\n",
    "        decoded_tgt = self.decode(tgt=tgt, tgt_mask=tgt_mask, encoded_src=encoded_src, src_mask=src_mask)\n",
    "        return self.generate_tgt_token_prob_distributions(decoded_tgt=decoded_tgt)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"Encodes the source sentences (English).\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): A batch of source sentences containing the token ids corresponding to the indices in the src vocabulary.\n",
    "                          SHAPE: [batch_size, seq_len]\n",
    "            src_mask (Tensor): Mask to be applied to the source sentences in each of the attention heads. Same mask will be \n",
    "                               applied to the sentence in all the attention heads.\n",
    "                               SHAPE: [batch_size, 1, seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Encoded source sentences. Each token in the source sentence is represented by a vector that encodes\n",
    "                    all the information about the token and its relationship with other tokens in the sentence.\n",
    "                    SHAPE: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Get the embeddings for the source sentences.\n",
    "        src_embeddings = self.src_embedding(src)\n",
    "        # Add the positional encodings to the embeddings.\n",
    "        src_embeddings = self.positional_encoding(src_embeddings)\n",
    "        # Pass the source sentence through the encoder.\n",
    "        encoded_src = self.encoder(input=src_embeddings, mask=src_mask)\n",
    "        return encoded_src\n",
    "\n",
    "    def decode(self, tgt: Tensor, tgt_mask: Tensor, encoded_src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"Encodes the target sentences (Telugu).\n",
    "\n",
    "        Args:\n",
    "            tgt (Tensor): A batch of target sentences containing the token ids corresponding to the indices in the tgt vocabulary.\n",
    "                          SHAPE: [batch_size, seq_len]\n",
    "            tgt_mask (Tensor): Mask to be applied to the target sentences in each of the attention heads. Same mask will be \n",
    "                               applied to the sentence in all the attention heads.\n",
    "                               SHAPE: [batch_size, 1, seq_len, seq_len]\n",
    "            encoded_src (Tensor): The encoded token representations of the source sentences. This is used to calculate the\n",
    "                                  source attention scores for the target sentence.\n",
    "                                  SHAPE: [batch_size, seq_len, d_model]\n",
    "            src_mask (Tensor): Mask to be applied to the source sentences in each of the attention heads. Same mask will be \n",
    "                               applied to the sentence in all the attention heads.\n",
    "                               SHAPE: [batch_size, 1, seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Encoded (or Decoded if that makes more sense) target sentences. Each token in the target sentence is \n",
    "                    represented by a vector that encodes all the information about the token and its relationship with\n",
    "                    other tokens in the target sentence and the corresponding source sentence.\n",
    "        \"\"\"\n",
    "        # Get the embeddings for the target sentences.\n",
    "        tgt_embeddings = self.tgt_embedding(tgt)\n",
    "        # Add the positional encodings to the embeddings.\n",
    "        tgt_embeddings = self.positional_encoding(tgt_embeddings)\n",
    "        # Pass the target sentence through the decoder.\n",
    "        decoded_tgt = self.decoder(input=tgt_embeddings, encoded_src=encoded_src, tgt_mask=tgt_mask, src_mask=src_mask)\n",
    "        return decoded_tgt\n",
    "\n",
    "    def generate_tgt_token_prob_distributions(self, decoded_tgt: Tensor) -> Tensor:\n",
    "        # Convert the output of the decoder to the probability distribution over the target vocabulary. This will be\n",
    "        # used to calculate the loss in the training phase.\n",
    "        return self.output_generator(decoded_tgt)\n",
    "\n",
    "\n",
    "# Refer to 'step_16_label_smoothing.ipynb' notebook to understand how this class works.\n",
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, vocab_size: int, padding_idx: int, smoothing: Optional[int]=0.1):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        # Number of classes in the classification problem. It is the size of the vocabulary in transformers.\n",
    "        self.vocab_size = vocab_size\n",
    "        # Index of the padding token or the class label for the padding token.\n",
    "        self.padding_idx = padding_idx\n",
    "        # Amount of probability to be shared among the tokens excluding correct token and padding tokens.\n",
    "        self.smoothing = smoothing\n",
    "        # Amount of probability shared with the correct token.\n",
    "        self.confidence = 1 - smoothing\n",
    "    \n",
    "    def forward(self, targets: Tensor) -> Tensor:\n",
    "        \"\"\"Calculates the smoothed probabilities for each of the target tokens within each sentence.\n",
    "\n",
    "        Args:\n",
    "            targets (Tensor): The target tensor containing the correct class labels (expected token indices from the \n",
    "                              vocab) for each token in the batch. An example target tensor for a batch of 2 sentences\n",
    "                              each with 8 tokens and 6 possible classes for prediction (including the padding token)\n",
    "                              would be: [[0, 3, 4, 5, 5, 1, 2, 2], [1, 5, 3, 3, 4, 0, 0, 2]]\n",
    "                              shape: [batch_size, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: A smoothed probability distribution (1D tensor) for each target token in the batch.\n",
    "                    shape: [batch_size, seq_len, vocab_size]                    \n",
    "        \"\"\"\n",
    "        batch_size, seq_len = targets.shape\n",
    "        # Creating a tensor that will hold the smoothed probabilities for each target token in all the sentences.\n",
    "        smoothed_probs = torch.zeros(size=(batch_size, seq_len, self.vocab_size), dtype=torch.float32)\n",
    "        # Filling the entire tensor with the smoothing probability. We will deal with the probabilities of the\n",
    "        # correct token and padding token later.\n",
    "        smoothed_probs = smoothed_probs.fill_(value=self.smoothing / (self.vocab_size - 2))\n",
    "        # Bringing the targets tensor to contain the same number of dimensions as the smoothed_probs tensor to use\n",
    "        # it with the scatter_ function inorder replace the probabilities in the smoothed_probs tensor in the next \n",
    "        # step.\n",
    "        unsqueezed_targets = targets.unsqueeze(dim=-1)\n",
    "        # Replacing the probabilities in the smoothed_probs tensor with the confidence probability at the \n",
    "        # positions that correspond to the correct class labels (expected output tokens in the target).\n",
    "        smoothed_probs.scatter_(dim=-1, index=unsqueezed_targets, value=self.confidence)\n",
    "        # The padding token should not be predicted at all by the model. So, the probability associated with the\n",
    "        # class label that correspond to the padding token within each target token distribution should be 0. \n",
    "        smoothed_probs[:, :, self.padding_idx] = 0\n",
    "        # The target tensor is appended with the padding tokens at the end. These are just dummy tokens added to bring \n",
    "        # all the sentences in the batch to the same length. We don't want the model to consider these tokens at all \n",
    "        # in the loss calculation. So, we set the probabilities of the entire rows corresponding to the padding tokens\n",
    "        # to 0.\n",
    "        mask = unsqueezed_targets.repeat(1, 1, self.vocab_size) == self.padding_idx\n",
    "        return smoothed_probs.masked_fill(mask=mask, value=0.0)\n",
    "\n",
    "\n",
    "# Refer to 'step_19_loss_computation.ipynb' notebook to understand how this class works.\n",
    "class LossCompute:\n",
    "    def __init__(self):\n",
    "        # We use the 'sum' reduction to sum the KL Divergence over all the tokens in all the sentences in the batch. \n",
    "        # The loss is then averaged over all the tokens in the batch to find the loss per token which is used as the \n",
    "        # objective function.         \n",
    "        self.kl_div_loss = nn.KLDivLoss(reduction=\"sum\")\n",
    "\n",
    "    # The '__call__' method allows an object of the class to be called just like a function.\n",
    "    def __call__(self, log_predictions: Tensor, targets: Tensor, num_non_pad_tokens: int) -> Tensor:\n",
    "        \"\"\"Computes the KL Divergence loss for the model predictions and the target tensor.\n",
    "\n",
    "        Args:\n",
    "            log_predictions (Tensor): The log of the model predictions for the target tokens in the batch.\n",
    "                                      Each token has a probability distribution over the vocabulary.\n",
    "                                      shape: [batch_size, seq_len, vocab_size]\n",
    "            targets (Tensor): The expected target for the model predictions. The target tensor is a smoothed\n",
    "                              probability distribution over the vocabulary for each token in the batch. \n",
    "                              shape: [batch_size, seq_len, vocab_size]\n",
    "            num_non_pad_tokens (int): The number of non-pad tokens in the target of the batch.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The KL Divergence per token in the batch which is used as the objective function for model\n",
    "                    training.\n",
    "        \"\"\"\n",
    "        # Calculates the KL Divergence loss between the model predictions and the targets.\n",
    "        kl_div_loss = self.kl_div_loss(input=log_predictions, target=targets)\n",
    "        # Calculate the KL Divergence loss per token in the batch.\n",
    "        return kl_div_loss / num_non_pad_tokens\n",
    "\n",
    "\n",
    "# Refer to 'step_20_learning_rates.ipynb' notebook to understand how this function works.\n",
    "def rate(step: int, d_model: int, warmup_steps: int, factor: Optional[float] = 1.0) -> float:\n",
    "    \"\"\"This functions implements the above mentioned learning rate schedule. The learning rate is increased linearly\n",
    "       for the first warmup_steps, and then decreased exponentially for the rest of the training steps. step \n",
    "       corresponds to 'epoch' number in the adam_optimizer functionality in pytorch.\n",
    "\n",
    "    Args:\n",
    "        step (int): current epoch number in the training loop. starts from 0.\n",
    "        d_model (int): size of the vectors in the model. This is 512 in the original transformer model.\n",
    "        warmup_steps (int): number of steps to increase the learning rate linearly.\n",
    "        factor (Optional[float], optional): factor to scale the learning rate. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        float: returns the learning rate by which the initial learning rate should be scaled.\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (d_model ** (-0.5) * min(step ** (-0.5), step * warmup_steps ** (-1.5)))\n",
    "\n",
    "\n",
    "# Refer to 'step_7_data_batching_and_masking.ipynb' notebook to understand how this function works.\n",
    "def construct_look_ahead_mask(size: int) -> Tensor:\n",
    "    \"\"\"Create a mask to prevent the tokens appearing after the current token \n",
    "       to attend to the current token or any token before it.\n",
    "\n",
    "    Args:\n",
    "        size (int): Size of the mask to be created i.e., the length of the sentence.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: A boolean tensor of shape (size, size).\n",
    "    \"\"\"\n",
    "    attention_mask = torch.triu(torch.ones(size, size, dtype=torch.uint8), diagonal=1)\n",
    "    return attention_mask == 0\n",
    "\n",
    "\n",
    "# Refer to 'step_7_data_batching_and_masking.ipynb' notebook to understand how this function works. \n",
    "def construct_padding_mask(input: Tensor, pad_token_id: int) -> Tensor:\n",
    "    \"\"\"Create a mask to prevent the padding tokens from attending to the tokens.\n",
    "\n",
    "    Args:\n",
    "        input (Tensor): A batch of sentences of shape (batch_size, seq_len).\n",
    "        pad_token_id (int): Id of the padding token.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: A boolean tensor of shape (batch_size, seq_len, seq_len).\n",
    "    \"\"\"\n",
    "    mask = (input != pad_token_id)\n",
    "    mask = mask.unsqueeze(1).repeat(1, input.size(1), 1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "# Refer to 'step_7_data_batching_and_masking.ipynb' notebook to understand how this class works.\n",
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data and the corresponding mask to be used for training.\"\"\"\n",
    "\n",
    "    def __init__(self, src_batch: Tensor, tgt_batch: Tensor, pad_token_id: int):\n",
    "        \"\"\"Initialize the Batch object. Updates the tgt_batch to the format expected by the decoder\n",
    "           during training. Also, creates the mask for the source and target sentences.\n",
    "\n",
    "        Args:\n",
    "            src_batch (Tensor): Tensor containing the source sentences in the batch. \n",
    "                                Has the shape [batch_size, seq_len].\n",
    "            tgt_batch (Tensor): Tensor containing the target sentences in the batch.\n",
    "                                Has the shape [batch_size, seq_len].\n",
    "            pad_token_id (int): Id of the pad token appended to the sentences in the batch.\n",
    "        \"\"\"\n",
    "        self.src = src_batch\n",
    "        # The source sentences only need the padding mask since the Encoder does not have to predict\n",
    "        # the next token in the sentence but just encode the input to be used by the Decoder.\n",
    "        # Shape of src_mask: [batch_size, 1, seq_len, seq_len]\n",
    "        self.src_mask = construct_padding_mask(input=src_batch, pad_token_id=pad_token_id).unsqueeze(1)\n",
    "        # Removes the last token (<eos> or <pad>) from the target sentences to create the target_decoder_input.\n",
    "        # Shape of tgt_decoder_input: [batch_size, seq_len - 1]\n",
    "        self.tgt_decoder_input = tgt_batch[:, :-1]\n",
    "        # Removes the first token (<sos>) from the target sentences to create the target_expected_decoder_output.\n",
    "        # Shape of tgt_expected_decoder_output: [batch_size, seq_len - 1]\n",
    "        self.tgt_expected_decoder_output = tgt_batch[:, 1:]\n",
    "        # Shape of tgt_mask: [batch_size, 1, seq_len, seq_len]\n",
    "        self.tgt_mask = self.construct_target_mask(tgt=self.tgt_decoder_input, pad_token_id=pad_token_id)\n",
    "        # Number of tokens in the target sentences excluding the padding tokens. This is used during model \n",
    "        # training for the loss calculation inorder to normalize the total loss and find the loss per token.\n",
    "        self.non_pad_tokens = (self.tgt_expected_decoder_output != pad_token_id).sum()\n",
    "\n",
    "    def construct_target_mask(self, tgt: Tensor, pad_token_id: int) -> Tensor:\n",
    "        # The target sentences need both the padding mask and the look ahead mask. The padding mask is used\n",
    "        # to prevent the padding tokens from attending to the other tokens in the target sentences. The look\n",
    "        # ahead mask is used to prevent the future tokens from attending to the current token or any token.\n",
    "        tgt_mask = construct_padding_mask(input=tgt, pad_token_id=pad_token_id)\n",
    "        tgt_mask = tgt_mask & construct_look_ahead_mask(tgt.size(-1)).type_as(tgt_mask.data)\n",
    "        return tgt_mask.unsqueeze(1)\n",
    "\n",
    "\n",
    "# Refer to 'step_18_label_smoothing.ipynb' notebook to understand how this class works.\n",
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, vocab_size: int, padding_idx: int, smoothing: Optional[float]=SMOOTHING_PROB):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        # Number of classes in the classification problem. It is the size of the vocabulary in transformers.\n",
    "        self.vocab_size = vocab_size\n",
    "        # Index of the padding token or the class label for the padding token.\n",
    "        self.padding_idx = padding_idx\n",
    "        # Amount of probability to be shared among the tokens excluding correct token and padding tokens.\n",
    "        self.smoothing = smoothing\n",
    "        # Amount of probability shared with the correct token.\n",
    "        self.confidence = 1 - smoothing\n",
    "    \n",
    "    def forward(self, targets: Tensor) -> Tensor:\n",
    "        \"\"\"Calculates the smoothed probabilities for each of the target tokens within each sentence.\n",
    "\n",
    "        Args:\n",
    "            targets (Tensor): The target tensor containing the correct class labels (expected token indices from the \n",
    "                              vocab) for each token in the batch. An example target tensor for a batch of 2 sentences\n",
    "                              each with 8 tokens and 6 possible classes for prediction (including the padding token)\n",
    "                              would be: [[0, 3, 4, 5, 5, 1, 2, 2], [1, 5, 3, 3, 4, 0, 0, 2]]\n",
    "                              shape: [batch_size, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: A smoothed probability distribution (1D tensor) for each target token in the batch.\n",
    "                    shape: [batch_size, seq_len, vocab_size]                    \n",
    "        \"\"\"\n",
    "        batch_size, seq_len = targets.shape\n",
    "        # Creating a tensor that will hold the smoothed probabilities for each target token in all the sentences.\n",
    "        smoothed_probs = torch.zeros(size=(batch_size, seq_len, self.vocab_size), dtype=torch.float32)\n",
    "        # Filling the entire tensor with the smoothing probability. We will deal with the probabilities of the\n",
    "        # correct token and padding token later.\n",
    "        smoothed_probs = smoothed_probs.fill_(value=self.smoothing / (self.vocab_size - 2))\n",
    "        # Bringing the targets tensor to contain the same number of dimensions as the smoothed_probs tensor to use\n",
    "        # it with the scatter_ function inorder replace the probabilities in the smoothed_probs tensor in the next \n",
    "        # step.\n",
    "        unsqueezed_targets = targets.unsqueeze(dim=-1)\n",
    "        # Replacing the probabilities in the smoothed_probs tensor with the confidence probability at the \n",
    "        # positions that correspond to the correct class labels (expected output tokens in the target).\n",
    "        smoothed_probs.scatter_(dim=-1, index=unsqueezed_targets, value=self.confidence)\n",
    "        # The padding token should not be predicted at all by the model. So, the probability associated with the\n",
    "        # class label that correspond to the padding token within each target token distribution should be 0. \n",
    "        smoothed_probs[:, :, self.padding_idx] = 0\n",
    "        # The target tensor is appended with the padding tokens at the end. These are just dummy tokens added to bring \n",
    "        # all the sentences in the batch to the same length. We don't want the model to consider these tokens at all \n",
    "        # in the loss calculation. So, we set the probabilities of the entire rows corresponding to the padding tokens\n",
    "        # to 0.\n",
    "        mask = unsqueezed_targets.repeat(1, 1, self.vocab_size) == self.padding_idx\n",
    "        return smoothed_probs.masked_fill(mask=mask, value=0.0)\n",
    "\n",
    "\n",
    "# Refer to 'step_6_dataloader_with_transformers.ipynb' to understand how this class works.\n",
    "def text_extractor(data_point: dict[str, str], language: str) -> str:\n",
    "    \"\"\"Extracts the appropriate text from the datapoint based on the language.\n",
    "\n",
    "    Args:\n",
    "        data_point (dict[str, str]): Datapoint containing the text in the form of a dictionary.\n",
    "                                     The sources sentence is stored in the key 'src' and the target sentence \n",
    "                                     is stored in the key 'tgt'.\n",
    "        language (str): Language of the text to be extracted from the data_point.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Raises an error if the language is not 'english' or 'telugu'.\n",
    "\n",
    "    Returns:\n",
    "        str: The text in the data_point.\n",
    "    \"\"\"\n",
    "    if language == \"english\":\n",
    "        return data_point[\"src\"]\n",
    "    elif language == \"telugu\":\n",
    "        return data_point[\"tgt\"]\n",
    "    raise ValueError(\"Language should be either 'english' or 'telugu'.\")\n",
    "\n",
    "\n",
    "# Refer to 'step_4_dataloader_with_transformers.ipynb' to understand how this class works.\n",
    "class BaseTokenizer(ABC):\n",
    "    \"\"\"A class created to hold different kinds of tokenizers and handle the token encoding in a common way.\n",
    "       Here, we only use SpacyTokenizer and HuggingFaceTokenizer.\"\"\"\n",
    "    def __init__(self, language: str, tokenizer_type: str):\n",
    "        self.language = language\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "\n",
    "    # Abstract methods need to be overridden by the child class. It raises TypeError if not overridden.\n",
    "    @abstractmethod\n",
    "    def initialize_tokenizer_and_build_vocab(self, data_iterator: datasets.arrow_dataset.Dataset, field_extractor: Callable[[dict[str, str], str], str], max_vocab_size: int = 40000):\n",
    "        \"\"\"Initializes the tokenizers and builds the vocabulary for the given dataset.\n",
    "\n",
    "        Args:\n",
    "            data_iterator (datasets.arrow_dataset.Dataset): An iterator that gives input sentences (text) when iterated upon.\n",
    "            field_extractor (Callable[[dict[str, str], str], str]): A function that extracts the appropriate text from the input \n",
    "                dataset. This parameter is added to make the tokenizer independent of the input dataset. If not provided as an \n",
    "                argument, we will have to extract the text from the dataset within the 'BaseTokenizer' class which makes it \n",
    "                dependent on the dataset format. \n",
    "            max_vocab_size (int, optional): Maximum size of the vocabulary to create from the input data corpus. Defaults to 40000.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Abstract methods need to be overridden by the child class. It raises TypeError if not overridden.\n",
    "    @abstractmethod\n",
    "    def tokenize(self, text: str) -> list[str]:\n",
    "        \"\"\"Returns the individual tokens (language text) for the given text\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Abstract methods need to be overridden by the child class. It raises TypeError if not overridden.\n",
    "    @abstractmethod\n",
    "    def encode(self, text: str) -> list[str]:\n",
    "        \"\"\"Returns the encoded token ids for the given text\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @abstractmethod\n",
    "    def decode(self, token_ids: list[int]) -> str:\n",
    "        \"\"\"Returns the decoded text for the given token ids\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Abstract methods need to be overridden by the child class. It raises TypeError if not overridden.\n",
    "    @abstractmethod\n",
    "    def get_token_id(self, token: str) -> int:\n",
    "        \"\"\"Returns the token id for the given token string\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_vocab_size(self) -> int:\n",
    "        \"\"\"Returns the size of the vocabulary i.e., number of tokens in the vocabulary\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "# Refer 'step_2_alternate_tokenization_with_spacy.ipynb' notebook to understand this class better.\n",
    "class SpacyTokenizer(BaseTokenizer):\n",
    "    \"\"\"Creats a tokenizer that tokenizes the text using the Spacy tokenizer models.\"\"\"\n",
    "    def __init__(self, language: str):\n",
    "        super().__init__(language, \"spacy\")\n",
    "        self.special_tokens = [\"<sos>\", \"<eos>\", \"<pad>\", \"<unk>\"]\n",
    "    \n",
    "    def initialize_tokenizer_and_build_vocab(self, data_iterator: datasets.arrow_dataset.Dataset, text_extractor: Callable[[dict[str, str], str], str], max_vocab_size: int = 40000):\n",
    "        # Load spacy models for English text tokenization.\n",
    "        if self.language == \"english\":\n",
    "            self.tokenizer = spacy.load(\"en_core_web_sm\").tokenizer          \n",
    "        elif self.language == \"telugu\":\n",
    "            # Load spacy model for Telugu text tokenization.\n",
    "            self.tokenizer = spacy.blank(\"te\").tokenizer            \n",
    "        else:\n",
    "            # Raise an error for unknown language\n",
    "            pass\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.__build_vocab(data_iterator=data_iterator, text_extractor=text_extractor)\n",
    "\n",
    "    def tokenize(self, text: str) -> list[str]:\n",
    "        return [token.text for token in self.tokenizer(text)]\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        return self.vocab(self.tokenize(text))\n",
    "    \n",
    "    def decode(self, token_ids: list[int]) -> str:\n",
    "        \"\"\"Returns the decoded text for the given token ids\"\"\"\n",
    "        return \" \".join(self.vocab.lookup_tokens(indices=token_ids))\n",
    "\n",
    "    def get_token_id(self, token: str) -> int:\n",
    "        return self.vocab([token])[0]\n",
    "    \n",
    "    def get_vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def __yield_tokens(self, data_iterator: datasets.arrow_dataset.Dataset, text_extractor: Callable[[dict[str, str], str], str]):\n",
    "        \"\"\"Returns a generator object that emits tokens for each sentence in the dataset\"\"\"\n",
    "        for data_point in data_iterator:\n",
    "            yield self.tokenize(text_extractor(data_point, self.language))\n",
    "\n",
    "    def __build_vocab(self, data_iterator: datasets.arrow_dataset.Dataset, text_extractor: Callable[[dict[str, str], str], str]):\n",
    "        \"\"\"Builds the vocabulary for the given dataset\"\"\"\n",
    "        self.vocab = build_vocab_from_iterator(iterator=self.__yield_tokens(data_iterator=data_iterator, text_extractor=text_extractor), min_freq=2, specials=self.special_tokens, special_first=True, max_tokens=self.max_vocab_size)\n",
    "        self.vocab.set_default_index(self.vocab[\"<unknown>\"])\n",
    "    \n",
    "\n",
    "# Refer to 'step_4_datasets_and_dataloaders_pytorch' to understand how this class works.\n",
    "class HuggingFaceDatasetWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset: datasets.arrow_dataset.Dataset):\n",
    "        \"\"\"Initializes the HuggingFaceDatasetWrapper with the given dataset.\n",
    "\n",
    "        Args:\n",
    "            hf_dataset (datasets.arrow_dataset.Dataset): The hugging face dataset to be wrapped.\n",
    "        \"\"\"\n",
    "        self.dataset = hf_dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Extracts the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Extracts the data_point at a particular index in the dataset.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the data_point to be extracted from the dataset.\n",
    "\n",
    "        Returns:\n",
    "            dict: Data_point at the given index in the dataset. This turns out to be a dictionary for our dataset but\n",
    "                  it could be any type in general.\n",
    "        \"\"\"\n",
    "        # Return the dataset at a particular index.\n",
    "        # The index provided will always be less then length (64 in this case) returned by __len__ function.\n",
    "        return self.dataset[index]\n",
    "\n",
    "\n",
    "# Refer to 'step_6_dataloader_with_transformers.ipynb' notebook to understand how this class works.\n",
    "class LengthAwareSampler(Sampler):\n",
    "    def __init__(self, dataset: Dataset, batch_size: int):\n",
    "        # dataset is the Dataset wrapper we created on top of HuggingFace dataset.\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.sorted_indices = self.extract_lengths()\n",
    "    \n",
    "    # We don't want the entire dataset to be loaded into the memory at once. So, we first iterate over the entire \n",
    "    # dataset, extract the lengths of the sentences and sort the indices of the sentences according to the sentence \n",
    "    # lengths. This is to ensure that sentences of similar lengths are grouped together in a batch to minimize the \n",
    "    # overall padding necessary. When we iterate over the dataset, we only load the necessary data from the dataset \n",
    "    # into memory and not the entire dataset at once --> This loading logic could be a little different based on the \n",
    "    # hugging face implementations. Please look into the hugging face documentation for more details.\n",
    "    def extract_lengths(self) -> list[int]:\n",
    "        \"\"\"Sorts the indices of the dataset based on the lengths of the sentences in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            list[int]: Indices of the dataset sorted in ascending order (small to big) based on the lengths of \n",
    "                       the sentences in the dataset.\n",
    "        \"\"\"\n",
    "        # Note that the lengths are calculated based on the number of characters in the sentence and not the number\n",
    "        # of tokens.\n",
    "        self.lengths = [len(data_point[\"src\"].split(\" \")) + len(data_point[\"tgt\"].split(\" \")) for data_point in self.dataset]\n",
    "        return sorted(range(len(self.dataset)), key=lambda index: self.lengths[index])\n",
    "\n",
    "    # The __iter__ function is called once per epoch. The returned iterator is iterated on to get the list of indices \n",
    "    # for the data points in a batch.\n",
    "    def __iter__(self) -> Generator[list[int], None, None]:\n",
    "        \"\"\"Provides an iterator that yields the indices of the dataset in the order of the sentence lengths.\n",
    "\n",
    "        Returns:\n",
    "            Generator: A generator object that yields the indices of the dataset in the order of the sentence lengths.\n",
    "\n",
    "        Yields:\n",
    "            Generator[list[int], None, None]: A generator object that yields the indices of the dataset in the order\n",
    "                                              of the sentence lengths.\n",
    "        \"\"\"\n",
    "        # Create the batches of indices based on the sentence lengths. \n",
    "        # batches look like: [[0, 5, 90], [23, 4, 5], ...] if batch_size is 3.\n",
    "        # [0, 5, 90] is a batch corresponding to the sentences at indices 0, 5 and 90 in the original dataset.\n",
    "        # [23, 4, 5] is a batch corresponding to the sentences at indices 23, 4 and 5 in the original dataset.\n",
    "        batches = [self.lengths[index: index + self.batch_size] for index in range(0, len(self.dataset), self.batch_size)]\n",
    "        # Shuffle the batches to ensure that the order of batches is different in every epoch. We want the model to \n",
    "        # see the data in different order in every epoch. So, we shuffle the order of the batches within the dataset.\n",
    "        random.shuffle(batches)\n",
    "        # Flatten the list of batches to get an iterable of indices. At the end, the dataloader expects an iterable of\n",
    "        # indices to get the data points from the dataset. So, we convert the list of batches back to an iterable of \n",
    "        # indices.\n",
    "        return iter([index for batch in batches for index in batch])\n",
    "\n",
    "\n",
    "# Refer to 'step_6_dataloader_with_transformers.ipynb' notebook to understand how this class works.\n",
    "def length_aware_collate_fn(batch, english_tokenizer: BaseTokenizer, telugu_tokenizer: BaseTokenizer, sos_id: int = 0, eos_id: int = 1, pad_id: int = 2):\n",
    "    \"\"\"Converts the raw data in the batch into the format required by the MachineTranslationTransformer model. It encodes the\n",
    "       sentences into token ids, adds start, end and padding tokens and batches the converted data back to be used by the \n",
    "       model.\n",
    "\n",
    "    Args:\n",
    "        batch (_type_): Holds the raw data points (the actual english (src) and telugu (tgt) sentences batched) from the dataset.\n",
    "        english_tokenizer (BaseTokenizer): Tokenizer to tokenize and encode the english sentences into corresponding token ids.\n",
    "        telugu_tokenizer (BaseTokenizer): Tokenizer to tokenize and encode the english sentences into corresponding token ids.\n",
    "        sos_id (int, optional): start of sentence token id. Defaults to 0.\n",
    "        eos_id (int, optional): end of sentence token id. Defaults to 1.\n",
    "        pad_id (int, optional): padding token id. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: Returns the encoded source and target tensors in the batch which can be used by the transformer model\n",
    "                               as input.\n",
    "    \"\"\"\n",
    "    # Holds all the encoded src sentences (english sentences) from the batch. encoded sentence means sentence divided \n",
    "    # into tokens and tokens converted into their integer ids.\n",
    "    # [[0, 223, 4345, 545, 1], [0, 23, 234, 67, 1]] is an example for the processed_src_sentences variable where\n",
    "    # [0, 223, 4345, 545, 1] represents an ecodeded sentence from the batch and 0 at the start is <sos> and 1 at the \n",
    "    # end is <eos>. \n",
    "    processed_src_sentences = []\n",
    "    # Holds all the encoded tgt sentences (telugu sentences) from the batch.\n",
    "    processed_tgt_sentences = []\n",
    "    for data_point in batch:\n",
    "        # src is english sentence.\n",
    "        src_sentence = data_point[\"src\"]\n",
    "        # tgt is telugu sentence.\n",
    "        tgt_sentence = data_point[\"tgt\"]\n",
    "        # start of sentence id to append at the start of every sentence.\n",
    "        sos_tensor = torch.tensor([sos_id], dtype=torch.int64)\n",
    "        # end of sentence id to append at the end of every sentence.\n",
    "        eos_tensor = torch.tensor([eos_id], dtype=torch.int64)\n",
    "        # It is important to set the dtype to 'torch.int64' because we map token_ids to their embeddings in the transformer model.\n",
    "        # '<sos>' and '<eos>' tokens are not added to the src sentences. They are only added to the target sentences.\n",
    "        encoded_src_sentence = torch.tensor(english_tokenizer.encode(src_sentence), dtype=torch.int64)\n",
    "        # prepares the tensor in the format 'token_id(<sos>) token_id1 token_id2 ... last_token_id token_id(<eos>)'. \n",
    "        encoded_tgt_sentence = torch.cat([sos_tensor, torch.tensor(telugu_tokenizer.encode(tgt_sentence), dtype=torch.int64), eos_tensor], dim=0)\n",
    "        processed_src_sentences.append(encoded_src_sentence)\n",
    "        processed_tgt_sentences.append(encoded_tgt_sentence)\n",
    "    # find the maximum length (max_len) of the sentences in the batch so that sentences are padded (if needed) to get all the\n",
    "    # sentences to the same length i.e., max_len.\n",
    "    max_len = max(max(src_ids.size(0) for src_ids in processed_src_sentences), max(tgt_ids.size(0) for tgt_ids in processed_tgt_sentences))\n",
    "    # We pad the sentences with pad token so that every sentence in the batch is of same length. Also, notice \n",
    "    # that the pad token is appended after (not before) the <eos> token is appended to every sentence.\n",
    "    src_ids = [torch.nn.functional.pad(input=src_ids, pad=(0, max_len - src_ids.size(0)), mode=\"constant\", value=pad_id) for src_ids in processed_src_sentences]\n",
    "    tgt_ids = [torch.nn.functional.pad(input=tgt_ids, pad=(0, max_len - tgt_ids.size(0)), mode=\"constant\", value=pad_id) for tgt_ids in processed_tgt_sentences]\n",
    "    # stack the src tensors along dimension 0. This then becomes a 2D tensor of shape (BATCH_SIZE, MAX_SENTENCE_LENGTH).\n",
    "    src = torch.stack(tensors=src_ids, dim=0)\n",
    "    tgt = torch.stack(tensors=tgt_ids, dim=0)\n",
    "    return (src, tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainState:\n",
    "    epoch_num: int\n",
    "    learning_rate: float\n",
    "    training_loss: float\n",
    "    num_tokens_processed: int\n",
    "    model_checkpoint_path: str\n",
    "    validation_loss: Optional[float] = None\n",
    "\n",
    "class ModelState:\n",
    "    def __init__(self):\n",
    "        self.state = []\n",
    "\n",
    "    def append_state(self, train_state: TrainState):\n",
    "        self.state.append(train_state)\n",
    "    \n",
    "    def plot_loss_variation(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintTrainState(train_state: TrainState):\n",
    "    print(\"epoch_num: \", train_state.epoch_num)\n",
    "    print(\"learning_rate: \", train_state.learning_rate)\n",
    "    print(\"num_tokens_processed until this epoch: \", train_state.num_tokens_processed)\n",
    "    print(\"model_checkpoint_path: \", train_state.model_checkpoint_path)\n",
    "    print(\"training_loss: \", train_state.training_loss)\n",
    "    print(\"validation_loss: \", train_state.validation_loss)\n",
    "    print(\"-\" * 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizers(train_dataset_path: str) -> Tuple[BaseTokenizer, BaseTokenizer]:\n",
    "    translation_train_dataset = datasets.load_from_disk(train_dataset_path)\n",
    "    telugu_tokenizer: BaseTokenizer = SpacyTokenizer(language=\"telugu\")\n",
    "    telugu_tokenizer.initialize_tokenizer_and_build_vocab(data_iterator=translation_train_dataset, text_extractor=text_extractor, max_vocab_size=SRC_VOCAB_SIZE)\n",
    "    english_tokenizer: BaseTokenizer = SpacyTokenizer(language=\"english\")\n",
    "    english_tokenizer.initialize_tokenizer_and_build_vocab(data_iterator=translation_train_dataset, text_extractor=text_extractor, max_vocab_size=TGT_VOCAB_SIZE)\n",
    "    return english_tokenizer, telugu_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset_path: str, english_tokenizer: BaseTokenizer, telugu_tokenizer: BaseTokenizer):\n",
    "    # Load the train dataset we have created and saved to disk in step_1_data_exploration.ipynb. \n",
    "    translation_dataset = datasets.load_from_disk(dataset_path)\n",
    "    wrapped_translation_dataset = HuggingFaceDatasetWrapper(hf_dataset=translation_dataset)\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"The collate_fn is called by the DataLoader with just the batch of data points from the dataset. So, we wrap the \n",
    "        length_aware_collate_fn function with the required parameters to create a collate_fn that can be used by the \n",
    "        DataLoader.\n",
    "\n",
    "        Args:\n",
    "            batch (_type_): Batch of raw data points from the dataset.\n",
    "\n",
    "        Returns:\n",
    "            _type_: Returns the batch of data points in the format required by the MachineTranslationTransformer model.\n",
    "        \"\"\"\n",
    "        sos_id = english_tokenizer.get_token_id(token=\"<sos>\")\n",
    "        eos_id = english_tokenizer.get_token_id(token=\"<eos>\")\n",
    "        pad_id = telugu_tokenizer.get_token_id(token=\"<pad>\")\n",
    "        return length_aware_collate_fn(batch=batch, \n",
    "                                       english_tokenizer=english_tokenizer, \n",
    "                                       telugu_tokenizer=telugu_tokenizer, \n",
    "                                       sos_id=sos_id, \n",
    "                                       eos_id=eos_id, \n",
    "                                       pad_id=pad_id)\n",
    "    length_aware_sampler = LengthAwareSampler(dataset=wrapped_translation_dataset, batch_size=32)\n",
    "    return DataLoader(dataset=wrapped_translation_dataset, batch_size=128, sampler=length_aware_sampler, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(machine_translation_model: MachineTranslationTransformer, \n",
    "              train_dataLoader: DataLoader, \n",
    "              optimizer: torch.optim.Optimizer, \n",
    "              lr_scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "              loss_compute: LossCompute,\n",
    "              label_smoothing: LabelSmoothing,\n",
    "              epoch_num: int,\n",
    "              pad_token_id: Optional[int] = PAD_TOKEN_ID) -> TrainState:\n",
    "    # Holds the learning rate at the start of the epoch.\n",
    "    cur_learning_rate = lr_scheduler.get_last_lr()\n",
    "    num_tokens_processed = 0\n",
    "    for src_batch, tgt_batch in train_dataLoader:\n",
    "        # zero out the gradients.\n",
    "        optimizer.zero_grad()\n",
    "        # Create Batch from the input.\n",
    "        batch = Batch(src_batch=src_batch, tgt_batch=tgt_batch, pad_token_id=pad_token_id)\n",
    "        num_tokens_processed += batch.non_pad_tokens\n",
    "        # Forward pass of the machine translation model. Returns the predicted probability distributions for each token\n",
    "        # in the target sentences.\n",
    "        predicted_tgt_log_probability_distributions = machine_translation_model(src=batch.src, \n",
    "                                                                                tgt=batch.tgt_decoder_input, \n",
    "                                                                                src_mask=batch.src_mask, \n",
    "                                                                                tgt_mask=batch.tgt_mask)\n",
    "        # Applies label smoothing to the expected token ids.\n",
    "        expected_tgt_probability_distributions = label_smoothing(targets=batch.tgt_expected_decoder_output)\n",
    "        # Computes the KLDivergence loss between predicted and expected outputs.\n",
    "        loss = loss_compute(log_predictions=predicted_tgt_log_probability_distributions, \n",
    "                            targets=expected_tgt_probability_distributions, \n",
    "                            num_non_pad_tokens=int(batch.non_pad_tokens.item()))\n",
    "        # Computes the gradients wrt to the loss.\n",
    "        loss.backward()\n",
    "        # Updates the weights with the calculated gradients.\n",
    "        optimizer.step()\n",
    "        # Updates the learning rate.\n",
    "        lr_scheduler.step()\n",
    "    # Save the model to disk.    \n",
    "    model_checkpoint_path = f\"{MODEL_CHECK_POINTS_PATH}/epoch_{epoch_num}_translation_model.pt\"\n",
    "    torch.save(machine_translation_model.state_dict(), model_checkpoint_path)\n",
    "    train_state = TrainState(epoch_num=epoch_num, \n",
    "                             learning_rate=cur_learning_rate, \n",
    "                             training_loss=loss.item(), \n",
    "                             num_tokens_processed=num_tokens_processed, \n",
    "                             model_checkpoint_path=model_checkpoint_path)\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs: int, \n",
    "                english_tokenizer: BaseTokenizer, \n",
    "                telugu_tokenizer: BaseTokenizer, \n",
    "                pad_token_id: Optional[int]=PAD_TOKEN_ID) -> Tuple[MachineTranslationTransformer, ModelState]:\n",
    "    translation_model = MachineTranslationTransformer(d_model=D_MODEL, \n",
    "                                                      d_feed_forward=D_FEED_FORWARD,\n",
    "                                                      dropout_prob=DROPOUT_PROB, \n",
    "                                                      num_heads=NUM_HEADS, \n",
    "                                                      src_vocab_size=english_tokenizer.get_vocab_size(), \n",
    "                                                      tgt_vocab_size=telugu_tokenizer.get_vocab_size(), \n",
    "                                                      num_layers=NUM_LAYERS, \n",
    "                                                      max_seq_len=MAX_SEQ_LEN)\n",
    "    adam_optimizer = torch.optim.Adam(params=translation_model.parameters(), lr=INITIAL_LEARNING_RATE, betas=(BETA_1, BETA_2), eps=EPSILON)\n",
    "    rate_lambda = lambda step: rate(step, d_model=D_MODEL, warmup_steps=NUM_WARMUP_STEPS, factor=1.0)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=adam_optimizer, lr_lambda=rate_lambda)\n",
    "    label_smoothing = LabelSmoothing(vocab_size=TGT_VOCAB_SIZE, padding_idx=PAD_TOKEN_ID, smoothing=SMOOTHING_PROB)\n",
    "    loss_compute = LossCompute()\n",
    "    model_state = ModelState()\n",
    "    train_dataloader = create_dataloader(dataset_path=f\"{AI4_BHARAT_DATA_PATH}/mini_train_dataset\", english_tokenizer=english_tokenizer, telugu_tokenizer=telugu_tokenizer)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_state = run_epoch(machine_translation_model=translation_model, \n",
    "                                train_dataLoader=train_dataloader, \n",
    "                                optimizer=adam_optimizer, \n",
    "                                lr_scheduler=lr_scheduler, \n",
    "                                loss_compute=loss_compute, \n",
    "                                label_smoothing=label_smoothing, \n",
    "                                epoch_num=epoch, \n",
    "                                pad_token_id=pad_token_id)\n",
    "        model_state.append_state(train_state)\n",
    "        PrintTrainState(train_state=train_state)\n",
    "    return translation_model, model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenizer, telugu_tokenizer = create_tokenizers(train_dataset_path=f\"{AI4_BHARAT_DATA_PATH}/mini_train_dataset\")\n",
    "# translation_model, model_state = train_model(num_epochs=NUM_EPOCHS, english_tokenizer=english_tokenizer, telugu_tokenizer=telugu_tokenizer, pad_token_id=PAD_TOKEN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 29])\n",
      "(tensor([[  32,   11,  241,  ...,    2,    2,    2],\n",
      "        [  12,  101,   18,  ...,    2,    2,    2],\n",
      "        [ 720, 1742,   16,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [1074,  344,   27,  ...,    2,    2,    2],\n",
      "        [1089, 2361,   17,  ...,    2,    2,    2],\n",
      "        [  12,   87,   40,  ...,    2,    2,    2]]), tensor([[   0,    3,    4,  ...,    2,    2,    2],\n",
      "        [   0,    6, 1857,  ...,    2,    2,    2],\n",
      "        [   0, 6323, 1191,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [   0,    6,  256,  ...,    2,    2,    2],\n",
      "        [   0,  203,  542,  ...,    2,    2,    2],\n",
      "        [   0, 8902, 6946,  ...,    2,    2,    2]]))\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = create_dataloader(dataset_path=f\"{AI4_BHARAT_DATA_PATH}/mini_train_dataset\", english_tokenizer=english_tokenizer, telugu_tokenizer=telugu_tokenizer)\n",
    "data_iterator = iter(train_dataloader)\n",
    "# batched_data = next(data_iterator)\n",
    "index = 1\n",
    "for batched_data in data_iterator:\n",
    "    # print(\"shapes: \", batched_data[0].shape, batched_data[1].shape)\n",
    "    if index == 34:\n",
    "        print(batched_data[0].shape)\n",
    "        print(batched_data)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
