{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) Steps involved in the machine translation inference process. In this notebook, we use greedy search to generate translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from model_implementation.utils.helpers import get_absolute_path\n",
    "from tokenizers import ByteLevelBPETokenizer # type: ignore\n",
    "from torch import nn, Tensor\n",
    "from typing import Callable, Optional, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next few cells holds the preparation part for inference. All the content here is already explained in the previous notebooks. So, you can run the next few cells until the 'main part' (mentioned below) blindly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of the sequence token.\n",
    "START_TOKEN = \"<sos>\"\n",
    "# End of the sequence token.\n",
    "END_TOKEN = \"<eos>\"\n",
    "# Padding token.\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "# Token used to represent out-of-vocabulary words.\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "# Maximum vocabulary size.\n",
    "MAX_VOCAB_SIZE = 30000\n",
    "# Maximum number of tokens allowed to be predicted during inference.\n",
    "MAX_INFERENCE_SEQ_LEN = 150\n",
    "# Size of the model's hidden state.\n",
    "D_MODEL = 10\n",
    "# Path to the dataset to be used for Tokenizer training.\n",
    "TOKENIZER_DATA_PATH = \"../../Data/AI4Bharat/full_en_te_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a base class that will be inherited by the actual tokenizer classes.\n",
    "class BaseTokenizer(ABC):\n",
    "    \"\"\"A class created to hold different kinds of tokenizers and handle the token encoding in a common way.\n",
    "       Here, we only use SpacyTokenizer and HuggingFaceTokenizer.\"\"\"\n",
    "    def __init__(self, language: str, tokenizer_type: str):\n",
    "        self.language = language\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "        self.special_tokens = [START_TOKEN, END_TOKEN, PAD_TOKEN, UNK_TOKEN]\n",
    "\n",
    "    @abstractmethod\n",
    "    def initialize_tokenizer_and_build_vocab(self, \n",
    "                                             data_iterator: datasets.arrow_dataset.Dataset, \n",
    "                                             text_extractor: Callable[[dict[str, str], str], str], \n",
    "                                             max_vocab_size: Optional[int] = MAX_VOCAB_SIZE):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def tokenize(self, text: str) -> list[str]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        pass\n",
    "\n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_token_id(self, token: str) -> int:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_vocab_size(self) -> int:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class BPETokenizer(BaseTokenizer):\n",
    "    \"\"\"Trains a tokenizer using HuggingFace libraries\"\"\"\n",
    "    def __init__(self, language: str):\n",
    "        super().__init__(language, \"bpe\")\n",
    "\n",
    "    def initialize_tokenizer_and_build_vocab(self, \n",
    "                                             data_iterator: datasets.arrow_dataset.Dataset, \n",
    "                                             text_extractor: Callable[[dict[str, str], str], str], \n",
    "                                             max_vocab_size: Optional[int] = MAX_VOCAB_SIZE):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.tokenizer = self.__train_tokenizer(data_iterator=data_iterator, text_extractor=text_extractor, max_vocab_size=max_vocab_size)\n",
    "\n",
    "    def tokenize(self, text: str) -> list[str]:\n",
    "        encoded_text = self.tokenizer.encode(text)\n",
    "        return encoded_text.tokens\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        encoded_text = self.tokenizer.encode(text)\n",
    "        return encoded_text.ids\n",
    "\n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        return self.tokenizer.decode(token_ids)\n",
    "\n",
    "    def get_token_id(self, token: str) -> int:\n",
    "        return self.tokenizer.token_to_id(token)\n",
    "\n",
    "    def get_vocab_size(self) -> int:\n",
    "        return self.tokenizer.get_vocab_size()\n",
    "\n",
    "    def __get_data_iterator(self, data_iterator: datasets.arrow_dataset.Dataset, text_extractor: Callable[[dict[str, str], str], str]):\n",
    "        for data_point in data_iterator:\n",
    "            yield text_extractor(data_point=data_point, language=self.language) # type: ignore\n",
    "\n",
    "    def __train_tokenizer(self, data_iterator: datasets.arrow_dataset.Dataset, \n",
    "                          text_extractor: Callable[[dict[str, str], str], str], \n",
    "                          max_vocab_size: Optional[int]=MAX_VOCAB_SIZE) -> ByteLevelBPETokenizer:\n",
    "        # Use BPE to train a ByteLevel BPE tokenizer.\n",
    "        tokenizer = ByteLevelBPETokenizer()\n",
    "        # train_from_iterator is used so that the entire dataset is not loaded into memory at once.\n",
    "        tokenizer.train_from_iterator(iterator=self.__get_data_iterator(data_iterator=data_iterator, text_extractor=text_extractor), \n",
    "                                      vocab_size= max_vocab_size, \n",
    "                                      special_tokens=self.special_tokens)\n",
    "        return tokenizer\n",
    "\n",
    "    \n",
    "    def save_tokenizer_to_disk(self, directory_to_save: str):\n",
    "        absolute_directory_path = get_absolute_path(relative_path=directory_to_save)\n",
    "        self.tokenizer.save_model(absolute_directory_path)\n",
    "\n",
    "\n",
    "    def load_trained_tokenizer_from_disk(self, saved_tokenizer_directory: str):\n",
    "        absolute_directory_path = get_absolute_path(relative_path=saved_tokenizer_directory)\n",
    "        self.tokenizer = ByteLevelBPETokenizer.from_file(vocab_filename=f\"{absolute_directory_path}/vocab.json\", \n",
    "                                                         merges_filename=f\"{absolute_directory_path}/merges.txt\")\n",
    "        \n",
    "\n",
    "def text_extractor(data_point: dict[str, str], language: str) -> str:\n",
    "    if language == \"english\":\n",
    "        return data_point[\"src\"]\n",
    "    elif language == \"telugu\":\n",
    "        return data_point[\"tgt\"]\n",
    "    raise ValueError(\"Language should be either 'english' or 'telugu'.\")\n",
    "\n",
    "\n",
    "def construct_padding_mask(input: Tensor, pad_token_id: int) -> Tensor:\n",
    "    mask = (input != pad_token_id)\n",
    "    mask = mask.unsqueeze(1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def construct_look_ahead_mask(size: int) -> Tensor:\n",
    "    attention_mask = torch.triu(torch.ones(size, size, dtype=torch.uint8), diagonal=1)\n",
    "    return attention_mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['idx', 'src', 'tgt'],\n",
      "    num_rows: 4946035\n",
      "})\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "en_te_translation_dataset = datasets.load_from_disk(TOKENIZER_DATA_PATH)\n",
    "print(en_te_translation_dataset)\n",
    "print(type(en_te_translation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "english_tokenizer = BPETokenizer(language=\"english\")\n",
    "english_tokenizer.initialize_tokenizer_and_build_vocab(data_iterator=en_te_translation_dataset, text_extractor=text_extractor, max_vocab_size=MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "telugu_tokenizer = BPETokenizer(language=\"telugu\")\n",
    "telugu_tokenizer.initialize_tokenizer_and_build_vocab(data_iterator=en_te_translation_dataset, text_extractor=text_extractor, max_vocab_size=MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The main part specific to this notebook starts from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sentences = [\"I am a Software Engineer at Google.\", \n",
    "                 \"How do I learn Machine Learing and start working on awesome ideas?\", \n",
    "                 \"Lets do a Masters in Data Science at good university.\", \n",
    "                 \"I watched The Boys tv show last week. It was awesome\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a Software Engineer at Google.\n",
      "How do I learn Machine Learing and start working on awesome ideas?\n",
      "Lets do a Masters in Data Science at good university.\n",
      "I watched The Boys tv show last week. It was awesome\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[[44, 654, 262, 18571, 13324, 396, 5356, 17], [531, 464, 341, 2337, 26973, 477, 8020, 297, 1022, 1935, 332, 16570, 7335, 34], [3442, 464, 262, 17671, 285, 15400, 4671, 396, 839, 7315, 17], [44, 9597, 1029, 9711, 86, 260, 89, 1490, 1170, 2260, 17, 4357, 358, 16570]]\n"
     ]
    }
   ],
   "source": [
    "# The first step in inference is to tokenize the source sentences.\n",
    "\n",
    "tokenized_src_sequences: List[List[int]] = []\n",
    "# Converts each source sentence into a list of token ids.\n",
    "for sentence in src_sentences:\n",
    "    tokenized_src_sequences.append(english_tokenizer.encode(sentence))\n",
    "    # Confirm that the sentence is tokenized correctly and can be decoded back.\n",
    "    print(english_tokenizer.decode(tokenized_src_sequences[-1]))\n",
    "print(\"-\" * 150)\n",
    "print(tokenized_src_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad token id:  2\n",
      "start token id:  0\n",
      "end token id:  1\n"
     ]
    }
   ],
   "source": [
    "# These token ids should be the same in both src vocabulary and tgt vocabulary.\n",
    "# Gets the token id for the PAD token.\n",
    "pad_token_id = english_tokenizer.get_token_id(PAD_TOKEN)\n",
    "print(\"pad token id: \", pad_token_id)\n",
    "# Gets the token id for the START token.\n",
    "start_token_id = telugu_tokenizer.get_token_id(START_TOKEN)\n",
    "print(\"start token id: \", start_token_id)\n",
    "# Gets the token id for the END token.\n",
    "end_token_id = telugu_tokenizer.get_token_id(END_TOKEN)\n",
    "print(\"end token id: \", end_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44, 654, 262, 18571, 13324, 396, 5356, 17, 2, 2, 2, 2, 2, 2], [531, 464, 341, 2337, 26973, 477, 8020, 297, 1022, 1935, 332, 16570, 7335, 34], [3442, 464, 262, 17671, 285, 15400, 4671, 396, 839, 7315, 17, 2, 2, 2], [44, 9597, 1029, 9711, 86, 260, 89, 1490, 1170, 2260, 17, 4357, 358, 16570]]\n"
     ]
    }
   ],
   "source": [
    "# We need to pad the input sequences to make them of the same length. This is because we need to batch\n",
    "# the input sequences and pass them through the model in one go.\n",
    "\n",
    "# Find the maximum length of the source sequences.\n",
    "max_src_seq_len = max([len(seq) for seq in tokenized_src_sequences])\n",
    "for src_seq in tokenized_src_sequences:\n",
    "    # Pad the source sequence with pad_token_id to make it of length max_src_seq\n",
    "    src_seq.extend([pad_token_id] * (max_src_seq_len - len(src_seq)))\n",
    "\n",
    "# This should print the tokenized source sequences (as seen in the above cell) with padding.\n",
    "print(tokenized_src_sequences)\n",
    "assert len(tokenized_src_sequences[0]) == len(tokenized_src_sequences[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the source batch tensor:  torch.Size([4, 14])\n",
      "src_batch: \n",
      " tensor([[   44,   654,   262, 18571, 13324,   396,  5356,    17,     2,     2,\n",
      "             2,     2,     2,     2],\n",
      "        [  531,   464,   341,  2337, 26973,   477,  8020,   297,  1022,  1935,\n",
      "           332, 16570,  7335,    34],\n",
      "        [ 3442,   464,   262, 17671,   285, 15400,  4671,   396,   839,  7315,\n",
      "            17,     2,     2,     2],\n",
      "        [   44,  9597,  1029,  9711,    86,   260,    89,  1490,  1170,  2260,\n",
      "            17,  4357,   358, 16570]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Convert the tokenized source sequences to a tensor since the model expects tensors as input.\n",
    "src_batch = torch.tensor(data=tokenized_src_sequences, dtype=torch.int32)\n",
    "print(\"shape of the source batch tensor: \", src_batch.shape)\n",
    "print(\"src_batch: \\n\", src_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the source mask tensor:  torch.Size([4, 1, 14])\n",
      "src_mask: \n",
      " tensor([[[ True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
      "          False, False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True, False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True]]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of the source mask tensor:  torch.Size([4, 1, 1, 14])\n",
      "src_mask: \n",
      " tensor([[[[ True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
      "           False, False, False, False]]],\n",
      "\n",
      "\n",
      "        [[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "            True,  True,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "            True, False, False, False]]],\n",
      "\n",
      "\n",
      "        [[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "            True,  True,  True,  True]]]])\n"
     ]
    }
   ],
   "source": [
    "# We need to create the attention mask for the source sequences. This is because the model should not attend to the \n",
    "# padding tokens.\n",
    "\n",
    "# The attention mask is a tensor of shape [batch_size, 1, max_src_seq_len] where each element is True if the corresponding \n",
    "# element in the source sequence is not a padding token.\n",
    "src_mask = construct_padding_mask(input=src_batch, pad_token_id=pad_token_id)\n",
    "print(\"shape of the source mask tensor: \", src_mask.shape)\n",
    "print(\"src_mask: \\n\", src_mask)\n",
    "print(\"-\" * 150)\n",
    "# The same attention mask is applied to all the heads in the multi-head attention mechanism. To account for this, we\n",
    "# create a tensor of shape [batch_size, 1, 1, max_src_seq_len] and let python broadcast it to the required shape which\n",
    "# would be [batch_size, num_heads, max_tgt_seq_len, max_src_seq_len].\n",
    "src_mask = src_mask.unsqueeze(1)\n",
    "print(\"shape of the source mask tensor: \", src_mask.shape)\n",
    "print(\"src_mask: \\n\", src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the encoded source tensor:  torch.Size([4, 14, 10])\n",
      "encoded_src: \n",
      " tensor([[[-4.9424e-01,  2.2307e+00, -1.2217e+00, -1.1746e+00,  1.2302e+00,\n",
      "          -4.9120e-01, -2.3567e-01, -7.6592e-01,  5.0781e-01, -2.1646e+00],\n",
      "         [-6.8511e-01,  9.4651e-01,  6.3741e-01,  2.9685e-02,  2.6911e-01,\n",
      "          -7.2600e-01,  5.9115e-01,  1.3728e+00, -6.4206e-01, -1.5254e+00],\n",
      "         [-1.0006e+00,  1.3398e-01,  4.8169e-01, -1.0989e+00, -6.4641e-01,\n",
      "           1.0005e+00, -1.6974e-01,  5.8577e-03, -1.1800e+00, -5.3509e-01],\n",
      "         [ 1.6506e+00, -2.2694e+00, -1.2932e+00, -1.8767e-01, -1.1205e-01,\n",
      "          -7.7379e-01,  1.1858e-01,  4.2639e-01, -6.1384e-01,  8.0423e-01],\n",
      "         [-2.3534e-01,  1.9073e-01,  1.4027e+00, -3.0396e-01,  6.0874e-02,\n",
      "          -4.1424e-01,  2.3196e-02, -7.5784e-01, -3.8664e-01, -7.8360e-02],\n",
      "         [-1.2419e+00,  6.6320e-01,  1.3953e+00,  5.8699e-02, -1.7150e+00,\n",
      "          -9.3034e-01, -2.1458e+00,  5.7369e-01, -1.2550e+00, -1.8055e+00],\n",
      "         [-8.5708e-01,  4.6655e-01,  2.2768e+00, -1.0441e+00, -8.2010e-01,\n",
      "           3.2596e-01,  1.1083e+00,  1.0253e+00, -7.4667e-01,  5.0608e-01],\n",
      "         [-4.7109e-03,  2.2043e-01, -7.2112e-01,  9.6440e-01,  1.0485e-01,\n",
      "          -9.6358e-01, -2.8767e-01, -4.3110e-01, -1.1808e+00, -1.0593e+00],\n",
      "         [ 1.6539e+00,  2.8553e+00,  6.7049e-01,  1.0577e+00,  9.0920e-01,\n",
      "          -2.3345e-01, -3.6549e-01, -7.1170e-01, -1.5400e+00,  8.0413e-03],\n",
      "         [ 7.9298e-01, -2.0616e+00, -2.9574e-01,  1.0953e+00, -1.6538e-01,\n",
      "          -9.2666e-02, -1.2544e+00, -9.8461e-02, -1.1359e+00, -2.1020e+00],\n",
      "         [ 4.6759e-01, -6.4346e-01,  9.4320e-01,  6.2720e-01, -4.3079e-01,\n",
      "           1.0120e+00, -1.1552e+00, -1.3122e+00, -7.6069e-01,  3.0041e-01],\n",
      "         [ 1.8638e-01, -1.6738e+00,  4.2155e-01,  3.6537e-01,  1.2521e+00,\n",
      "          -1.1679e+00,  1.2520e+00,  1.5329e+00, -1.1002e-01,  5.0513e-01],\n",
      "         [-1.5158e+00,  2.4651e-01, -2.1642e+00,  4.5320e-01, -1.3168e+00,\n",
      "          -1.3068e+00, -8.2761e-01, -3.7676e-02,  8.8095e-01,  4.8955e-01],\n",
      "         [ 1.0234e+00,  5.9453e-01, -3.4365e-01,  1.0538e+00, -3.3141e-02,\n",
      "           2.3094e-01,  1.5742e+00,  1.0136e+00,  3.4149e-01, -1.3395e+00]],\n",
      "\n",
      "        [[-2.4902e+00, -9.9417e-01, -8.0143e-01, -7.5896e-01,  5.0580e-01,\n",
      "           1.3401e-01,  1.3287e-01,  2.8359e-01, -1.6637e-03, -7.9855e-01],\n",
      "         [-1.1991e+00, -3.3993e-04, -8.3694e-01, -7.4666e-01,  4.5856e-01,\n",
      "          -5.1094e-01,  4.2317e-01, -4.9384e-01,  8.6811e-01, -3.8639e-01],\n",
      "         [-1.9548e+00,  4.0870e-01,  1.6223e+00, -4.9561e-01, -1.1922e+00,\n",
      "           1.5754e+00, -8.1093e-01,  2.2073e-02, -2.1708e+00,  2.6610e-01],\n",
      "         [ 1.4616e+00,  3.4943e-01, -2.1855e-01,  1.1885e+00,  9.6025e-01,\n",
      "           6.0123e-01,  4.0623e-01, -2.2542e-01, -1.2512e-01, -1.7884e-01],\n",
      "         [ 1.2439e+00, -1.8823e+00,  1.9368e+00,  5.4822e-02, -5.9296e-01,\n",
      "          -2.3713e-02,  8.1758e-02,  1.4147e+00, -5.8235e-01, -1.4054e+00],\n",
      "         [ 9.6873e-01, -5.2829e-01, -1.3552e+00, -7.9847e-01,  7.9524e-01,\n",
      "           1.0714e+00, -5.1203e-01, -8.2532e-01, -1.3972e+00, -3.7732e-02],\n",
      "         [-3.2555e-01, -5.4605e-01, -7.9143e-01, -1.6770e-01, -1.3106e+00,\n",
      "           2.4609e-01, -1.7201e+00, -1.1952e+00, -1.3173e+00,  1.1493e+00],\n",
      "         [-5.7302e-01,  2.6565e-01,  1.4054e+00, -7.2587e-01, -2.1278e+00,\n",
      "           6.5334e-01,  1.4740e-01,  9.7220e-01,  1.9981e-01,  1.0024e+00],\n",
      "         [ 1.8114e+00, -1.4503e-03,  4.1736e-02,  1.3760e+00,  1.0127e+00,\n",
      "           3.3069e-01, -3.5127e-01, -1.7492e-01, -1.0783e+00,  1.4424e+00],\n",
      "         [-1.8244e-01,  2.3049e-01, -7.0429e-01,  2.7802e-01, -1.0637e-01,\n",
      "           3.2922e-01,  2.3827e+00,  4.2301e-01,  1.0370e+00,  2.0279e-01],\n",
      "         [ 5.0756e-01, -6.2921e-01, -2.3153e+00, -5.6637e-01, -7.3537e-01,\n",
      "          -6.5626e-01,  3.0456e-01,  1.0501e-02, -6.3081e-01,  1.7156e+00],\n",
      "         [ 9.5909e-02, -1.0722e+00, -1.2369e+00,  3.8437e-01, -1.4324e+00,\n",
      "           2.7105e-01, -7.8125e-01,  2.4991e-01, -2.1730e-01, -1.0840e+00],\n",
      "         [ 3.0493e-01, -1.0385e+00, -5.4263e-01, -2.6165e+00, -6.6840e-01,\n",
      "           3.3927e-01, -6.2271e-01, -7.2220e-01,  4.3708e-01, -9.3066e-01],\n",
      "         [-4.4092e-01, -1.0224e+00,  9.8682e-01, -2.0451e-01,  1.9211e+00,\n",
      "          -4.2134e-01,  1.5366e+00, -4.5516e-01,  5.0817e-01,  5.7843e-01]],\n",
      "\n",
      "        [[ 6.9803e-02, -1.5997e+00, -1.6851e+00, -4.0591e-01,  1.7274e+00,\n",
      "          -1.9519e+00,  3.4684e-01, -4.8851e-01, -9.9060e-01, -2.4913e-01],\n",
      "         [ 1.9451e-01,  7.2149e-02, -8.8374e-01, -2.5379e+00, -5.9167e-01,\n",
      "          -2.0122e-01, -2.2149e-01, -2.8561e-01, -6.1785e-01,  9.2104e-01],\n",
      "         [ 2.0874e-01, -4.2246e-01, -4.1860e-01,  3.9649e-01,  7.4937e-01,\n",
      "          -2.0125e+00, -3.4736e-01, -1.1147e-01,  2.1067e+00,  7.4906e-01],\n",
      "         [ 6.2475e-01, -1.7548e+00, -2.3672e+00, -1.1841e-01,  2.0005e-01,\n",
      "          -4.0137e-01,  1.1328e+00,  8.5201e-01, -8.6443e-02,  1.0585e+00],\n",
      "         [-7.8137e-01, -2.9606e-01,  9.1357e-01,  1.5277e+00, -1.3091e+00,\n",
      "           1.3442e+00,  1.2550e+00, -2.2438e-01,  2.0774e-01, -6.1237e-01],\n",
      "         [ 1.4420e+00, -5.6025e-01,  9.1750e-01, -4.4420e-01, -1.1387e+00,\n",
      "           4.8169e-01, -1.1077e+00, -8.3263e-02, -2.3938e-01,  4.9954e-01],\n",
      "         [-5.7306e-01, -8.6973e-01, -8.8126e-02,  8.3336e-01, -6.1362e-01,\n",
      "           9.2825e-01, -2.0063e+00, -1.1499e+00, -5.9539e-01, -9.9499e-01],\n",
      "         [-1.3615e+00,  2.8313e+00, -1.4097e-01, -8.2828e-03,  7.8860e-01,\n",
      "          -4.5103e-01,  2.3740e-02, -1.4906e+00, -1.7461e+00, -1.0265e+00],\n",
      "         [-1.0685e+00, -7.4589e-01, -3.2916e-01, -1.4036e-01, -5.0315e-01,\n",
      "          -7.0095e-01,  1.1976e+00,  1.4111e+00, -7.3989e-01,  7.3565e-01],\n",
      "         [ 1.7871e+00, -1.3500e+00,  4.1534e-01,  7.2553e-02,  8.6202e-01,\n",
      "          -8.1952e-01, -1.3641e+00,  1.0031e+00,  5.0127e-01,  1.3753e+00],\n",
      "         [-4.1245e-01, -6.6505e-01, -4.4263e-01, -5.4413e-01, -1.5659e-01,\n",
      "           1.0169e+00,  2.5400e-02,  1.0162e+00, -3.7895e-01,  1.1406e+00],\n",
      "         [-1.0064e+00,  1.5541e+00,  9.1227e-02,  7.5202e-01, -1.7559e+00,\n",
      "          -5.5528e-01, -1.9697e-01, -5.7036e-01, -2.4339e-01,  2.0968e+00],\n",
      "         [ 4.5117e-01,  9.8361e-01, -6.7767e-01, -5.8277e-01,  2.7368e-01,\n",
      "          -6.3849e-01, -1.8103e+00,  2.1104e+00,  1.5692e+00,  1.2015e+00],\n",
      "         [-1.3696e+00, -1.6384e+00, -2.0079e-02,  1.1459e+00,  2.9255e-01,\n",
      "           1.9043e+00, -7.6503e-01,  3.3973e-02, -4.7724e-01,  1.2633e-01]],\n",
      "\n",
      "        [[ 3.1565e-01, -2.3071e-01,  1.2835e+00,  3.0303e+00,  1.5602e-01,\n",
      "          -2.3847e-02,  2.5669e+00, -3.5601e-01, -4.6947e-01,  3.0643e-01],\n",
      "         [ 1.6393e+00,  2.0620e-01,  1.7787e+00,  7.6741e-01,  2.5023e-01,\n",
      "          -1.3045e+00, -3.0661e-01,  9.9989e-02, -6.8609e-01, -4.8419e-02],\n",
      "         [-9.0246e-02,  5.4851e-01,  4.0459e-01, -7.1488e-02,  2.7979e-01,\n",
      "           3.0294e+00,  1.0561e+00,  2.1570e-01, -9.1009e-01,  7.8279e-01],\n",
      "         [-1.6915e+00, -3.1862e-01,  1.9960e+00,  2.1810e+00,  4.5620e-02,\n",
      "           6.4955e-02,  6.3584e-01, -8.5737e-01,  1.4062e+00, -2.2241e+00],\n",
      "         [-7.3501e-01,  6.9727e-01, -1.2417e+00,  1.1870e+00,  1.9432e+00,\n",
      "           1.4137e+00,  4.4575e-01,  1.8433e+00,  4.3464e-02, -7.4371e-02],\n",
      "         [ 9.1481e-01, -1.1934e+00,  3.0803e-01, -1.7484e-01, -5.1976e-01,\n",
      "           5.4621e-02, -9.0644e-02,  1.0499e+00,  2.2769e+00,  1.7095e-01],\n",
      "         [ 1.7591e+00,  8.5337e-01, -1.8193e-01, -9.6506e-01,  9.2865e-02,\n",
      "          -6.9411e-01,  4.3682e-01, -1.2979e-01, -8.6922e-01,  4.0558e-01],\n",
      "         [-1.9862e+00, -1.3771e-01, -6.4682e-01,  4.0374e-01,  1.4386e+00,\n",
      "           1.2071e+00,  5.1795e-01, -4.9351e-02, -1.5094e+00, -2.2051e-01],\n",
      "         [ 1.5274e+00, -1.1450e-02,  7.4518e-01,  1.4440e-01, -2.4044e+00,\n",
      "          -2.7785e-01, -1.3890e+00, -1.9872e+00, -3.4713e-01, -3.8361e-01],\n",
      "         [ 6.0089e-01, -3.9952e-02, -2.3995e-01, -4.6822e-01, -1.8249e-01,\n",
      "           1.1407e+00,  1.5674e-01, -1.3659e-01, -1.6084e+00, -8.4849e-01],\n",
      "         [-5.7860e-02,  9.2891e-02,  1.3970e+00, -1.5841e+00,  7.5031e-01,\n",
      "          -8.9248e-01,  8.0912e-01,  1.3905e-01,  1.6313e-01, -6.2743e-01],\n",
      "         [ 1.2028e+00, -1.1138e-01,  9.3298e-01, -1.7088e+00, -1.6658e+00,\n",
      "           1.0576e+00,  5.3831e-01, -9.7829e-01, -6.0238e-01, -1.1274e+00],\n",
      "         [-2.8691e-01,  1.8009e+00,  3.8462e-01, -3.3895e+00,  9.3672e-01,\n",
      "          -5.2038e-01, -1.3176e+00,  3.2197e-01,  2.1046e+00, -2.2415e-01],\n",
      "         [ 1.0161e+00, -2.1356e+00, -5.2194e-01, -1.3232e+00, -8.0757e-01,\n",
      "           1.7578e-01, -3.3907e-01, -7.9825e-01,  4.9978e-01,  3.6645e-01]]])\n"
     ]
    }
   ],
   "source": [
    "# The next step is to pass the source sequences through the encoder to get the encoder output. The encoder output is a \n",
    "# tensor of shape [batch_size, max_src_seq_len, d_model].\n",
    "#\n",
    "# The operation would be as follows: encoded_src = translation_model.encode(src=src_batch, src_mask=src_mask)\n",
    "# Instead of using the translation_model, we will use a random tensor to simulate the encoder output.\n",
    "encoded_src = torch.randn(src_batch.shape[0], src_batch.shape[1], D_MODEL)\n",
    "print(\"shape of the encoded source tensor: \", encoded_src.shape)\n",
    "print(\"encoded_src: \\n\", encoded_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogSoftmax(dim=-1)\n"
     ]
    }
   ],
   "source": [
    "# Commonly used in every iteration. So, just creating it here.\n",
    "log_softmax = nn.LogSoftmax(dim=-1)\n",
    "print(log_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 1 of the target token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the target decoder input tensor:  torch.Size([4, 1])\n",
      "target_decoder_input: \n",
      " tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Now, we have to create the input for the Decoder. The input to the decoder is the target sequences. The target sequences\n",
    "# are initially empty. So, we need to start with the START_TOKEN and then keep appending the tokens predicted by the model\n",
    "# until the END_TOKEN is predicted.\n",
    "\n",
    "tgt_decoder_input_iter_1 = torch.tensor(data=[[start_token_id] for _ in range(src_batch.size(0))], dtype=torch.int32)\n",
    "print(\"shape of the target decoder input tensor: \", tgt_decoder_input_iter_1.shape)\n",
    "print(\"target_decoder_input: \\n\", tgt_decoder_input_iter_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the target mask tensor:  torch.Size([1, 1])\n",
      "target_mask: \n",
      " tensor([[True]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of the target mask tensor:  torch.Size([4, 1, 1])\n",
      "target_mask: \n",
      " tensor([[[True]],\n",
      "\n",
      "        [[True]],\n",
      "\n",
      "        [[True]],\n",
      "\n",
      "        [[True]]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of the target mask tensor:  torch.Size([4, 1, 1, 1])\n",
      "target_mask: \n",
      " tensor([[[[True]]],\n",
      "\n",
      "\n",
      "        [[[True]]],\n",
      "\n",
      "\n",
      "        [[[True]]],\n",
      "\n",
      "\n",
      "        [[[True]]]])\n"
     ]
    }
   ],
   "source": [
    "# I initially thought not to use any tgt_mask during inference since it is not strictly required. However, the results\n",
    "# are looking way too different without the tgt_mask. Did not expect such a huge difference. So, I will use the tgt_mask\n",
    "# during inference as well.\n",
    "tgt_mask = construct_look_ahead_mask(size=tgt_decoder_input_iter_1.size(1))\n",
    "print(\"shape of the target mask tensor: \", tgt_mask.shape)\n",
    "print(\"target_mask: \\n\", tgt_mask)\n",
    "print(\"-\" * 150)\n",
    "# The look ahead mask is same for every sequence in the batch and so, we repeat the mask for every sequence in the batch.\n",
    "tgt_mask = tgt_mask.unsqueeze(0).repeat(tgt_decoder_input_iter_1.size(0), 1, 1)\n",
    "print(\"shape of the target mask tensor: \", tgt_mask.shape)\n",
    "print(\"target_mask: \\n\", tgt_mask)\n",
    "print(\"-\" * 150)\n",
    "# The same attention mask is applied to all the heads in the multi-head attention mechanism. To account for this, we\n",
    "# create a tensor of shape [batch_size, 1, 1, max_tgt_seq_len] and let python broadcast it to the required shape which\n",
    "# would be [batch_size, num_heads, max_tgt_seq_len, max_tgt_seq_len].\n",
    "tgt_mask = tgt_mask.unsqueeze(1)\n",
    "print(\"shape of the target mask tensor: \", tgt_mask.shape)\n",
    "print(\"target_mask: \\n\", tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the target decoder output tensor:  torch.Size([4, 1, 10])\n",
      "target_decoder_output: \n",
      " tensor([[[-0.3742,  0.2313, -1.1678,  1.9597,  0.8790, -0.9169, -1.1343,\n",
      "           0.0150,  0.1704,  0.0639]],\n",
      "\n",
      "        [[ 0.0252, -1.4017,  0.4027,  0.5777, -0.6343, -0.2286, -0.4737,\n",
      "           1.9160, -1.6930,  0.9677]],\n",
      "\n",
      "        [[ 0.4050, -1.0697, -2.0056,  0.9286, -1.3345,  0.2843,  1.3475,\n",
      "           0.8535,  1.4111, -0.9893]],\n",
      "\n",
      "        [[-0.3084, -0.3953, -0.0435, -0.8055,  0.2498,  0.3684,  0.6436,\n",
      "           1.4065,  0.4810,  0.1747]]])\n"
     ]
    }
   ],
   "source": [
    "# The next step is to pass the target sequences through the decoder to get the decoder output. The decoder output is a\n",
    "# tensor of shape [batch_size, tgt_seq_len, d_model]. Instead of using the translation_model, we will use a random tensor\n",
    "# to simulate the decoder output.\n",
    "# This cell is just for understanding purposes. We did not use this variable anywhere below in the notebook.\n",
    "tgt_decoder_output_iter_1 = torch.randn(tgt_decoder_input_iter_1.size(0), tgt_decoder_input_iter_1.size(1), D_MODEL)\n",
    "print(\"shape of the target decoder output tensor: \", tgt_decoder_output_iter_1.shape)\n",
    "print(\"target_decoder_output: \\n\", tgt_decoder_output_iter_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the predicted log probabilities tensor:  torch.Size([4, 1, 30000])\n",
      "predicted_log_probs: \n",
      " tensor([[[-11.7448,  -9.3692, -10.9705,  ...,  -9.2906, -11.1848, -11.3099]],\n",
      "\n",
      "        [[-12.7606,  -7.4802, -11.5967,  ..., -10.2324, -10.0172,  -9.8340]],\n",
      "\n",
      "        [[ -9.1747,  -8.3682, -10.5096,  ..., -11.9631, -11.8825, -10.6328]],\n",
      "\n",
      "        [[-10.1766,  -9.3968, -10.7139,  ..., -13.2947, -11.3629, -10.3721]]])\n"
     ]
    }
   ],
   "source": [
    "# The next step is to convert the decoder output to probability distribution over the target vocabulary. This is done\n",
    "# using the token prediction layer in the translation model. Instead of using the translation_model, we will use a random\n",
    "# tensor to simulate output of the token prediction layer.\n",
    "random_tensor_iter_1 = torch.randn(tgt_decoder_output_iter_1.size(0), tgt_decoder_output_iter_1.size(1), telugu_tokenizer.get_vocab_size())\n",
    "predicted_log_probs_iter_1 = log_softmax(random_tensor_iter_1)\n",
    "print(\"shape of the predicted log probabilities tensor: \", predicted_log_probs_iter_1.shape)\n",
    "print(\"predicted_log_probs: \\n\", predicted_log_probs_iter_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the predicted last token log probabilities tensor:  torch.Size([4, 30000])\n",
      "predicted_last_tok_log_probs: \n",
      " tensor([[-11.7448,  -9.3692, -10.9705,  ...,  -9.2906, -11.1848, -11.3099],\n",
      "        [-12.7606,  -7.4802, -11.5967,  ..., -10.2324, -10.0172,  -9.8340],\n",
      "        [ -9.1747,  -8.3682, -10.5096,  ..., -11.9631, -11.8825, -10.6328],\n",
      "        [-10.1766,  -9.3968, -10.7139,  ..., -13.2947, -11.3629, -10.3721]])\n"
     ]
    }
   ],
   "source": [
    "# In this step, we will have to extract the token with maximum probability from the predicted_log_probs tensor. This token\n",
    "# will be appended to the target sequences. This process will be repeated until the END_TOKEN is predicted or the maximum\n",
    "# number of tokens is reached.\n",
    "# \n",
    "# We only care about the last token in the predicted_log_probs tensor. So, we will extract the last token log probabilities.\n",
    "predicted_last_tok_log_probs_iter_1 = predicted_log_probs_iter_1[:, -1, :]\n",
    "print(\"shape of the predicted last token log probabilities tensor: \", predicted_last_tok_log_probs_iter_1.shape)\n",
    "print(\"predicted_last_tok_log_probs: \\n\", predicted_last_tok_log_probs_iter_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the predicted tokens tensor:  torch.Size([4, 1])\n",
      "predicted_tokens: \n",
      " tensor([[ 1379],\n",
      "        [13490],\n",
      "        [ 6536],\n",
      "        [27987]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of the max probabilities tensor:  torch.Size([4, 1])\n",
      "max_probs: \n",
      " tensor([[-6.6468],\n",
      "        [-7.1067],\n",
      "        [-7.0000],\n",
      "        [-6.5555]])\n"
     ]
    }
   ],
   "source": [
    "# Extract the token with maximum probability i.e., basically the index of the token with maximum probability.\n",
    "max_probs_iter_1, predicted_tokens_iter_1 = predicted_last_tok_log_probs_iter_1.max(dim=1, keepdim=True)\n",
    "print(\"shape of the predicted tokens tensor: \", predicted_tokens_iter_1.shape)\n",
    "print(\"predicted_tokens: \\n\", predicted_tokens_iter_1)\n",
    "print(\"-\" * 150)\n",
    "print(\"shape of the max probabilities tensor: \", max_probs_iter_1.shape)\n",
    "print(\"max_probs: \\n\", max_probs_iter_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the updated target batch tensor:  torch.Size([4, 2])\n",
      "updated_target_batch: \n",
      " tensor([[    0,  1379],\n",
      "        [    0, 13490],\n",
      "        [    0,  6536],\n",
      "        [    0, 27987]])\n"
     ]
    }
   ],
   "source": [
    "updated_tgt_batch_iter_1 = torch.cat([tgt_decoder_input_iter_1, predicted_tokens_iter_1], dim=-1)\n",
    "print(\"shape of the updated target batch tensor: \", updated_tgt_batch_iter_1.shape)\n",
    "print(\"updated_target_batch: \\n\", updated_tgt_batch_iter_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 2 of the target token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets not go through all the steps again but lets see how a few additional steps get added when the predicted\n",
    "# token is end of sentence token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the updated target batch tensor:  torch.Size([4, 3])\n",
      "updated_target_batch: \n",
      " tensor([[    0,  1379,    35],\n",
      "        [    0, 13490,   567],\n",
      "        [    0,  6536,     1],\n",
      "        [    0, 27987,  7684]])\n"
     ]
    }
   ],
   "source": [
    "# Based on the updated_tgt_batch_iter_1, lets create the updated_tgt_batch_iter_2 tensor manually.\n",
    "updated_tgt_batch_iter_2 = updated_tgt_batch_iter_1.clone()\n",
    "random_token_ids_iter_2 = torch.tensor(data=[[35], [567], [1], [7684]], dtype=torch.int32)\n",
    "updated_tgt_batch_iter_2 = torch.cat([updated_tgt_batch_iter_2, random_token_ids_iter_2], dim=-1)\n",
    "print(\"shape of the updated target batch tensor: \", updated_tgt_batch_iter_2.shape)\n",
    "print(\"updated_target_batch: \\n\", updated_tgt_batch_iter_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, the next set of token predictions should only run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
