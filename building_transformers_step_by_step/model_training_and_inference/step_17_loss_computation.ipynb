{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) How to use KL Divergence loss in the transformer model?\n",
    "#\n",
    "# Resources to learn more about KL Divergence:\n",
    "# 1) https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained\n",
    "#       -- Gives and intuitive explanation of KL Divergence\n",
    "# 2) https://encord.com/blog/kl-divergence-in-machine-learning/\n",
    "#       -- Similar to 1 but explains more in the context of machine learning.\n",
    "# 3) https://dibyaghosh.com/blog/probability/kldivergence.html\n",
    "#       -- Explains the math behind KL Divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the padding token or the class label for the padding token.\n",
    "padding_idx = 2\n",
    "# Amount of probability to be shared among the tokens excluding correct token and padding tokens.\n",
    "smoothing = 0.1\n",
    "# Amount of probability shared with the correct token.\n",
    "confidence = 1 - smoothing\n",
    "# Number of classes in the classification problem. It is the size of the vocabulary in transformers.\n",
    "# It includes the padding token.\n",
    "vocab_size = 6\n",
    "# Number of sentences in the batch.\n",
    "batch_size = 2\n",
    "# Number of tokens in each sentence. Please note that, for the src sentences the 'seq_len' will\n",
    "# be 9 and for the tgt sentences, the 'seq_len' will be 8. In this notebook, since we are focusing on\n",
    "# the KL Divergence loss, we will only consider the tgt sentences and use 'seq_len' variable is not\n",
    "# distinguished between src and tgt sentences.\n",
    "seq_len = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that the term token is used loosely here. I am using it to refer to the probability \n",
    "# distribution over the vocabulary for a particular token (could be a word). I am also using it \n",
    "# to refer to the token (could be a word) itself. Please differentiate the usage based on the \n",
    "# context in which the term token is used.\n",
    "#\n",
    "# Decoder has an input sentence, a predicted sentence as its output and a target output sentence.\n",
    "# A sentence is made of multiple tokens. An output token is predicted for each decoder input \n",
    "# token. The predicted output token is basically a probability distribution over the target \n",
    "# vocabulary. We convert this probability distribution into a token (predicted token) by finding \n",
    "# out the token in the target vocabulary with maximum probability. The decoder target output per \n",
    "# token is a Label Smoothed version of the one-hot encoded target token i.e., the target output \n",
    "# is a probability distribution over the vocabulary. For each predicted token, the KL Divergence \n",
    "# is calculated between the predicted distribution and the target distribution. For each sentence, \n",
    "# the KL Divergence loss is summed over all the tokens in the sentence. The summed loss is then \n",
    "# averaged over all the tokens in the batch. The final loss per token is then used to update the \n",
    "# model parameters using backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 8, 6])\n",
      "predictions: \n",
      " tensor([[[-1.4787,  0.3393,  0.8852, -2.6089,  0.1993,  1.2473],\n",
      "         [ 0.7730, -0.5182,  1.1035, -0.9056,  1.9427,  0.0062],\n",
      "         [-0.3036,  1.5448, -0.7564,  1.4636,  0.1306,  0.1318],\n",
      "         [ 0.0919, -0.5379, -0.9243,  0.8664, -0.0310, -0.4949],\n",
      "         [ 0.4991,  1.7868, -0.9093, -0.4672,  0.6120,  0.9794],\n",
      "         [ 0.9309, -1.6936,  0.0163,  1.6846,  0.1531, -0.8738],\n",
      "         [ 0.1850, -0.1906, -0.0452,  0.5845,  0.3870, -0.8785],\n",
      "         [ 0.4096, -1.3969, -1.7540, -1.6454,  0.8043, -0.3939]],\n",
      "\n",
      "        [[-1.8552, -1.5187, -0.2340, -1.4955, -0.4870,  0.3772],\n",
      "         [-1.0312, -0.4752, -0.4100,  1.6838, -0.1283, -0.5884],\n",
      "         [ 2.4114, -0.9768,  0.3813,  0.2883,  0.4947, -1.6481],\n",
      "         [ 0.9232,  0.1053,  0.8024, -0.2156, -1.3285, -0.0193],\n",
      "         [-0.5860, -0.6488,  0.0139,  1.1086,  0.4817,  1.6839],\n",
      "         [-0.8347, -0.7269, -0.0947, -0.3415, -0.6350,  0.6456],\n",
      "         [ 1.2797, -0.1782,  0.3955,  0.5594, -0.4987,  0.5324],\n",
      "         [ 1.4742,  0.8687,  0.8159, -0.8802,  0.6776, -0.7476]]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([2, 8, 6])\n",
      "log_softmax_predictions: \n",
      " tensor([[[-3.6569, -1.8389, -1.2931, -4.7871, -1.9789, -0.9309],\n",
      "         [-1.8778, -3.1689, -1.5472, -3.5563, -0.7080, -2.6446],\n",
      "         [-2.8291, -0.9806, -3.2819, -1.0618, -2.3949, -2.3937],\n",
      "         [-1.7056, -2.3354, -2.7218, -0.9311, -1.8285, -2.2924],\n",
      "         [-2.0777, -0.7899, -3.4861, -3.0439, -1.9648, -1.5973],\n",
      "         [-1.4403, -4.0648, -2.3549, -0.6866, -2.2181, -3.2450],\n",
      "         [-1.7122, -2.0879, -1.9425, -1.3128, -1.5103, -2.7757],\n",
      "         [-1.2056, -3.0121, -3.3692, -3.2607, -0.8110, -2.0092]],\n",
      "\n",
      "        [[-3.0975, -2.7610, -1.4763, -2.7379, -1.7293, -0.8651],\n",
      "         [-3.1669, -2.6109, -2.5456, -0.4519, -2.2640, -2.7241],\n",
      "         [-0.3710, -3.7591, -2.4010, -2.4940, -2.2876, -4.4304],\n",
      "         [-1.1451, -1.9629, -1.2659, -2.2838, -3.3968, -2.0876],\n",
      "         [-3.0816, -3.1444, -2.4817, -1.3870, -2.0139, -0.8116],\n",
      "         [-2.4391, -2.3313, -1.6991, -1.9458, -2.2394, -0.9588],\n",
      "         [-1.0206, -2.4786, -1.9048, -1.7409, -2.7990, -1.7679],\n",
      "         [-0.9998, -1.6053, -1.6581, -3.3542, -1.7964, -3.2216]]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lets create a random prediction probability distribution tensor to use with the KL Divergence loss.\n",
    "predictions = torch.randn(size=(batch_size, seq_len, vocab_size), dtype=torch.float32)\n",
    "print(\"shape: \", predictions.shape)\n",
    "print(\"predictions: \\n\", predictions)\n",
    "print(\"-\" * 150)\n",
    "# The prediction probability distribution is expected to be in the log space by the KL Divergence \n",
    "# loss object in pytorch.\n",
    "log_softmax = nn.LogSoftmax(dim=-1)\n",
    "log_softmax_predictions = log_softmax(predictions)\n",
    "print(\"shape: \", log_softmax_predictions.shape)\n",
    "print(\"log_softmax_predictions: \\n\", log_softmax_predictions)\n",
    "print(\"-\" * 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP THIS CELL IF YOU ALREADY LOOKED INTO THE STEP_16 NOTEBOOK. THE CODE FROM PREVIOUS NOTEBOOK IS JUST COPIED HERE \n",
    "# TO BE USED IN LOSS COMPUTATION.\n",
    "# \n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# JUST RUN THIS CELL BLINDLY | JUST RUN THIS CELL BLINDLY | JUST RUN THIS CELL BLINDLY | JUST RUN THIS CELL BLINDLY \n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# This code block is copied from the 'step_19_label_smoothing.ipynb' notebook and discussed in detail there.\n",
    "# Please refer to that notebook for more details.\n",
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size: int, padding_idx: int, smoothing: Optional[int]=0.1):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        # Number of classes in the classification problem. It is the size of the vocabulary in transformers.\n",
    "        self.vocab_size = tgt_vocab_size\n",
    "        # Index of the padding token or the class label for the padding token.\n",
    "        self.padding_idx = padding_idx\n",
    "        # Amount of probability to be shared among the tokens excluding correct token and padding tokens.\n",
    "        self.smoothing = smoothing\n",
    "        # Amount of probability shared with the correct token.\n",
    "        self.confidence = 1 - smoothing\n",
    "    \n",
    "    def forward(self, targets: Tensor) -> Tensor:\n",
    "        \"\"\"Calculates the smoothed probabilities for each of the target tokens within each sentence.\n",
    "\n",
    "        Args:\n",
    "            targets (Tensor): The target tensor containing the correct class labels (expected token indices from the \n",
    "                              vocab) for each token in the batch. An example target tensor for a batch of 2 sentences\n",
    "                              each with 8 tokens and 6 possible classes for prediction (including the padding token)\n",
    "                              would be: [[0, 3, 4, 5, 5, 1, 2, 2], [1, 5, 3, 3, 4, 0, 0, 2]]\n",
    "                              shape: [batch_size, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: A smoothed probability distribution (1D tensor) for each target token in the batch.\n",
    "                    shape: [batch_size, seq_len, vocab_size]                    \n",
    "        \"\"\"\n",
    "        batch_size, seq_len = targets.shape\n",
    "        # Creating a tensor that will hold the smoothed probabilities for each target token in all the sentences.\n",
    "        smoothed_probs = torch.zeros(size=(batch_size, seq_len, self.vocab_size), dtype=torch.float32)\n",
    "        # Filling the entire tensor with the smoothing probability. We will deal with the probabilities of the\n",
    "        # correct token and padding token later.\n",
    "        smoothed_probs = smoothed_probs.fill_(value=self.smoothing / (self.vocab_size - 2))\n",
    "        # Bringing the targets tensor to contain the same number of dimensions as the smoothed_probs tensor to use\n",
    "        # it with the scatter_ function inorder replace the probabilities in the smoothed_probs tensor in the next \n",
    "        # step.\n",
    "        unsqueezed_targets = targets.unsqueeze(dim=-1)\n",
    "        # Replacing the probabilities in the smoothed_probs tensor with the confidence probability at the \n",
    "        # positions that correspond to the correct class labels (expected output tokens in the target).\n",
    "        smoothed_probs.scatter_(dim=-1, index=unsqueezed_targets, value=self.confidence)\n",
    "        # The padding token should not be predicted at all by the model. So, the probability associated with the\n",
    "        # class label that correspond to the padding token within each target token distribution should be 0. \n",
    "        smoothed_probs[:, :, self.padding_idx] = 0\n",
    "        # The target tensor is appended with the padding tokens at the end. These are just dummy tokens added to bring \n",
    "        # all the sentences in the batch to the same length. We don't want the model to consider these tokens at all \n",
    "        # in the loss calculation. So, we set the probabilities of the entire rows corresponding to the padding tokens\n",
    "        # to 0. More about why this setup works is explained in the notebook 'step_17_loss_computation.ipynb'.\n",
    "        mask = unsqueezed_targets.repeat(1, 1, self.vocab_size) == self.padding_idx\n",
    "        return smoothed_probs.masked_fill(mask=mask, value=0.0)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# CELL CONTAINING THE COPIED CODE FROM PREVIOUS NOTEBOOKS ENDS HERE.\n",
    "# CELL CONTAINING THE COPIED CODE FROM PREVIOUS NOTEBOOKS ENDS HERE.\n",
    "# CELL CONTAINING THE COPIED CODE FROM PREVIOUS NOTEBOOKS ENDS HERE.\n",
    "# -------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 8])\n",
      "transformer_target_ids: \n",
      " tensor([[0, 3, 4, 5, 5, 1, 2, 2],\n",
      "        [1, 5, 3, 3, 4, 0, 0, 2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([2, 8, 6])\n",
      "targets: \n",
      " tensor([[[0.9000, 0.0250, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.9000, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.9000, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.0250, 0.9000],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.0250, 0.9000],\n",
      "         [0.0250, 0.9000, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0250, 0.9000, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.0250, 0.9000],\n",
      "         [0.0250, 0.0250, 0.0000, 0.9000, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.9000, 0.0250, 0.0250],\n",
      "         [0.0250, 0.0250, 0.0000, 0.0250, 0.9000, 0.0250],\n",
      "         [0.9000, 0.0250, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.9000, 0.0250, 0.0000, 0.0250, 0.0250, 0.0250],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# Creating random target tensor to use with the Label Smoothing object.\n",
    "# target = [[0, 3, 4, 5, 5, 1, 2, 2], [1, 5, 3, 3, 4, 0, 0, 2]] \n",
    "# target[0][0] = 0 --> For the first sentence and the zeroth token, the expected output token is 0.\n",
    "# target[0][1] = 3 --> For the first sentence and the first token, the expected output token is 3.\n",
    "# target[0][2] = 4 --> For the first sentence and the second token, the expected output token is 4.\n",
    "# ...\n",
    "# target[1][7] = 2 --> For the second sentence and the seventh token (last), the expected output token is 2.\n",
    "#       -- 2 is a pad token which means this token is not expected to be used in the loss computation. \n",
    "transformer_target_ids = torch.tensor(data=[[0, 3, 4, 5, 5, 1, 2, 2], [1, 5, 3, 3, 4, 0, 0, 2]], dtype=torch.int64)\n",
    "print(\"shape: \", transformer_target_ids.shape)\n",
    "print(\"transformer_target_ids: \\n\", transformer_target_ids)\n",
    "print(\"-\" * 150)\n",
    "# targets need not be in the log space inorder to be used with the KL Divergence loss unlike with predictions.\n",
    "# Applying the Label Smoothing to the target tensor before computing the loss.\n",
    "targets = LabelSmoothing(tgt_vocab_size=vocab_size, padding_idx=padding_idx, smoothing=smoothing)(targets=transformer_target_ids)\n",
    "print(\"shape: \", targets.shape)\n",
    "print(\"targets: \\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_non_pad_tokens:  13\n"
     ]
    }
   ],
   "source": [
    "# This information (number of non-pad tokens) is used to calculate the KL Divergence per token in the batch\n",
    "# i.e., the KL divergence is averaged over all the tokens in the batch. This per token KL Divergence is \n",
    "# then used as an objective function to train the whole model.\n",
    "num_non_pad_tokens = (transformer_target_ids != padding_idx).sum().item()\n",
    "print(\"num_non_pad_tokens: \", num_non_pad_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLDivLoss()\n"
     ]
    }
   ],
   "source": [
    "# We use the 'sum' reduction to sum the KL Divergence over all the tokens in all the sentences in the batch. The loss is \n",
    "# then averaged over all the tokens in the batch to find the loss per token which is used as the objective function. \n",
    "#\n",
    "# Refer to 'UnderstandingPytorch/miscellaneous/loss_functions.ipynb' (Different repository) to learn more about \n",
    "# nn.KVDivLoss. The use of this loss function will get much clearer after going through the 'loss_functions.ipynb' \n",
    "# notebook.\n",
    "kl_div_loss_obj = nn.KLDivLoss(reduction=\"sum\")\n",
    "print(kl_div_loss_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([])\n",
      "model_kl_loss: \n",
      " tensor(16.2052)\n",
      "----------------------------------------------------\n",
      "model_kl_loss_per_token:  tensor(1.2466)\n"
     ]
    }
   ],
   "source": [
    "# Calculates the KL Divergence loss between the model predictions and the targets. The KL Divergence loss is calculated per \n",
    "# token in the batch. If the target token is a padding token, the probability of the padding token in the predicted \n",
    "# distribution is set to zero (0) during Label Smoothing. If the target probability is set to zero, it will not contribute \n",
    "# to the KL Divergence loss i.e., it's contribution to the loss is zero. This is shown in the \n",
    "# 'Understanding_Pytorch/miscellaneous/loss_functions.ipynb' notebook. Please refer to that notebook for more details.\n",
    "model_kl_loss = kl_div_loss_obj(input=log_softmax_predictions, target=targets)\n",
    "print(\"shape: \", model_kl_loss.shape)\n",
    "print(\"model_kl_loss: \\n\", model_kl_loss)\n",
    "print(\"-\" * 150)\n",
    "# Calculate the KL Divergence loss per token in the batch.\n",
    "model_kl_loss_per_token = model_kl_loss / num_non_pad_tokens\n",
    "print(\"model_kl_loss_per_token: \", model_kl_loss_per_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines the code from the above cells into a simple class that computes the KL Divergence loss.\n",
    "class LossCompute:\n",
    "    def __init__(self):\n",
    "        # We use the 'sum' reduction to sum the KL Divergence over all the tokens in all the sentences in the batch. \n",
    "        # The loss is then averaged over all the tokens in the batch to find the loss per token which is used as the \n",
    "        # objective function.         \n",
    "        self.kl_div_loss = nn.KLDivLoss(reduction=\"sum\")\n",
    "\n",
    "    # The '__call__' method allows an object of the class to be called just like a function.\n",
    "    def __call__(self, log_predictions: Tensor, targets: Tensor, num_non_pad_tokens: int) -> Tensor:\n",
    "        \"\"\"Computes the KL Divergence loss for the model predictions and the target tensor.\n",
    "\n",
    "        Args:\n",
    "            log_predictions (Tensor): The log of the model predictions for the target tokens in the batch.\n",
    "                                      Each token has a probability distribution over the vocabulary.\n",
    "                                      shape: [batch_size, seq_len, vocab_size]\n",
    "            targets (Tensor): The expected target for the model predictions. The target tensor is a smoothed\n",
    "                              probability distribution over the vocabulary for each token in the batch. \n",
    "                              shape: [batch_size, seq_len, vocab_size]\n",
    "            num_non_pad_tokens (int): The number of non-pad tokens in the target of the batch.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The KL Divergence per token in the batch which is used as the objective function for model\n",
    "                    training.\n",
    "        \"\"\"\n",
    "        # Calculates the KL Divergence loss between the model predictions and the targets.\n",
    "        kl_div_loss = self.kl_div_loss(input=log_predictions, target=targets)\n",
    "        # Calculate the KL Divergence loss per token in the batch.\n",
    "        return kl_div_loss / num_non_pad_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2466)\n"
     ]
    }
   ],
   "source": [
    "loss_compute = LossCompute()\n",
    "# This should give the same result as the model_kl_loss_per_token calculated above.\n",
    "model_loss = loss_compute(log_predictions=log_softmax_predictions, targets=targets, num_non_pad_tokens=num_non_pad_tokens)\n",
    "print(model_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
