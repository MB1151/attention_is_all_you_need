{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) How to measure the quality of a machine translation model?\n",
    "# 2) How to calculate BLEU score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources to understand BLEU score:\n",
    "#\n",
    "# 1) https://youtu.be/DejHQYAGb7Q?si=no3A70rxzxnFywXd\n",
    "#       -- Video by Andrew NG explaining BLEU score and how to calculate it.\n",
    "# 2) https://blog.modernmt.com/understanding-mt-quality-bleu-scores/\n",
    "#       -- Blog post explaining the advantages and disadvantages of BLEU score.\n",
    "# 3) https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b\n",
    "#       -- Blog post explaining the math behind BLEU score clearly.\n",
    "# 4) https://docs.google.com/document/d/1OPldZW_9NbG8JLywnqJ91yJV9olP9npG8FJG9csnpwc/edit?tab=t.0#bookmark=id.cbldv5yohjf9\n",
    "#       -- Google document explaining why BLEU score is always in the range [0, 1].\n",
    "#       -- This is a conversation between me and Gemini.\n",
    "# 5) https://docs.google.com/document/d/1OPldZW_9NbG8JLywnqJ91yJV9olP9npG8FJG9csnpwc/edit?tab=t.0#bookmark=id.2uvpt29pxts9\n",
    "#       -- Google document explaining how BLEU score is calculated for a corpus of translations as opposed to a single translation.\n",
    "# 6) https://docs.google.com/document/d/1OPldZW_9NbG8JLywnqJ91yJV9olP9npG8FJG9csnpwc/edit?tab=t.0#bookmark=id.nfzxm12zp3bu\n",
    "#       -- Google document showing a running example of how BLEU score is calculated for a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see how to use 'sacrebleu' package to calculate the BELU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 8.15k/8.15k [00:00<00:00, 16.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "sacrebleu = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"sacrebleu\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\n",
       "Produces BLEU scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n",
       "    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n",
       "    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n",
       "        - `'none'`: no smoothing\n",
       "        - `'floor'`: increment zero counts\n",
       "        - `'add-k'`: increment num/denom by k for n>1\n",
       "        - `'exp'`: exponential decay\n",
       "    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n",
       "    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n",
       "        - `'none'`: No tokenization.\n",
       "        - `'zh'`: Chinese tokenization.\n",
       "        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n",
       "        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n",
       "        - `'char'`: Language-agnostic character-level tokenization.\n",
       "        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n",
       "    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n",
       "    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n",
       "    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n",
       "\n",
       "Returns:\n",
       "    'score': BLEU score,\n",
       "    'counts': Counts,\n",
       "    'totals': Totals,\n",
       "    'precisions': Precisions,\n",
       "    'bp': Brevity penalty,\n",
       "    'sys_len': predictions length,\n",
       "    'ref_len': reference length,\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1:\n",
       "        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n",
       "        >>> sacrebleu = evaluate.load(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        100.0\n",
       "\n",
       "    Example 2:\n",
       "        >>> predictions = [\"hello there general kenobi\",\n",
       "        ...                 \"on our way to ankh morpork\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n",
       "        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n",
       "        >>> sacrebleu = evaluate.load(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions,\n",
       "        ...                             references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        39.8\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_1 = [\"Hello My Name is Kabil\"]\n",
    "references_1 = [\n",
    "    [\"I am Kabil\", \"My Name is Kabil\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'score': 66.87403049764218, 'counts': [4, 3, 2, 1], 'totals': [5, 4, 3, 2], 'precisions': [80.0, 75.0, 66.66666666666667, 50.0], 'bp': 1.0, 'sys_len': 5, 'ref_len': 4}\n"
     ]
    }
   ],
   "source": [
    "# Lets go through the output returned by sacrebleu and understand what each term means.\n",
    "# Please note that though these explanations work here for this example, they might not be accurate for all cases\n",
    "# because the tokenization used internally by sacrebleu might be different and the words might be tokenized \n",
    "# differently which might affect the n-grams produced.\n",
    "# \n",
    "# score (66.874): This is the BLEU score, given as a percentage. The actual BLEU score is this number divided by 100 which is 0.66 in this case.\n",
    "# counts ([4, 3, 2, 1]): This gives the number of n-grams matched in the candidate and reference translations.\n",
    "#       -- counts[0] = 4: Number of matching uni-grams i.e., single words.\n",
    "#               -- My, Name, is, Kabil\n",
    "#       -- counts[1] = 3: Number of matching bi-grams i.e., two words.\n",
    "#               -- My Name, Name is, is Kabil\n",
    "#       -- counts[2] = 2: Number of matching tri-grams i.e., three words.\n",
    "#               -- My Name is, Name is Kabil\n",
    "#       -- counts[3] = 1: Number of matching 4-grams i.e., four words.\n",
    "#               -- My Name is Kabil\n",
    "# totals ([4, 3, 2, 1]): This gives the total number of n-grams in the candidate translations.\n",
    "#       -- totals[0] = 5: Number of uni-grams in the candidate translation.\n",
    "#               -- Hello, My, Name, is, Kabil\n",
    "#       -- totals[1] = 4: Number of bi-grams in the candidate translation.\n",
    "#               -- Hello My, My Name, Name is, is Kabil\n",
    "#       -- totals[2] = 3: Number of tri-grams in the candidate translation.\n",
    "#               -- Hello My Name, My Name is, Name is Kabil\n",
    "#       -- totals[3] = 2: Number of 4-grams in the candidate translation.\n",
    "#               -- Hello My Name is, My Name is Kabil\n",
    "# precisions ([0.8, 0.75, 0.6666666666666666, 0.5]): This gives the precision for each n-gram size (Not each individual n-gram).\n",
    "#       -- precision for each n-gram size is calculated as follows: counts[n] / totals[n]. This is multiplied by 100 to get the percentage.\n",
    "#       -- precisions[0] = counts[0] / totals[0] = 4/5 = 0.8 \n",
    "#       -- precisions[1] = counts[1] / totals[1] = 3/4 = 0.75\n",
    "#       -- precisions[2] = counts[2] / totals[2] = 2/3 = 0.6666666666666666\n",
    "#       -- precisions[3] = counts[3] / totals[3] = 1/2 = 0.5\n",
    "# sys_len (5): This is the length of the candidate translation.\n",
    "# ref_len (4): This is the length of the reference translations.\n",
    "#       -- I DIDN'T UNDERSTAND WHY THIS IS 4. MY GUESS FOR NOW IS THAT THIS IS BECAUSE OF THE TOKENIZATION BEING USED INTERNALLY\n",
    "#          BY SACREBLEU.\n",
    "# bp (1.0): This is the brevity penalty. This is 1.0 because the candidate translation is longer than the reference translations i.e.,\n",
    "#           sys_len > ref_len.\n",
    "bleu_1 = sacrebleu.compute(predictions=predictions_1, references=references_1)\n",
    "print(type(bleu_1))\n",
    "print(bleu_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2 = [\"Hey, that is the great Sachin Tendulkar\", \"It is not easy to learn machine learning from scratch in a short time\"]\n",
    "references_2 = [\n",
    "    [\"That is sachin tendulkar\", \"Great Sachin Tendulkar is here\"],\n",
    "    [\"Not easy to learn machine learning from scratch in a short time\", \"It is hard to learn machine learning in a short time\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 57.47078645171894, 'counts': [16, 12, 9, 8], 'totals': [22, 20, 18, 16], 'precisions': [72.72727272727273, 60.0, 50.0, 50.0], 'bp': 1.0, 'sys_len': 22, 'ref_len': 17}\n"
     ]
    }
   ],
   "source": [
    "bleu_2 = sacrebleu.compute(predictions=predictions_2, references=references_2)\n",
    "print(bleu_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, Lets see how to use 'nltk' library to calculate the BELU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for predictions_1 using nltk: 0.7062594378058554\n",
      "BLEU score for predictions_2 using nltk: 0.8447728290556981\n"
     ]
    }
   ],
   "source": [
    "# Notice that the BLEU scores calculated using nltk are different from the BLEU scores calculated using sacrebleu.\n",
    "# This could be because of the different tokenization used by sacrebleu and nltk or because of the different way in which\n",
    "# the brevity penalty is calculated. \n",
    "# \n",
    "# Calculate BLEU score for the first set of predictions and references\n",
    "bleu_1_nltk = corpus_bleu(references_1, predictions_1)\n",
    "print(f\"BLEU score for predictions_1 using nltk: {bleu_1_nltk}\")\n",
    "\n",
    "# Calculate BLEU score for the second set of predictions and references\n",
    "bleu_2_nltk = corpus_bleu(references_2, predictions_2)\n",
    "print(f\"BLEU score for predictions_2 using nltk: {bleu_2_nltk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
