{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) How to implement Beam Search for Neural Machine Translation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources to understand Beam Search:\n",
    "#\n",
    "# 1) https://www.youtube.com/watch?v=RLWuzLLSIgw\n",
    "#       -- Intuitive and simple explanation of Beam Search algorithm.\n",
    "# 2) https://www.youtube.com/watch?v=gb__z7LlN_4\n",
    "#       -- Improvements to Beam Search.\n",
    "# 3) https://www.youtube.com/watch?v=ZGUZwk7xIwk\n",
    "#       -- Error analysis of Beam Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already know about Beam Search, this is an implementation of the basic version of Beam Search for \n",
    "# Machine Translation. Lets go through the implementation logic at a high level before we dive into the code.\n",
    "# \n",
    "# The input to beam search is a batch of source sequences -- [batch_size, source_sequence_length].\n",
    "# Each 1D tensor in the batch represents a source sequence where each element is a token (from source vocabulary) \n",
    "# in the sequence.\n",
    "#\n",
    "# The output of beam search is a batch of target sequences (or translated sequences) -- [batch_size, target_sequence_length].\n",
    "# Each 1D tensor in the batch represents a target sequence where each element is a token (from target vocabulary) \n",
    "# in the sequence.\n",
    "#\n",
    "# In Beam Search, at every position, instead of just predicting the token with the highest probability, we keep \n",
    "# track of top 'beam_width' number of tokens with the largest probabilities. So, we will have 'beam_width' \n",
    "# number of potential target sequences at each time step. The 'beam_width' is a hyperparameter that we need to \n",
    "# set before running the beam search algorithm.\n",
    "#\n",
    "# Now going into the details, at each time step, each source sequence can have multiple potential target \n",
    "# sequences (or tokens) since we keep track 'beam_width' number of potential target sequences at each time step.\n",
    "# So, along with the target sequences, we need to keep track of the index of the source sequence this target \n",
    "# sequence (prediction) belongs to in the batch. The state of each target sequence is stored in the \n",
    "# 'SequenceState' (below in the code) object.\n",
    "#\n",
    "# During the beam search, some of the target sequences may reach the end of the sequence (<eos> token predicted \n",
    "# for that target sequence) before others. We need to keep track of the target sequences that have reached the \n",
    "# end of the sequence. We store the target sequences that have reached the end of the sequence in the \n",
    "# 'complete_state' dictionary (below in the code). The key of the dictionary is the index of the source sequence \n",
    "# in the source batch and the value is the 'SequenceState' object that has reached the end of the sequence (tgt) \n",
    "# and has the highest probability among all the sequences that reached the end. The list of target sequences \n",
    "# that haven't reached the end of the sequence are stored in the 'running_state' list.\n",
    "#\n",
    "# The beam search algorithm is iterated 'TGT_SEQ_LIMIT' number of times. At each iteration, we predict the next \n",
    "# token for each target sequence in the 'running_state' list. The 'running_state' and 'complete_state' are \n",
    "# updated based on the predicted tokens.\n",
    "#\n",
    "# The state update logic is as follows:\n",
    "# 1) If the target sequence has reached the end of the sequence, then we store the target sequence in the \n",
    "#    'complete_state' if this target sequence has the highest probability among all the target sequences that \n",
    "#    have reached the end of the sequence.\n",
    "# 2) If the target sequence hasn't reached the end of the sequence, then this target sequence is a potential \n",
    "#    target sequence to be considered for the next iteration. For each (source sequence, target sequence) pair, \n",
    "#    we retrieve 'beam_width' number of potential tokens with the highest probability from the model. We will \n",
    "#    now have a list of target sequences for a specific source sequence. We will sort this list based on the \n",
    "#    probability of the target sequences and keep only the 'beam_width' number of target sequences with the \n",
    "#    highest probability for the next iteration.\n",
    "# 3) If all the target sequences for a source sequence have reached the end of the sequence, then this source \n",
    "#    sequence will not be considered for the next iteration.\n",
    "# \n",
    "# Once we have iterated 'TGT_SEQ_LIMIT' number of times, we will have the 'complete_state' dictionary with the \n",
    "# target sequences that have the highest probability for most (hopefully) of the source sequence in the batch. \n",
    "# However, some of the source sequences may not have any target sequences in the 'complete_state' dictionary. \n",
    "# In this case, we will consider the target sequence with the highest probability from the 'running_state' \n",
    "# list that for these specific source sequence and add them to the 'complete_state' dictionary.\n",
    "#\n",
    "# Finally, we will have the target sequences with the highest probability for each source sequence in the \n",
    "# batch stored in the 'complete_state' dictionary. We will return these target sequences as the output of the \n",
    "# beam search algorithm.\n",
    "#\n",
    "# Back tracking a bit, lets expand on the part where we have a 'running_state' and we need to update it based \n",
    "# on the predicted tokens. As noted above, the 'running_state' might contain multiple target sequences for a \n",
    "# source sequence. We are passing these (source sequence, target sequence) pairs to the decoder part of the \n",
    "# model to get the probability distribution over the target vocabulary. So, we will have to construct the \n",
    "# input to the decoder from the 'running_state' list. The input to the decoder is a batch of source sequences \n",
    "# and target sequences. So, we bascially copy the source sequence as many times as it is used in the \n",
    "# 'running_state' list and form the source sequence batch.\n",
    "# -- This might sound a bit confusing, but you will understand (hopefully) it better when you see the code. \n",
    "# src_mask is also constructed in a similar way. The target sequence batch is constructed by taking the \n",
    "# target sequences from the 'running_state' list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from torch import Tensor\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the embeddings and other intermediate vector representations.\n",
    "D_MODEL = 6\n",
    "# Number of sequences in the batch.\n",
    "BATCH_SIZE  = 2\n",
    "# Number of tokens in the source sequences.\n",
    "SRC_SEQ_LEN = 3\n",
    "# Number of potential tokens to be considered for every position in the target sequences.\n",
    "BEAM_WIDTH = 3\n",
    "# Start of the sequence token id.\n",
    "SOS_TOKEN_ID = 0\n",
    "# End of the sequence token id.\n",
    "EOS_TOKEN_ID = 3\n",
    "# Maximum number of tokens allowed in the target sequences.\n",
    "TGT_SEQ_LIMIT = 6\n",
    "# Size of the target vocabulary.\n",
    "TGT_VOCAB_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Items useful for implementing Beam Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for storing the state of the beam search.\n",
    "\n",
    "# Contains the state of the sequence that is predicted by the beam search.\n",
    "@dataclass\n",
    "class SequenceState:\n",
    "    # Index of the source sequence for which the current target sequence has been predicted.\n",
    "    index: int\n",
    "    # Sequence of tokens in the target prediction. This is a 1D tensor.\n",
    "    tokens: Tensor\n",
    "    # Log of the probability that this is the translation for the source sequence.\n",
    "    log_prob: float\n",
    "\n",
    "\n",
    "# Holds the state of Beam search.\n",
    "class SearchState:\n",
    "    def __init__(self):\n",
    "        # Holds the final translations (predictions) for the source sequences keyed by the source \n",
    "        # sequence index in the batch.\n",
    "        self.complete_state: Dict[int, SequenceState] = {}\n",
    "        # Holds the tgt sequences that are not complete (<eos> token not predicted yet) and are \n",
    "        # being predicted.\n",
    "        self.running_state: List[SequenceState] = []\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Complete State: {self.complete_state}\\nRunning State: {self.running_state}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a random mask.\n",
    "def generate_random_mask(batch_size: int, seq_len: int) -> Tensor:\n",
    "    src_mask = torch.randn(seq_len, seq_len) > 0 \n",
    "    src_mask = src_mask.unsqueeze(0).unsqueeze(0)\n",
    "    src_mask = src_mask.repeat(batch_size, 1, 1, 1)\n",
    "    print(\"shape of src_mask: \", src_mask.shape)\n",
    "    print(\"src_mask: \\n\", src_mask)\n",
    "    print(\"-\" * 150)\n",
    "    return src_mask\n",
    "\n",
    "def construct_look_ahead_mask(batch_size, seq_len: int) -> Tensor:\n",
    "    attention_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.uint8), diagonal=1)\n",
    "    return (attention_mask == 0).unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoded_src: torch.Size([2, 3, 6])\n",
      "encoded_src:\n",
      " tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
      "         [ 6.,  7.,  8.,  9., 10., 11.],\n",
      "         [12., 13., 14., 15., 16., 17.]],\n",
      "\n",
      "        [[18., 19., 20., 21., 22., 23.],\n",
      "         [24., 25., 26., 27., 28., 29.],\n",
      "         [30., 31., 32., 33., 34., 35.]]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of src_mask:  torch.Size([2, 1, 3, 3])\n",
      "src_mask: \n",
      " tensor([[[[False,  True, False],\n",
      "          [False,  True,  True],\n",
      "          [False, False, False]]],\n",
      "\n",
      "\n",
      "        [[[False,  True, False],\n",
      "          [False,  True,  True],\n",
      "          [False, False, False]]]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create an example tensor that serves as encoded source and corresponding mask for experimentation.\n",
    "encoded_src = torch.arange(36, dtype=torch.float32).reshape(BATCH_SIZE, SRC_SEQ_LEN, D_MODEL)\n",
    "print(\"shape of encoded_src:\", encoded_src.shape)\n",
    "print(\"encoded_src:\\n\", encoded_src)\n",
    "print(\"-\" * 150)\n",
    "# Generating a random mask for experimentation. This might not be a valid mask in real scenarios.\n",
    "src_mask = generate_random_mask(BATCH_SIZE, SRC_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(beam_state.running_state):  2\n",
      "beam_state.running_state: \n",
      " [SequenceState(index=0, tokens=tensor([0], dtype=torch.int32), log_prob=0.0), SequenceState(index=1, tokens=tensor([0], dtype=torch.int32), log_prob=0.0)]\n"
     ]
    }
   ],
   "source": [
    "# Create 'SearchState' object to save the state of the beam search algorithm.\n",
    "beam_state = SearchState()\n",
    "# At the start of the algorithm, every src sequence will only have a single tgt sequence with the <sos> token.\n",
    "# The log probability of this sequence will be 0.0 since the probability of the <sos> token is 1.0 i.e., \n",
    "# log(1.0) = 0.0. The batch size (BATCH_SIZE defined above) is 2 that means there are 2 source sequences. So, \n",
    "# we will have 2 target sequences in the running_state list.\n",
    "beam_state.running_state = [SequenceState(index=idx, tokens=torch.tensor(data=[SOS_TOKEN_ID], dtype=torch.int32), log_prob=0.0) for idx in range(BATCH_SIZE)]\n",
    "print(\"len(beam_state.running_state): \", len(beam_state.running_state))\n",
    "print(\"beam_state.running_state: \\n\", beam_state.running_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will predict the next token for each sequence in the 'running_state' list. This step will be <br>\n",
    "repeated until the \\<eos\\> token is predicted for all the sequences in the 'running_state' list or we <br>\n",
    "predict the 'TGT_SEQ_LIMIT' number of tokens for each sequence in the 'running_state' list. Lets do <br>\n",
    "2 iterations of the beam search algorithm and understand how components work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st Iteration of Beam Search starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tgt_for_inference:  torch.Size([2, 1])\n",
      "tgt_for_inference: \n",
      " tensor([[0],\n",
      "        [0]], dtype=torch.int32)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of tgt_mask_for_inference:  torch.Size([2, 1, 1, 1])\n",
      "tgt_mask_for_inference: \n",
      " tensor([[[[True]]],\n",
      "\n",
      "\n",
      "        [[[True]]]])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the tgt_batch and tgt_mask for the decoder. The tgt_batch will just be the token sequences stacked\n",
    "# together for all the sequences in the running_state list. The tgt_mask will be a random mask for now. We \n",
    "# will not be using these tgt tensors in this notebook since we are not actually using the 'Decoder' but just \n",
    "# creating random data that appears as the same as Decoder output. This cell is created just to show how the \n",
    "# target_batch and target_mask input for Decoder looks like and how they are created from the running_state.\n",
    "tgt_for_inference = torch.stack(tensors=[state.tokens for state in beam_state.running_state], dim=0) \n",
    "print(\"shape of tgt_for_inference: \", tgt_for_inference.shape)\n",
    "print(\"tgt_for_inference: \\n\", tgt_for_inference)\n",
    "print(\"-\" * 150)\n",
    "tgt_mask_for_inference = construct_look_ahead_mask(batch_size=len(beam_state.running_state), seq_len=tgt_for_inference.size(1))\n",
    "print(\"shape of tgt_mask_for_inference: \", tgt_mask_for_inference.shape)\n",
    "print(\"tgt_mask_for_inference: \\n\", tgt_mask_for_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of predicted_log_probabilities:  torch.Size([2, 1, 10])\n",
      "predicted_log_probabilities: \n",
      " tensor([[[-2.9145, -2.5996, -2.9133, -1.9518, -2.5989, -1.9350, -2.1859,\n",
      "          -2.1319, -2.1800, -2.1863]],\n",
      "\n",
      "        [[-2.4938, -2.3863, -1.9947, -2.7851, -2.5315, -2.2796, -2.3758,\n",
      "          -1.9841, -2.4892, -2.0220]]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of predicted_log_probabilities:  torch.Size([2, 10])\n",
      "predicted_log_probabilities: \n",
      " tensor([[-2.9145, -2.5996, -2.9133, -1.9518, -2.5989, -1.9350, -2.1859, -2.1319,\n",
      "         -2.1800, -2.1863],\n",
      "        [-2.4938, -2.3863, -1.9947, -2.7851, -2.5315, -2.2796, -2.3758, -1.9841,\n",
      "         -2.4892, -2.0220]])\n"
     ]
    }
   ],
   "source": [
    "# Create a random tensor for the predicted probabilities. This will be the output of the 'TokenPredictor' module \n",
    "# in the transformer and will be used to identify the next token for each sequence in the 'running_state' list.\n",
    "predicted_log_probabilities = torch.log_softmax(torch.rand(tgt_for_inference.size(0), tgt_for_inference.size(1), TGT_VOCAB_SIZE), dim=-1)\n",
    "print(\"shape of predicted_log_probabilities: \", predicted_log_probabilities.shape)\n",
    "print(\"predicted_log_probabilities: \\n\", predicted_log_probabilities)\n",
    "print(\"-\" * 150)\n",
    "# We only need the probabilities for the last token in the sequence. So, we will only consider the last token\n",
    "# probabilities for the calculation of the log probabilities for any sequence. Please notice that we get a 2D \n",
    "# tensor after slicing the predicted_log_probabilities tensor which is originally a 3D tensor.\n",
    "predicted_log_probabilities = predicted_log_probabilities[:, -1, :]\n",
    "print(\"shape of predicted_log_probabilities: \", predicted_log_probabilities.shape)\n",
    "print(\"predicted_log_probabilities: \\n\", predicted_log_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(new_tgt_probs):  <class 'tuple'>\n",
      "len(new_tgt_probs):  2\n",
      "new_tgt_probs: \n",
      " (tensor([-2.9145, -2.5996, -2.9133, -1.9518, -2.5989, -1.9350, -2.1859, -2.1319,\n",
      "        -2.1800, -2.1863]), tensor([-2.4938, -2.3863, -1.9947, -2.7851, -2.5315, -2.2796, -2.3758, -1.9841,\n",
      "        -2.4892, -2.0220]))\n"
     ]
    }
   ],
   "source": [
    "# Convert the probabilities tensor into an iterator (tuple here) so that we can iterate over probabilities and \n",
    "# corresponding 'SequenceState' objects in the 'running_state' list simultaneously using 'zip' function. This \n",
    "# will basically separate each 1D tensor in the 2D tensor into a separate tensor and store them in a tuple.\n",
    "new_tgt_probs = torch.unbind(predicted_log_probabilities, dim=0)\n",
    "print(\"type(new_tgt_probs): \", type(new_tgt_probs))\n",
    "print(\"len(new_tgt_probs): \", len(new_tgt_probs))\n",
    "print(\"new_tgt_probs: \\n\", new_tgt_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_probs:  tensor([-1.9350, -1.9518, -2.1319])\n",
      "top_tokens:  tensor([5, 3, 7])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_token:  tensor(5)\n",
      "old_seq_state tokens:  tensor([0], dtype=torch.int32)\n",
      "updated_token_seq:  tensor([0, 5], dtype=torch.int32)\n",
      "updated_seq_prob:  -1.9349650144577026\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_token:  tensor(3)\n",
      "old_seq_state tokens:  tensor([0], dtype=torch.int32)\n",
      "updated_token_seq:  tensor([0, 3], dtype=torch.int32)\n",
      "updated_seq_prob:  -1.951758861541748\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_token:  tensor(7)\n",
      "old_seq_state tokens:  tensor([0], dtype=torch.int32)\n",
      "updated_token_seq:  tensor([0, 7], dtype=torch.int32)\n",
      "updated_seq_prob:  -2.1318984031677246\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "top_probs:  tensor([-1.9841, -1.9947, -2.0220])\n",
      "top_tokens:  tensor([7, 2, 9])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_token:  tensor(7)\n",
      "old_seq_state tokens:  tensor([0], dtype=torch.int32)\n",
      "updated_token_seq:  tensor([0, 7], dtype=torch.int32)\n",
      "updated_seq_prob:  -1.9841415882110596\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_token:  tensor(2)\n",
      "old_seq_state tokens:  tensor([0], dtype=torch.int32)\n",
      "updated_token_seq:  tensor([0, 2], dtype=torch.int32)\n",
      "updated_seq_prob:  -1.994722604751587\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_token:  tensor(9)\n",
      "old_seq_state tokens:  tensor([0], dtype=torch.int32)\n",
      "updated_token_seq:  tensor([0, 9], dtype=torch.int32)\n",
      "updated_seq_prob:  -2.0220141410827637\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "len(new_running_state):  6\n",
      "new_running_state: \n",
      " [SequenceState(index=0, tokens=tensor([0, 5], dtype=torch.int32), log_prob=-1.9349650144577026), SequenceState(index=0, tokens=tensor([0, 3], dtype=torch.int32), log_prob=-1.951758861541748), SequenceState(index=0, tokens=tensor([0, 7], dtype=torch.int32), log_prob=-2.1318984031677246), SequenceState(index=1, tokens=tensor([0, 7], dtype=torch.int32), log_prob=-1.9841415882110596), SequenceState(index=1, tokens=tensor([0, 2], dtype=torch.int32), log_prob=-1.994722604751587), SequenceState(index=1, tokens=tensor([0, 9], dtype=torch.int32), log_prob=-2.0220141410827637)]\n"
     ]
    }
   ],
   "source": [
    "# Update the beam_state object with the new target token predictions of the beam search algorithm.\n",
    "new_running_state: List[SequenceState] = []\n",
    "for new_tgt_prob, old_seq_state in zip(new_tgt_probs, beam_state.running_state):\n",
    "    # Extract the top 3 tokens (BEAM_WIDTH) to be considered as the next token via beam search.\n",
    "    top_probs, top_tokens = new_tgt_prob.topk(k=BEAM_WIDTH, dim=-1)\n",
    "    print(\"top_probs: \", top_probs)\n",
    "    print(\"top_tokens: \", top_tokens)\n",
    "    print(\"-\" * 150)\n",
    "    # Iterate on each predicted token, create the sequence with this token appended and calculate\n",
    "    # the probability of the new sequence (with token appended).\n",
    "    for pred_prob, pred_token in zip(top_probs, top_tokens):\n",
    "        print(\"pred_token: \", pred_token)\n",
    "        print(\"old_seq_state tokens: \", old_seq_state.tokens)\n",
    "        # Append the newly predicted token to the existing sequence of tokens.\n",
    "        updated_token_seq = torch.cat(tensors=[old_seq_state.tokens, pred_token.unsqueeze(0).to(torch.int32)], dim=0)\n",
    "        print(\"updated_token_seq: \", updated_token_seq)\n",
    "        # The log probability of the extended sequence is the probability of the old sequence added\n",
    "        # to the probability associated with the newly predicted token.\n",
    "        updated_seq_prob = old_seq_state.log_prob + pred_prob.item()\n",
    "        print(\"updated_seq_prob: \", updated_seq_prob)\n",
    "        print(\"-\" * 150)\n",
    "        # Creates a new SequenceState object associated with the extended tgt sequence.\n",
    "        new_state = SequenceState(index=old_seq_state.index, tokens=updated_token_seq, log_prob=updated_seq_prob)\n",
    "        # If the newly predicted token is not <eos>, then this tgt sequence is not complete and\n",
    "        # can be extended by predicting further tokens.\n",
    "        new_running_state.append(new_state)\n",
    "\n",
    "print(\"len(new_running_state): \", len(new_running_state))\n",
    "print(\"new_running_state: \\n\", new_running_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete State: {}\n",
      "Running State: [SequenceState(index=0, tokens=tensor([0, 5], dtype=torch.int32), log_prob=-1.9349650144577026), SequenceState(index=0, tokens=tensor([0, 3], dtype=torch.int32), log_prob=-1.951758861541748), SequenceState(index=0, tokens=tensor([0, 7], dtype=torch.int32), log_prob=-2.1318984031677246), SequenceState(index=1, tokens=tensor([0, 7], dtype=torch.int32), log_prob=-1.9841415882110596), SequenceState(index=1, tokens=tensor([0, 2], dtype=torch.int32), log_prob=-1.994722604751587), SequenceState(index=1, tokens=tensor([0, 9], dtype=torch.int32), log_prob=-2.0220141410827637)]\n"
     ]
    }
   ],
   "source": [
    "# Update the beam state with the new running state.\n",
    "beam_state.running_state = new_running_state\n",
    "print(beam_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd Iteration of Beam Search starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the 1st iteration, we did not handle a lot of cases that we usually run into. Lets go through each of the\n",
    "# cases now:\n",
    "#\n",
    "# 1) We can have multiple potential target sequences in the 'running_state' for the same source sequence for which \n",
    "#    we predict the next tokens.\n",
    "#       -- This will effect the src_batch, src_mask since these are passed as inputs to the Decoder. \n",
    "#       -- The src_batch in this case needs to contain the same source sequence repeated as many as times as the \n",
    "#          corresponding tgt sequences. Similarly the src_mask needs to be updated.\n",
    "# 2) Some of the newly predicted tokens can be '<eos>' token and these should not be added back to the new \n",
    "#    'running_state'.\n",
    "#       -- If the newly predicted token is '<eos>' token, this token should be appended to the tgt sequence and \n",
    "#          added to the list of complete state objects.\n",
    "#       -- Only if the predicted token is not '<eos>' token, it needs to be added back to the 'running_state' for \n",
    "#          next iteration of predictions.\n",
    "# 3) The complete_state only need to hold the sequence with the maximum probability at any point of time. So, we \n",
    "#    need to find the complete tgt sequence with max log probability and save it to the complete state. \n",
    "# 4) If we have more than 3 potential tgt sequences, we need to find out the top 3 sequences sorted by probability, \n",
    "#    add these 3 sequences to the 'running_state' and discard the additional sequences.  \n",
    "#\n",
    "# We will now see how to handle all these cases here as we go through the 2nd iteration of Beam Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_indices shape:  torch.Size([6])\n",
      "src_indices:  tensor([0, 0, 0, 1, 1, 1], dtype=torch.int32)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "itr_2_src_for_inference shape:  torch.Size([6, 3, 6])\n",
      "itr_2_src_for_inference:  tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
      "         [ 6.,  7.,  8.,  9., 10., 11.],\n",
      "         [12., 13., 14., 15., 16., 17.]],\n",
      "\n",
      "        [[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
      "         [ 6.,  7.,  8.,  9., 10., 11.],\n",
      "         [12., 13., 14., 15., 16., 17.]],\n",
      "\n",
      "        [[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
      "         [ 6.,  7.,  8.,  9., 10., 11.],\n",
      "         [12., 13., 14., 15., 16., 17.]],\n",
      "\n",
      "        [[18., 19., 20., 21., 22., 23.],\n",
      "         [24., 25., 26., 27., 28., 29.],\n",
      "         [30., 31., 32., 33., 34., 35.]],\n",
      "\n",
      "        [[18., 19., 20., 21., 22., 23.],\n",
      "         [24., 25., 26., 27., 28., 29.],\n",
      "         [30., 31., 32., 33., 34., 35.]],\n",
      "\n",
      "        [[18., 19., 20., 21., 22., 23.],\n",
      "         [24., 25., 26., 27., 28., 29.],\n",
      "         [30., 31., 32., 33., 34., 35.]]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "itr_2_src_mask_for_inference shape:  torch.Size([6, 1, 3, 3])\n",
      "itr_2_src_mask_for_inference:  tensor([[[[False,  True, False],\n",
      "          [False,  True,  True],\n",
      "          [False, False, False]]],\n",
      "\n",
      "\n",
      "        [[[False,  True, False],\n",
      "          [False,  True,  True],\n",
      "          [False, False, False]]],\n",
      "\n",
      "\n",
      "        [[[False,  True, False],\n",
      "          [False,  True,  True],\n",
      "          [False, False, False]]],\n",
      "\n",
      "\n",
      "        [[[False,  True, False],\n",
      "          [False,  True,  True],\n",
      "          [False, False, False]]],\n",
      "\n",
      "\n",
      "        [[[False,  True, False],\n",
      "          [False,  True,  True],\n",
      "          [False, False, False]]],\n",
      "\n",
      "\n",
      "        [[[False,  True, False],\n",
      "          [False,  True,  True],\n",
      "          [False, False, False]]]])\n"
     ]
    }
   ],
   "source": [
    "# Handling case 1:\n",
    "#\n",
    "# We need to create the 'encoded_src' that can be passed as input to the Decoder. In the beam search \n",
    "# 'running_state', we have 3 SequenceState objects for which the index is 0 i.e., we have 3 potential target \n",
    "# sequences for the 0th source sequence. So, we need to repeat the encoded tokens for the 0th source \n",
    "# sequence 3 times in the 'itr_2_src_for_inference' (variable below).\n",
    "#\n",
    "# Similarly we have 3 SequenceState objects for which the index is 1 i.e., we have 3 potential target sequences\n",
    "# for the 1st source sequence. So, we again need to repeat the encoded tokens for the 1st sequence 3 times in \n",
    "# the 'itr_2_src_for_inference' (variable below).\n",
    "#\n",
    "# Please note that this number 3 being the same for both source sequence 1 and source sequence 2 is just a \n",
    "# coincidence and could be different. We have to find out how many tgt sequences are in the running_state for a\n",
    "# source sequence and repeat the encoded src based on that.\n",
    "#\n",
    "# This gives all the source sequence indices from the running_state. \n",
    "src_indices = torch.tensor(data=[state.index for state in beam_state.running_state], dtype=torch.int32) \n",
    "print(\"src_indices shape: \", src_indices.shape)\n",
    "print(\"src_indices: \", src_indices)\n",
    "print(\"-\" * 150)\n",
    "# Now, we just create a new src based on the original encoded_src and the src_indices. This will basically\n",
    "# take the tensors from original encoded_src and copy it as many times as specified by the src_indices tensor.\n",
    "itr_2_src_for_inference = torch.index_select(input=encoded_src, dim=0, index=src_indices)\n",
    "print(\"itr_2_src_for_inference shape: \", itr_2_src_for_inference.shape)\n",
    "print(\"itr_2_src_for_inference: \", itr_2_src_for_inference)\n",
    "print(\"-\" * 150)\n",
    "# Similarly we create the source mask for inference.\n",
    "itr_2_src_mask_for_inference = torch.index_select(input=src_mask, dim=0, index=src_indices)\n",
    "print(\"itr_2_src_mask_for_inference shape: \", itr_2_src_mask_for_inference.shape)\n",
    "print(\"itr_2_src_mask_for_inference: \", itr_2_src_mask_for_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of itr_2_tgt_for_inference:  torch.Size([6, 2])\n",
      "itr_2_tgt_for_inference: \n",
      " tensor([[0, 5],\n",
      "        [0, 3],\n",
      "        [0, 7],\n",
      "        [0, 7],\n",
      "        [0, 2],\n",
      "        [0, 9]], dtype=torch.int32)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of itr_2_tgt_mask_for_inference:  torch.Size([6, 1, 2, 2])\n",
      "itr_2_tgt_mask_for_inference: \n",
      " tensor([[[[ True, False],\n",
      "          [ True,  True]]],\n",
      "\n",
      "\n",
      "        [[[ True, False],\n",
      "          [ True,  True]]],\n",
      "\n",
      "\n",
      "        [[[ True, False],\n",
      "          [ True,  True]]],\n",
      "\n",
      "\n",
      "        [[[ True, False],\n",
      "          [ True,  True]]],\n",
      "\n",
      "\n",
      "        [[[ True, False],\n",
      "          [ True,  True]]],\n",
      "\n",
      "\n",
      "        [[[ True, False],\n",
      "          [ True,  True]]]])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the tgt_batch and tgt_mask for the 2nd iteration of the decoder. The tgt_batch will be just \n",
    "# the token sequences stacked together for all the sequences in the running_state list. The tgt_mask \n",
    "# will be a random mask for now. We will not be using these tgt tensors since we are not actually\n",
    "# using the Decoder but just creating random data that appears as the same as Decoder output. This cell\n",
    "# is created just to show how the tgts are created for inference.\n",
    "itr_2_tgt_for_inference = torch.stack(tensors=[state.tokens for state in beam_state.running_state], dim=0) \n",
    "print(\"shape of itr_2_tgt_for_inference: \", itr_2_tgt_for_inference.shape)\n",
    "print(\"itr_2_tgt_for_inference: \\n\", itr_2_tgt_for_inference)\n",
    "print(\"-\" * 150)\n",
    "itr_2_tgt_mask_for_inference = construct_look_ahead_mask(batch_size=len(beam_state.running_state), seq_len=itr_2_tgt_for_inference.size(1))\n",
    "print(\"shape of itr_2_tgt_mask_for_inference: \", itr_2_tgt_mask_for_inference.shape)\n",
    "print(\"itr_2_tgt_mask_for_inference: \\n\", itr_2_tgt_mask_for_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of itr_2_predicted_log_probabilities:  torch.Size([6, 2, 10])\n",
      "itr_2_predicted_log_probabilities: \n",
      " tensor([[[-2.6758, -2.1868, -2.6052, -2.3803, -2.0308, -1.9903, -2.3367,\n",
      "          -2.5929, -2.5734, -1.9889],\n",
      "         [-1.9587, -2.6052, -1.9610, -2.4634, -2.6421, -2.6777, -2.7243,\n",
      "          -2.0137, -2.2799, -2.1379]],\n",
      "\n",
      "        [[-1.9284, -2.5868, -2.1620, -2.2617, -2.6320, -2.7865, -2.2264,\n",
      "          -2.2296, -2.4123, -2.1087],\n",
      "         [-2.8518, -2.3136, -2.1585, -2.4839, -2.4208, -2.6184, -2.1139,\n",
      "          -1.9845, -2.0909, -2.2941]],\n",
      "\n",
      "        [[-2.5583, -2.0368, -2.6204, -1.8999, -2.4938, -2.2499, -2.4966,\n",
      "          -2.5997, -2.4901, -1.9506],\n",
      "         [-2.6331, -2.6461, -2.3672, -2.1944, -2.5626, -1.9010, -2.3785,\n",
      "          -2.6150, -1.9536, -2.1378]],\n",
      "\n",
      "        [[-1.9151, -2.1589, -2.2824, -2.7962, -1.9311, -2.6187, -2.6744,\n",
      "          -2.1058, -2.5357, -2.4492],\n",
      "         [-2.3691, -2.0651, -1.9704, -2.3472, -2.6806, -2.9178, -2.9207,\n",
      "          -2.1407, -2.1183, -2.0379]],\n",
      "\n",
      "        [[-2.2561, -2.4767, -2.4708, -1.9400, -1.8834, -2.6147, -2.6277,\n",
      "          -1.8863, -2.6800, -2.7288],\n",
      "         [-2.1260, -2.2782, -2.7599, -2.3254, -1.9253, -2.3891, -2.2465,\n",
      "          -2.3371, -2.1971, -2.7161]],\n",
      "\n",
      "        [[-2.4009, -2.1398, -2.1802, -2.1847, -1.9391, -2.8310, -2.2744,\n",
      "          -2.8364, -2.3980, -2.1984],\n",
      "         [-2.2105, -2.1917, -2.0423, -2.4773, -2.4572, -2.4880, -2.3540,\n",
      "          -2.6864, -2.3726, -1.9674]]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape of itr_2_predicted_log_probabilities:  torch.Size([6, 10])\n",
      "itr_2_predicted_log_probabilities: \n",
      " tensor([[-1.9587, -2.6052, -1.9610, -2.4634, -2.6421, -2.6777, -2.7243, -2.0137,\n",
      "         -2.2799, -2.1379],\n",
      "        [-2.8518, -2.3136, -2.1585, -2.4839, -2.4208, -2.6184, -2.1139, -1.9845,\n",
      "         -2.0909, -2.2941],\n",
      "        [-2.6331, -2.6461, -2.3672, -2.1944, -2.5626, -1.9010, -2.3785, -2.6150,\n",
      "         -1.9536, -2.1378],\n",
      "        [-2.3691, -2.0651, -1.9704, -2.3472, -2.6806, -2.9178, -2.9207, -2.1407,\n",
      "         -2.1183, -2.0379],\n",
      "        [-2.1260, -2.2782, -2.7599, -2.3254, -1.9253, -2.3891, -2.2465, -2.3371,\n",
      "         -2.1971, -2.7161],\n",
      "        [-2.2105, -2.1917, -2.0423, -2.4773, -2.4572, -2.4880, -2.3540, -2.6864,\n",
      "         -2.3726, -1.9674]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create a random tensor for the predicted probabilities. This will be the output of the 'TokenPredictor' module \n",
    "# in the transformer and will be used to identify the next token for each sequence in the running_state list.\n",
    "itr_2_predicted_log_probabilities = torch.log_softmax(torch.rand(itr_2_tgt_for_inference.size(0), itr_2_tgt_for_inference.size(1), TGT_VOCAB_SIZE), dim=-1)\n",
    "print(\"shape of itr_2_predicted_log_probabilities: \", itr_2_predicted_log_probabilities.shape)\n",
    "print(\"itr_2_predicted_log_probabilities: \\n\", itr_2_predicted_log_probabilities)\n",
    "print(\"-\" * 150)\n",
    "# We only need the probabilities for the last token in the sequence. So, we will only consider the last token\n",
    "# probabilities for the calculation of the log probabilities of the sequences. Please notice that we get a \n",
    "# 2D tensor after slicing the predicted_log_probabilities tensor which is originally a 3D tensor.\n",
    "itr_2_predicted_log_probabilities = itr_2_predicted_log_probabilities[:, -1, :]\n",
    "print(\"shape of itr_2_predicted_log_probabilities: \", itr_2_predicted_log_probabilities.shape)\n",
    "print(\"itr_2_predicted_log_probabilities: \\n\", itr_2_predicted_log_probabilities)\n",
    "print(\"-\" * 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr_2_tgt_group_counts shape:  torch.Size([2])\n",
      "itr_2_tgt_group_counts:  tensor([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# A given source sentence can have multiple potential target sequences if beam_width > 1. Here, we find the number \n",
    "# of target sequences currently being used (in running_state) to predict the next token for each of the source \n",
    "# sequences. We have 3 tgt sequences for source sequence 1 and 3 tgt sequences for source sequence 2. So, the \n",
    "# counts output should be tensor([3, 3]).\n",
    "_, itr_2_tgt_group_counts = torch.unique(input=torch.tensor(data=[seq_state.index for seq_state in beam_state.running_state], dtype=torch.int16), return_counts=True)\n",
    "print(\"itr_2_tgt_group_counts shape: \", itr_2_tgt_group_counts.shape)\n",
    "print(\"itr_2_tgt_group_counts: \", itr_2_tgt_group_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holds the running_state i.e., the SequenceState for the incomplete token sequences at the end of iteration 2.\n",
    "itr_2_new_running_state: List[SequenceState] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_index:  0\n",
      "tgt_group_size:  tensor(3)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "old_tgt_state_group:  [SequenceState(index=0, tokens=tensor([0, 5], dtype=torch.int32), log_prob=-1.9349650144577026), SequenceState(index=0, tokens=tensor([0, 3], dtype=torch.int32), log_prob=-1.951758861541748), SequenceState(index=0, tokens=tensor([0, 7], dtype=torch.int32), log_prob=-2.1318984031677246)]\n",
      "new_tgt_prob_group:  (tensor([-1.9587, -2.6052, -1.9610, -2.4634, -2.6421, -2.6777, -2.7243, -2.0137,\n",
      "        -2.2799, -2.1379]), tensor([-2.8518, -2.3136, -2.1585, -2.4839, -2.4208, -2.6184, -2.1139, -1.9845,\n",
      "        -2.0909, -2.2941]), tensor([-2.6331, -2.6461, -2.3672, -2.1944, -2.5626, -1.9010, -2.3785, -2.6150,\n",
      "        -1.9536, -2.1378]))\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "top_probs:  tensor([-1.9587, -1.9610, -2.0137])\n",
      "top_tokens:  tensor([0, 2, 7])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_prob:  tensor(-1.9587)\n",
      "pred_token:  tensor(0)\n",
      "updated_token_seq:  tensor([0, 5, 0], dtype=torch.int32)\n",
      "updated_seq_prob:  -3.8936809301376343\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_prob:  tensor(-1.9610)\n",
      "pred_token:  tensor(2)\n",
      "updated_token_seq:  tensor([0, 5, 2], dtype=torch.int32)\n",
      "updated_seq_prob:  -3.8959431648254395\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_prob:  tensor(-2.0137)\n",
      "pred_token:  tensor(7)\n",
      "updated_token_seq:  tensor([0, 5, 7], dtype=torch.int32)\n",
      "updated_seq_prob:  -3.9486573934555054\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "top_probs:  tensor([-1.9845, -2.0909, -2.1139])\n",
      "top_tokens:  tensor([7, 8, 6])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_prob:  tensor(-1.9845)\n",
      "pred_token:  tensor(7)\n",
      "updated_token_seq:  tensor([0, 3, 7], dtype=torch.int32)\n",
      "updated_seq_prob:  -3.936246633529663\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_prob:  tensor(-2.0909)\n",
      "pred_token:  tensor(8)\n",
      "updated_token_seq:  tensor([0, 3, 8], dtype=torch.int32)\n",
      "updated_seq_prob:  -4.042621850967407\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_prob:  tensor(-2.1139)\n",
      "pred_token:  tensor(6)\n",
      "updated_token_seq:  tensor([0, 3, 6], dtype=torch.int32)\n",
      "updated_seq_prob:  -4.065648078918457\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "top_probs:  tensor([-1.9010, -1.9536, -2.1378])\n",
      "top_tokens:  tensor([5, 8, 9])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_prob:  tensor(-1.9010)\n",
      "pred_token:  tensor(5)\n",
      "updated_token_seq:  tensor([0, 7, 5], dtype=torch.int32)\n",
      "updated_seq_prob:  -4.032911777496338\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_prob:  tensor(-1.9536)\n",
      "pred_token:  tensor(8)\n",
      "updated_token_seq:  tensor([0, 7, 8], dtype=torch.int32)\n",
      "updated_seq_prob:  -4.085497498512268\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_prob:  tensor(-2.1378)\n",
      "pred_token:  tensor(9)\n",
      "updated_token_seq:  tensor([0, 7, 9], dtype=torch.int32)\n",
      "updated_seq_prob:  -4.269698619842529\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "running_beams:  [SequenceState(index=0, tokens=tensor([0, 5, 0], dtype=torch.int32), log_prob=-3.8936809301376343), SequenceState(index=0, tokens=tensor([0, 5, 2], dtype=torch.int32), log_prob=-3.8959431648254395), SequenceState(index=0, tokens=tensor([0, 3, 7], dtype=torch.int32), log_prob=-3.936246633529663), SequenceState(index=0, tokens=tensor([0, 5, 7], dtype=torch.int32), log_prob=-3.9486573934555054), SequenceState(index=0, tokens=tensor([0, 7, 5], dtype=torch.int32), log_prob=-4.032911777496338), SequenceState(index=0, tokens=tensor([0, 3, 8], dtype=torch.int32), log_prob=-4.042621850967407), SequenceState(index=0, tokens=tensor([0, 3, 6], dtype=torch.int32), log_prob=-4.065648078918457), SequenceState(index=0, tokens=tensor([0, 7, 8], dtype=torch.int32), log_prob=-4.085497498512268), SequenceState(index=0, tokens=tensor([0, 7, 9], dtype=torch.int32), log_prob=-4.269698619842529)]\n",
      "new_running_state:  [SequenceState(index=0, tokens=tensor([0, 5, 0], dtype=torch.int32), log_prob=-3.8936809301376343), SequenceState(index=0, tokens=tensor([0, 5, 2], dtype=torch.int32), log_prob=-3.8959431648254395), SequenceState(index=0, tokens=tensor([0, 3, 7], dtype=torch.int32), log_prob=-3.936246633529663)]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "complete_beams:  []\n",
      "running_beams:  [SequenceState(index=0, tokens=tensor([0, 5, 0], dtype=torch.int32), log_prob=-3.8936809301376343), SequenceState(index=0, tokens=tensor([0, 5, 2], dtype=torch.int32), log_prob=-3.8959431648254395), SequenceState(index=0, tokens=tensor([0, 3, 7], dtype=torch.int32), log_prob=-3.936246633529663), SequenceState(index=0, tokens=tensor([0, 5, 7], dtype=torch.int32), log_prob=-3.9486573934555054), SequenceState(index=0, tokens=tensor([0, 7, 5], dtype=torch.int32), log_prob=-4.032911777496338), SequenceState(index=0, tokens=tensor([0, 3, 8], dtype=torch.int32), log_prob=-4.042621850967407), SequenceState(index=0, tokens=tensor([0, 3, 6], dtype=torch.int32), log_prob=-4.065648078918457), SequenceState(index=0, tokens=tensor([0, 7, 8], dtype=torch.int32), log_prob=-4.085497498512268), SequenceState(index=0, tokens=tensor([0, 7, 9], dtype=torch.int32), log_prob=-4.269698619842529)]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "start_index:  3\n",
      "tgt_group_size:  tensor(3)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "old_tgt_state_group:  [SequenceState(index=1, tokens=tensor([0, 7], dtype=torch.int32), log_prob=-1.9841415882110596), SequenceState(index=1, tokens=tensor([0, 2], dtype=torch.int32), log_prob=-1.994722604751587), SequenceState(index=1, tokens=tensor([0, 9], dtype=torch.int32), log_prob=-2.0220141410827637)]\n",
      "new_tgt_prob_group:  (tensor([-2.3691, -2.0651, -1.9704, -2.3472, -2.6806, -2.9178, -2.9207, -2.1407,\n",
      "        -2.1183, -2.0379]), tensor([-2.1260, -2.2782, -2.7599, -2.3254, -1.9253, -2.3891, -2.2465, -2.3371,\n",
      "        -2.1971, -2.7161]), tensor([-2.2105, -2.1917, -2.0423, -2.4773, -2.4572, -2.4880, -2.3540, -2.6864,\n",
      "        -2.3726, -1.9674]))\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "top_probs:  tensor([-1.9704, -2.0379, -2.0651])\n",
      "top_tokens:  tensor([2, 9, 1])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_prob:  tensor(-1.9704)\n",
      "pred_token:  tensor(2)\n",
      "updated_token_seq:  tensor([0, 7, 2], dtype=torch.int32)\n",
      "updated_seq_prob:  -3.9545127153396606\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_prob:  tensor(-2.0379)\n",
      "pred_token:  tensor(9)\n",
      "updated_token_seq:  tensor([0, 7, 9], dtype=torch.int32)\n",
      "updated_seq_prob:  -4.02203631401062\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_prob:  tensor(-2.0651)\n",
      "pred_token:  tensor(1)\n",
      "updated_token_seq:  tensor([0, 7, 1], dtype=torch.int32)\n",
      "updated_seq_prob:  -4.049215078353882\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "top_probs:  tensor([-1.9253, -2.1260, -2.1971])\n",
      "top_tokens:  tensor([4, 0, 8])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_prob:  tensor(-1.9253)\n",
      "pred_token:  tensor(4)\n",
      "updated_token_seq:  tensor([0, 2, 4], dtype=torch.int32)\n",
      "updated_seq_prob:  -3.920013427734375\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_prob:  tensor(-2.1260)\n",
      "pred_token:  tensor(0)\n",
      "updated_token_seq:  tensor([0, 2, 0], dtype=torch.int32)\n",
      "updated_seq_prob:  -4.120744705200195\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_prob:  tensor(-2.1971)\n",
      "pred_token:  tensor(8)\n",
      "updated_token_seq:  tensor([0, 2, 8], dtype=torch.int32)\n",
      "updated_seq_prob:  -4.19180703163147\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "top_probs:  tensor([-1.9674, -2.0423, -2.1917])\n",
      "top_tokens:  tensor([9, 2, 1])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_prob:  tensor(-1.9674)\n",
      "pred_token:  tensor(9)\n",
      "updated_token_seq:  tensor([0, 9, 9], dtype=torch.int32)\n",
      "updated_seq_prob:  -3.9894182682037354\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_prob:  tensor(-2.0423)\n",
      "pred_token:  tensor(2)\n",
      "updated_token_seq:  tensor([0, 9, 2], dtype=torch.int32)\n",
      "updated_seq_prob:  -4.064345836639404\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pred_prob:  tensor(-2.1917)\n",
      "pred_token:  tensor(1)\n",
      "updated_token_seq:  tensor([0, 9, 1], dtype=torch.int32)\n",
      "updated_seq_prob:  -4.213682174682617\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "running_beams:  [SequenceState(index=1, tokens=tensor([0, 2, 4], dtype=torch.int32), log_prob=-3.920013427734375), SequenceState(index=1, tokens=tensor([0, 7, 2], dtype=torch.int32), log_prob=-3.9545127153396606), SequenceState(index=1, tokens=tensor([0, 9, 9], dtype=torch.int32), log_prob=-3.9894182682037354), SequenceState(index=1, tokens=tensor([0, 7, 9], dtype=torch.int32), log_prob=-4.02203631401062), SequenceState(index=1, tokens=tensor([0, 7, 1], dtype=torch.int32), log_prob=-4.049215078353882), SequenceState(index=1, tokens=tensor([0, 9, 2], dtype=torch.int32), log_prob=-4.064345836639404), SequenceState(index=1, tokens=tensor([0, 2, 0], dtype=torch.int32), log_prob=-4.120744705200195), SequenceState(index=1, tokens=tensor([0, 2, 8], dtype=torch.int32), log_prob=-4.19180703163147), SequenceState(index=1, tokens=tensor([0, 9, 1], dtype=torch.int32), log_prob=-4.213682174682617)]\n",
      "new_running_state:  [SequenceState(index=0, tokens=tensor([0, 5, 0], dtype=torch.int32), log_prob=-3.8936809301376343), SequenceState(index=0, tokens=tensor([0, 5, 2], dtype=torch.int32), log_prob=-3.8959431648254395), SequenceState(index=0, tokens=tensor([0, 3, 7], dtype=torch.int32), log_prob=-3.936246633529663), SequenceState(index=1, tokens=tensor([0, 2, 4], dtype=torch.int32), log_prob=-3.920013427734375), SequenceState(index=1, tokens=tensor([0, 7, 2], dtype=torch.int32), log_prob=-3.9545127153396606), SequenceState(index=1, tokens=tensor([0, 9, 9], dtype=torch.int32), log_prob=-3.9894182682037354)]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "complete_beams:  []\n",
      "running_beams:  [SequenceState(index=1, tokens=tensor([0, 2, 4], dtype=torch.int32), log_prob=-3.920013427734375), SequenceState(index=1, tokens=tensor([0, 7, 2], dtype=torch.int32), log_prob=-3.9545127153396606), SequenceState(index=1, tokens=tensor([0, 9, 9], dtype=torch.int32), log_prob=-3.9894182682037354), SequenceState(index=1, tokens=tensor([0, 7, 9], dtype=torch.int32), log_prob=-4.02203631401062), SequenceState(index=1, tokens=tensor([0, 7, 1], dtype=torch.int32), log_prob=-4.049215078353882), SequenceState(index=1, tokens=tensor([0, 9, 2], dtype=torch.int32), log_prob=-4.064345836639404), SequenceState(index=1, tokens=tensor([0, 2, 0], dtype=torch.int32), log_prob=-4.120744705200195), SequenceState(index=1, tokens=tensor([0, 2, 8], dtype=torch.int32), log_prob=-4.19180703163147), SequenceState(index=1, tokens=tensor([0, 9, 1], dtype=torch.int32), log_prob=-4.213682174682617)]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Handling cases 2, 3 and 4:\n",
    "#\n",
    "# For each group of tgt sequences that correspond to the same src sequence, we need to find the number of \n",
    "# running sequences and the complete sequences.\n",
    "#\n",
    "# Index to keep track of the start of the group of tgt sequences for a single source sequence.\n",
    "start_index = 0\n",
    "# Iterate on each group independently to process the group.\n",
    "for tgt_group_size in itr_2_tgt_group_counts:\n",
    "    print(\"start_index: \", start_index)\n",
    "    print(\"tgt_group_size: \", tgt_group_size)\n",
    "    print(\"-\" * 150)\n",
    "    # Extract the group of SequenceState objects corresponding to a single source sequence.\n",
    "    old_tgt_state_group = beam_state.running_state[start_index: start_index + tgt_group_size.item()]\n",
    "    # Extract the group of probabilities for the corresponding tgt sequence token predictions and convert the \n",
    "    # tensor into a iterator (tuple here) to be used with 'zip' function below.\n",
    "    new_tgt_prob_group = torch.unbind(itr_2_predicted_log_probabilities[start_index: start_index + tgt_group_size.item()], dim=0)\n",
    "    print(\"old_tgt_state_group: \", old_tgt_state_group)\n",
    "    print(\"new_tgt_prob_group: \", new_tgt_prob_group)\n",
    "    print(\"-\" * 150)\n",
    "    # Holds all the new sequences formed after appending the token for the previous group of tgt sequences.\n",
    "    running_beams: List[SequenceState] = []\n",
    "    # Holds all the sequences for which the <eos> token has been predicted.\n",
    "    complete_beams: List[SequenceState] = []\n",
    "    # Iterate on the old SequenceState object and the corresponding next prediction probabilities for this \n",
    "    # tgt sequence.\n",
    "    for old_seq_state, new_tgt_pred_probs in (zip(old_tgt_state_group, new_tgt_prob_group)):\n",
    "        # Index of the source sentence for which the translations are being calculated.\n",
    "        src_seq_idx = old_seq_state.index\n",
    "        # Extract the top few tokens to be considered as the next token via beam search.\n",
    "        top_probs, top_tokens = new_tgt_pred_probs.topk(k=BEAM_WIDTH, dim=-1)\n",
    "        print(\"top_probs: \", top_probs)\n",
    "        print(\"top_tokens: \", top_tokens)\n",
    "        print(\"-\" * 150)\n",
    "        # Iterate on each predicted token, create the sequence with this token appended and calculate\n",
    "        # the probability of the new sequence (with token appended).\n",
    "        for pred_prob, pred_token in zip(top_probs, top_tokens):\n",
    "            print(\"pred_prob: \", pred_prob)\n",
    "            print(\"pred_token: \", pred_token)\n",
    "            # Append the newly predicted token to the existing sequence of tokens.\n",
    "            updated_token_seq = torch.cat(tensors=[old_seq_state.tokens, pred_token.unsqueeze(0).to(torch.int32)])\n",
    "            print(\"updated_token_seq: \", updated_token_seq)\n",
    "            # The log probability of the extended sequence is the probability of the old sequence added\n",
    "            # to the probability associated with the newly predicted token.\n",
    "            updated_seq_prob = old_seq_state.log_prob + pred_prob.item()\n",
    "            print(\"updated_seq_prob: \", updated_seq_prob)\n",
    "            # Creates a new SequenceState object associated with the extended tgt sequence.\n",
    "            new_state = SequenceState(index=src_seq_idx, tokens=updated_token_seq, log_prob=updated_seq_prob)\n",
    "            # THIS CONDITIONAL IF-ELSE BLOCK TOGETHER HANDLE CASE 2,\n",
    "            if pred_token.item() == EOS_TOKEN_ID:\n",
    "                # If the newly predicted token is <eos>, then this tgt sequence is complete and we add it\n",
    "                # to the list of complete sequences for this specific src sequence.\n",
    "                complete_beams.append(new_state)\n",
    "            else:\n",
    "                # If the newly predicted token is not <eos>, then this tgt sequence is not complete and\n",
    "                # can be extended by predicting further tokens.\n",
    "                running_beams.append(new_state)\n",
    "            print(\"-\" * 150)\n",
    "    # If the newly predicted token is an <eos> token, then we remove these tgt sequences from the \n",
    "    # beam search and update the complete state for the corresponding src sequence accordingly.\n",
    "    # THIS CONDITIONAL IF BLOCK HANDLES CASE 3.\n",
    "    if len(complete_beams) > 0:\n",
    "        # Index of the source sentence for which the translations are being calculated.\n",
    "        src_seq_idx = complete_beams[0].index\n",
    "        # sort the completed sequences according to their probabilities in descending order. \n",
    "        complete_beams.sort(key=lambda seq_state: seq_state.log_prob, reverse=True)\n",
    "        print(\"complete_beams: \", complete_beams)\n",
    "        if src_seq_idx in beam_state.complete_state:\n",
    "            # If we found complete sequences before for this specific source sequence, we only store\n",
    "            # the complete sequence for which the probability of occurence is the highest.\n",
    "            if beam_state.complete_state[src_seq_idx].log_prob < complete_beams[0].log_prob:\n",
    "                beam_state.complete_state[src_seq_idx] = complete_beams[0]\n",
    "        else:\n",
    "            # If this is the first complete sequence we found, we just store this specific sequence.\n",
    "            beam_state.complete_state[src_seq_idx] = complete_beams[0]        \n",
    "        print(\"beam_state.complete_state: \", beam_state.complete_state)\n",
    "        print(\"-\" * 150)   \n",
    "    #THIS CONDITIONAL IF BLOCK HANDLES CASE 4.\n",
    "    if len(running_beams) > 0:              \n",
    "        # sort the running sequences according to their probabilities in descending order.\n",
    "        running_beams.sort(key=lambda seq_state: seq_state.log_prob, reverse=True)\n",
    "        print(\"running_beams: \", running_beams)\n",
    "        # Add the running sequences for further predictions. Only add the first 'beam_width' number\n",
    "        # of sequences.\n",
    "        itr_2_new_running_state.extend(running_beams[:min(BEAM_WIDTH, len(running_beams))])  \n",
    "        print(\"new_running_state: \", itr_2_new_running_state)\n",
    "        print(\"-\" * 150)\n",
    "    # Update the start_index to the start of the next group of tgt sequences.\n",
    "    start_index += tgt_group_size.item()\n",
    "    print(\"complete_beams: \", complete_beams)\n",
    "    print(\"running_beams: \", running_beams)\n",
    "    print(\"-\" * 150)\n",
    "\n",
    "beam_state.running_state = itr_2_new_running_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above process (iterations) are repeated until the <eos> token is predicted for all the sequences in the\n",
    "# running_state list or we predict the 'TGT_SEQ_LIMIT' number of tokens for each sequence in the running_state \n",
    "# list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we complete 'TGT_SEQ_LIMIT' number of iterations for token predictions for each running sequence, we end\n",
    "# the beam search algorithm. The complete_state will contain the tgt sequences that end in <eos> token with the \n",
    "# maximum probability for a particular source sequence. If the <eos> token is not predicted for a tgt sequence, \n",
    "# we can just take the incomplete sequence from the running_state list with the maximum probability and add it \n",
    "# to the complete_state list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets manually create the complete_state and running_state for the beam_state object to show how the beam search\n",
    "# algorithm ends.\n",
    "beam_state_end = SearchState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: SequenceState(index=0, tokens=tensor([0, 4, 5, 8, 7, 3], dtype=torch.int32), log_prob=-3.1855079746338193), 2: SequenceState(index=2, tokens=tensor([0, 8, 6, 4, 7, 3], dtype=torch.int32), log_prob=-3.9063676123287365), 4: SequenceState(index=4, tokens=tensor([0, 4, 6, 8, 7, 3], dtype=torch.int32), log_prob=-0.7895624838182663)}\n"
     ]
    }
   ],
   "source": [
    "# We add the complete sequences for the source sequences 0, 2 and 4.\n",
    "for idx in range(0, 6, 2):\n",
    "    # Creating a random token sequence for the source sequence idx. Token sequence needs to start with <sos> (0) token\n",
    "    # and end with <eos> (3) token.\n",
    "    random_token_sequence = [0] + random.sample(range(4, 10), 4) + [3]\n",
    "    beam_state_end.complete_state[idx] = SequenceState(index=idx, tokens=torch.tensor(data=random_token_sequence, dtype=torch.int32), log_prob=-random.uniform(0, 5))\n",
    "    \n",
    "print(beam_state_end.complete_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SequenceState(index=1, tokens=tensor([0, 6, 7, 5, 4, 8, 9, 7], dtype=torch.int32), log_prob=-0.18177964969088856), SequenceState(index=1, tokens=tensor([0, 6, 7, 5, 8, 6, 4, 9], dtype=torch.int32), log_prob=-3.11712461127309), SequenceState(index=1, tokens=tensor([0, 6, 8, 9, 8, 4, 5, 7], dtype=torch.int32), log_prob=-1.617822470253099), SequenceState(index=3, tokens=tensor([0, 8, 9, 7, 4, 5, 6, 5], dtype=torch.int32), log_prob=-0.07893848570126316), SequenceState(index=3, tokens=tensor([0, 6, 8, 5, 9, 6, 9, 7], dtype=torch.int32), log_prob=-2.4689468108769446), SequenceState(index=3, tokens=tensor([0, 6, 9, 6, 9, 5, 5, 7], dtype=torch.int32), log_prob=-3.562795768979456), SequenceState(index=5, tokens=tensor([0, 7, 6, 8, 8, 9, 7, 5], dtype=torch.int32), log_prob=-4.363238021561321), SequenceState(index=5, tokens=tensor([0, 9, 8, 8, 9, 4, 5, 6], dtype=torch.int32), log_prob=-3.1369504026578054), SequenceState(index=5, tokens=tensor([0, 8, 8, 4, 7, 5, 6, 9], dtype=torch.int32), log_prob=-3.208347839579147), SequenceState(index=0, tokens=tensor([0, 4, 5, 6, 7, 8, 9], dtype=torch.int32), log_prob=-0.001)]\n"
     ]
    }
   ],
   "source": [
    "# We add the running sequences for the source sequences 1, 3 and 5.\n",
    "for idx in range(1, 6, 2):\n",
    "    # Creating a random token sequence for the source sequence idx. Token sequence needs to start with <sos> (0) token\n",
    "    # and ends with a token other than <eos> (3) token.\n",
    "    random_token_sequence = [0] + random.sample(list(range(4, 10)) + list(range(4, 10)), 7)\n",
    "    beam_state_end.running_state.append(SequenceState(index=idx, tokens=torch.tensor(data=random_token_sequence, dtype=torch.int32), log_prob=-random.uniform(0, 5)))\n",
    "    random_token_sequence = [0] + random.sample(list(range(4, 10)) + list(range(4, 10)), 7)\n",
    "    beam_state_end.running_state.append(SequenceState(index=idx, tokens=torch.tensor(data=random_token_sequence, dtype=torch.int32), log_prob=-random.uniform(0, 5)))\n",
    "    random_token_sequence = [0] + random.sample(list(range(4, 10)) + list(range(4, 10)), 7)\n",
    "    beam_state_end.running_state.append(SequenceState(index=idx, tokens=torch.tensor(data=random_token_sequence, dtype=torch.int32), log_prob=-random.uniform(0, 5)))\n",
    "\n",
    "# We also add a running tgt sequence for the source sequence 0. This tgt sequence is incomplete and has a higher log \n",
    "# probability than the complete sequence for the source sequence 0. However, this sequence should still not be used as a \n",
    "# final translation for the source sequence 0 since it is incomplete and we have other complete tgt sequences for \n",
    "# source sequence 0.\n",
    "beam_state_end.running_state.append(SequenceState(index=0, tokens=torch.tensor(data=[0, 4, 5, 6, 7, 8, 9], dtype=torch.int32), log_prob=-0.001))\n",
    "print(beam_state_end.running_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets go through the running_state to find the source sequences for which the <eos> token has not been predicted\n",
    "# in any of the corresponding target sequences. We will find the tgt sequence with the maximum probability from the \n",
    "# running_state and add it to the complete_state list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "dummy_tensor = torch.tensor(data=[SOS_TOKEN_ID], dtype=torch.int32)\n",
    "print(dummy_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  SequenceState(index=1, tokens=tensor([0, 6, 7, 5, 4, 8, 9, 7], dtype=torch.int32), log_prob=-0.18177964969088856)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "max_prob_state:  SequenceState(index=1, tokens=tensor([0, 6, 7, 5, 4, 8, 9, 7], dtype=torch.int32), log_prob=-0.18177964969088856)\n",
      "state:  SequenceState(index=1, tokens=tensor([0, 6, 7, 5, 8, 6, 4, 9], dtype=torch.int32), log_prob=-3.11712461127309)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "max_prob_state:  SequenceState(index=1, tokens=tensor([0, 6, 7, 5, 4, 8, 9, 7], dtype=torch.int32), log_prob=-0.18177964969088856)\n",
      "state:  SequenceState(index=1, tokens=tensor([0, 6, 8, 9, 8, 4, 5, 7], dtype=torch.int32), log_prob=-1.617822470253099)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "max_prob_state:  SequenceState(index=1, tokens=tensor([0, 6, 7, 5, 4, 8, 9, 7], dtype=torch.int32), log_prob=-0.18177964969088856)\n",
      "state:  SequenceState(index=3, tokens=tensor([0, 8, 9, 7, 4, 5, 6, 5], dtype=torch.int32), log_prob=-0.07893848570126316)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "max_prob_state:  SequenceState(index=3, tokens=tensor([0, 8, 9, 7, 4, 5, 6, 5], dtype=torch.int32), log_prob=-0.07893848570126316)\n",
      "state:  SequenceState(index=3, tokens=tensor([0, 6, 8, 5, 9, 6, 9, 7], dtype=torch.int32), log_prob=-2.4689468108769446)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "state:  SequenceState(index=3, tokens=tensor([0, 6, 9, 6, 9, 5, 5, 7], dtype=torch.int32), log_prob=-3.562795768979456)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "state:  SequenceState(index=5, tokens=tensor([0, 7, 6, 8, 8, 9, 7, 5], dtype=torch.int32), log_prob=-4.363238021561321)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "max_prob_state:  SequenceState(index=5, tokens=tensor([0, 7, 6, 8, 8, 9, 7, 5], dtype=torch.int32), log_prob=-4.363238021561321)\n",
      "state:  SequenceState(index=5, tokens=tensor([0, 9, 8, 8, 9, 4, 5, 6], dtype=torch.int32), log_prob=-3.1369504026578054)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "state:  SequenceState(index=5, tokens=tensor([0, 8, 8, 4, 7, 5, 6, 9], dtype=torch.int32), log_prob=-3.208347839579147)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "state:  SequenceState(index=0, tokens=tensor([0, 4, 5, 6, 7, 8, 9], dtype=torch.int32), log_prob=-0.001)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "beam_state_end.complete_state:  {0: SequenceState(index=0, tokens=tensor([0, 4, 5, 8, 7, 3], dtype=torch.int32), log_prob=-3.1855079746338193), 2: SequenceState(index=2, tokens=tensor([0, 8, 6, 4, 7, 3], dtype=torch.int32), log_prob=-3.9063676123287365), 4: SequenceState(index=4, tokens=tensor([0, 4, 6, 8, 7, 3], dtype=torch.int32), log_prob=-0.7895624838182663), 3: SequenceState(index=1, tokens=tensor([0, 6, 7, 5, 4, 8, 9, 7], dtype=torch.int32), log_prob=-0.18177964969088856), 5: SequenceState(index=5, tokens=tensor([0, 7, 6, 8, 8, 9, 7, 5], dtype=torch.int32), log_prob=-4.363238021561321)}\n"
     ]
    }
   ],
   "source": [
    "# Holds the sequence with the maximum probability for a given source sequence.\n",
    "max_prob_state = SequenceState(index=-1, tokens=dummy_tensor, log_prob=float('-inf'))\n",
    "for state in beam_state_end.running_state:\n",
    "    print(\"state: \", state)\n",
    "    print(\"-\" * 150)\n",
    "    # If the source sequence of the current running sequence is not already in the complete sequences, \n",
    "    # that means we haven't found a complete sequence for this (identified by the index) source sequence \n",
    "    # yet. So, we hold this sequence in the max_prob_state as a potential complete sequence for the\n",
    "    # source sequence identified by the index.\n",
    "    if state.index not in beam_state_end.complete_state:\n",
    "        # If we haven't found any complete sequence yet, we just store this sequence as the potential\n",
    "        # complete sequence in the max_prob_state.\n",
    "        if max_prob_state.index == -1:\n",
    "            max_prob_state = state\n",
    "        elif max_prob_state.index != state.index:\n",
    "            # If all the running sequences have been looked at for the previous source sequence (identified \n",
    "            # by max_prob_state.index), then we just add the potential complete sequence to the list of\n",
    "            # complete sequences.\n",
    "            beam_state_end.complete_state[state.index] = max_prob_state\n",
    "            # We also store the new running sequence as a new potential running sequence for the new src\n",
    "            # sequence (identified by the index state.index).\n",
    "            max_prob_state = state\n",
    "        else:\n",
    "            # If a new potential complete sequence is found, we pick the one with maximum probability and\n",
    "            # store it as the potential complete sequence.\n",
    "            if max_prob_state.log_prob < state.log_prob:\n",
    "                max_prob_state = state\n",
    "        print(\"max_prob_state: \", max_prob_state)\n",
    "# Add the left out potential sequence to the list of complete sequences.\n",
    "if max_prob_state.index != -1:\n",
    "    beam_state_end.complete_state[max_prob_state.index] = max_prob_state\n",
    "print(\"-\" * 150)\n",
    "print(\"beam_state_end.complete_state: \", beam_state_end.complete_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
