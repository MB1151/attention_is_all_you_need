{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) What does the token predictor look like?\n",
    "# 2) How does the token predictor convert the Decoder output to a sequence of tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output of the Decoder is a tensor of shape [batch_size, tgt_seq_len - 1, d_model]. The token predictor converts \n",
    "# this decoder output tensor into probabilities. The token predictor is a simple linear layer followed by a softmax \n",
    "# function. The linear layer projects the 'd_model' dimensional vector into a 'vocab_size' (tgt vocab size) dimensional \n",
    "# vector. The softmax function converts the 'vocab_size' dimensional vector into a probability distribution over the \n",
    "# vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../Data/Images/OutputGenerator.png\" alt=\"Output Generator\" width=\"550\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "credits: The above image is taken from this blog post: [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS TO BE USED IN THIS NOTEBOOK.\n",
    "# Number of sentences in a batch.\n",
    "batch_size = 3\n",
    "# Number of tokens in a sentence.\n",
    "seq_len = 4\n",
    "# Dimension of the word embeddings.\n",
    "d_model = 8\n",
    "# Size of the vocabulary.\n",
    "vocab_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates input to experiment with the pipeline.\n",
    "def generate_batch_of_input_data(batch_size: int, seq_len: int, d_model: int) -> Tensor:\n",
    "    return torch.randn(batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ideas is to use softmax on the output of linear layer to calculate the probability distribution over the <br>\n",
    "target vocabulary. However, using softmax directly can lead to numerical instability. To avoid issues with <br>\n",
    "floating point values, we use log_softmax to calculate log of the probabilities obtained using softmax. <br>\n",
    "Ofcourse, log is not applied after the softmax is applied but the softmax expression is simplified using <br>\n",
    "log and the log of softmax is calculated in a single shot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenPredictor(nn.Module):\n",
    "    def __init__(self, d_model: int, tgt_vocab_size: int):\n",
    "        super(TokenPredictor, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = tgt_vocab_size\n",
    "        self.linear = nn.Linear(in_features=d_model, out_features=tgt_vocab_size)\n",
    "        # The non-module variables are not added to the list of parameters of the model.\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, decoder_output: Tensor) -> Tensor:\n",
    "        \"\"\"The forward pass of the token predictor. Calculates the probability distribution over the \n",
    "           vocabulary. Each token vector has a corresponding probability distribution over the \n",
    "           vocabulary since we predict one token per output.\n",
    "\n",
    "        Args:\n",
    "            decoder_output (Tensor): Output of the Decoder.\n",
    "                                     SHAPE: [batch_size, tgt_seq_len - 1, d_model]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Log probability distribution over the vocabulary. \n",
    "                    SHAPE: [batch_size, tgt_seq_len - 1, vocab_size]\n",
    "        \"\"\"\n",
    "        # Project the decoder output to the vocab_size dimensional space.\n",
    "        logits = self.linear(decoder_output)\n",
    "        # Convert the logits to a probability distribution over the vocabulary. All the entries in the\n",
    "        # output tensor are negative since we are using log softmax. The log softmax is used to make\n",
    "        # the training more numerically stable as explained above. However, the maximum value in \n",
    "        # log_softmax is still the same as the maximum value of the general softmax output.\n",
    "        return self.log_softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OutputGenerator(\n",
      "  (linear): Linear(in_features=8, out_features=5, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "token_predictor = TokenPredictor(d_model=d_model, tgt_vocab_size=vocab_size)\n",
    "print(token_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([3, 4, 8])\n",
      "input_data: \n",
      " tensor([[[ 0.6648, -0.0731,  1.4228,  0.2574,  0.4639,  0.2357, -1.0842,\n",
      "           0.8967],\n",
      "         [ 1.1176, -1.0840,  1.3274,  1.3652, -0.1541, -0.4554,  0.1552,\n",
      "           0.1834],\n",
      "         [ 0.7545, -0.6348, -0.1767,  2.6542,  0.6799, -1.1835, -0.4171,\n",
      "           0.0487],\n",
      "         [-0.9292,  0.4037, -0.4447,  0.0704, -0.6954, -0.1496,  0.1776,\n",
      "          -0.5893]],\n",
      "\n",
      "        [[-0.3329, -0.9574, -1.6375,  0.1879,  0.6625,  0.4439,  0.1016,\n",
      "           1.0151],\n",
      "         [ 1.1896,  0.0778, -0.1971, -0.3428,  0.7501, -1.3605, -0.7687,\n",
      "          -1.4977],\n",
      "         [-2.9320,  1.0243,  0.6493, -0.3205,  0.8143,  1.1340, -1.4613,\n",
      "           0.2017],\n",
      "         [-1.2985,  0.1435, -0.0709, -0.5146, -0.5569,  0.2387,  1.8324,\n",
      "          -0.2424]],\n",
      "\n",
      "        [[-0.0513,  0.3328,  0.7430,  0.3366,  1.3838, -0.0557,  0.2952,\n",
      "           1.8550],\n",
      "         [-0.9629, -0.3769,  0.3185, -1.2493, -1.1389,  1.5914,  0.4296,\n",
      "          -0.8643],\n",
      "         [-0.7303, -1.6439, -0.4647, -0.1179, -0.9783,  0.7976,  1.2608,\n",
      "           0.8248],\n",
      "         [-1.2366,  0.0817, -0.2340, -1.4976,  1.0111,  0.6583,  0.0119,\n",
      "           0.0387]]])\n"
     ]
    }
   ],
   "source": [
    "# Generate a tensor which is equivalent to 'decoder_output' to experiment with the 'TokenPredictor' class.\n",
    "input_data = generate_batch_of_input_data(batch_size=batch_size, seq_len=seq_len, d_model=d_model)\n",
    "print(\"shape: \", input_data.shape)\n",
    "print(\"input_data: \\n\", input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([3, 4, 5])\n",
      "probability_distributions: \n",
      " tensor([[[-0.8743, -2.2330, -2.1661, -1.2204, -2.7191],\n",
      "         [-0.8987, -2.1929, -2.2450, -1.6275, -1.7206],\n",
      "         [-1.6397, -1.3997, -2.8080, -1.6699, -1.1689],\n",
      "         [-2.4470, -2.0811, -1.6307, -2.1580, -0.7396]],\n",
      "\n",
      "        [[-1.8463, -1.1323, -1.5098, -2.5524, -1.5095],\n",
      "         [-1.6705, -2.3230, -1.4088, -1.6888, -1.2564],\n",
      "         [-2.0533, -1.6800, -1.5841, -1.1228, -1.8654],\n",
      "         [-2.2552, -2.6807, -1.2657, -2.2573, -0.8211]],\n",
      "\n",
      "        [[-1.0882, -2.1935, -2.3287, -0.9736, -2.5707],\n",
      "         [-2.0173, -2.7733, -0.6563, -2.8773, -1.4720],\n",
      "         [-1.7282, -1.6615, -1.3470, -2.8469, -1.1568],\n",
      "         [-1.7844, -2.2027, -0.8811, -1.8872, -1.8593]]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Passing the input_data (equivalent to decoder_output) gives us the probability distributions over the target vocabulary.\n",
    "# Each 1D tensor in this 'probability_distributions' tensor corresponds to one token in a specific sentence.\n",
    "# The values of the 1D tensor is a log(probability distribution) over the target vocabulary i.e., each entry in the 1D \n",
    "# tensor corresponds to the log(probability) that the token is the corresponding text in the target vocabulary.\n",
    "# probability_distributions[i][j][k] = Log of Probability that token j in sentence i is the text corresponding to token k \n",
    "#                                      in the target vocabulary.  \n",
    "probability_distributions = token_predictor(input_data) \n",
    "print(\"shape: \", probability_distributions.shape)\n",
    "print(\"probability_distributions: \\n\", probability_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
