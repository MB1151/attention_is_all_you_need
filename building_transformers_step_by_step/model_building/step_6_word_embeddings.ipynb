{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, we learn:\n",
    "# \n",
    "# 1) How to use the Embedding module in pytorch?\n",
    "# 2) How to use the embedding layer in the attention_is_all_you_need paper?\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Resources:\n",
    "#\n",
    "# 1) https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/\n",
    "#       -- Explains what word embeddings are, how they are useful and how they are generated traditionally.\n",
    "# 2) https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca\n",
    "# 3) https://www.youtube.com/watch?v=D-ekE-Wlcds\n",
    "#       -- Excellent Video explaining word embeddings\n",
    "# 4) https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#embedding\n",
    "#       -- Official pytorch documentation for the embedding module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "\n",
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding nn.Embedding module in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our vocabulary only has 10 tokens.\n",
    "vocab_size = 10\n",
    "# Every token is associated with a 10 sized embedding vector.\n",
    "embedding_vector_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(10, 10) <class 'torch.nn.modules.sparse.Embedding'>\n"
     ]
    }
   ],
   "source": [
    "# Embedding module basically serves as a look-up table for us. Given a token id, we get the\n",
    "# embedding vector associated with the token id. We will look at training embeddings at a \n",
    "# later stage.\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_vector_size)\n",
    "print(embedding_layer, type(embedding_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3012,  1.1632, -0.2358, -0.8029,  0.9754, -0.2711,  0.1415,  0.4453,\n",
      "          0.2625, -0.5352],\n",
      "        [ 1.0317,  2.2983,  0.2876, -1.0384,  1.4763,  2.4843,  0.5297,  0.5989,\n",
      "          0.1117,  1.1091],\n",
      "        [ 0.7685,  1.6019,  0.3784, -0.2048, -0.6040,  0.1599,  1.5740,  1.0345,\n",
      "          0.9879,  0.0837]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([3, 10])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.tensor(data=[0, 3, 4], dtype=torch.int)\n",
    "# Retrieves the embeddings for the tokens 0, 3 and 4.\n",
    "embeddings = embedding_layer(token_ids)\n",
    "print(embeddings)\n",
    "print(embeddings.shape)\n",
    "print(type(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# The embedding_layer should raise an error if a token >= 10 is provided.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m out_of_bound_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m10\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint)\n\u001b[0;32m----> 3\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43membedding_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_of_bound_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Learning/AI/GenAI/Projects/attention_is_all_you_need/.attention_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Learning/AI/GenAI/Projects/attention_is_all_you_need/.attention_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Learning/AI/GenAI/Projects/attention_is_all_you_need/.attention_venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Learning/AI/GenAI/Projects/attention_is_all_you_need/.attention_venv/lib/python3.10/site-packages/torch/nn/functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# The embedding_layer should raise an error if a token >= 10 is provided.\n",
    "out_of_bound_ids = torch.tensor(data=[10], dtype=torch.int)\n",
    "_ = embedding_layer(out_of_bound_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Embeddings in transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model in 'Attention Is All You Need' paper uses the embedding vectors to represent English \n",
    "# language. The input to the model is a list of indices that represent the tokens. Inside the \n",
    "# model, we then assign an embedding vector per index and train the embeddding vectors \n",
    "# along with the model.\n",
    "#\n",
    "# Explaining the above with an example:\n",
    "# If the English sentence input to transformer is: \"I am Batman\", then the model gets \n",
    "# [7, 5, 89] (excluding <sos>, <eos> in this example) as input i.e., there is a fixed mapping \n",
    "# between the English tokens and corresponding indices. \n",
    "# 'I' is mapped to 7.\n",
    "# 'am' is mapped to 5.\n",
    "# 'Batman' is mapped to 89.\n",
    "#\n",
    "# Within the transformer model, we then convert these indices (7, 5, 89) to embedding vectors of \n",
    "# size 512 and train these vectors along with the model. Our Embedding module below takes care\n",
    "# of the above process for us in the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer to 'using_modules.ipynb' to understand more about modules in pytorch.\n",
    "# We will train the embeddings as part of the transformer model. \n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        \"\"\"Creates the embedding layer that serves as a look-up table for the tokens in the transformer model.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary i.e., number of distinct tokens in the vocabulary.\n",
    "            embedding_dim (int): The size of the embedding vector to be generated for each token.\n",
    "        \"\"\"\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.look_up_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    # The input is be a '2D' tensor where each '1D' tensor within the '2D' tensor is the list\n",
    "    # of indices corresponding to the tokens in the vocab.\n",
    "    # [[0, 123, 3455, 4556, 7, 1, 2, 2], [0, 56, 98, 6234, 909, 56, 1, 2]]\n",
    "    # 0 - <SOS>, 1 - <eos>, 2 - <pad>\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"Converts the input tensor of token indices to their corresponding embedding vectors.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): The input tensor of token indices.\n",
    "                            shape: [batch_size, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The tensor of embedding vectors for the corresponding input tokens.\n",
    "                    shape: [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        # There is no reasoning as to why the original 'attention_is_all_you_need' paper scaled the\n",
    "        # embeddings using 'math.sqrt(embedding_dim)'. A few blogs attempted to explain this reasoning,\n",
    "        # but I haven't found anything with solid reasoning.\n",
    "        return self.look_up_table(input) * math.sqrt(self.embedding_dim)\n",
    "\n",
    "# The discussion here (https://datascience.stackexchange.com/a/88159) attempts to explain the reason \n",
    "# for scaling the embeddings but that is incorrect.\n",
    "#\n",
    "# In general, the word_embeddings in the nn.Embedding layer are initialized using N(0, 1) distribution.\n",
    "# You can find the evidence for this in the source code of Embedding class in pytorch.\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#Embedding\n",
    "# Within the above source code, the 'weight' property (which are our word embeddings) is initalized\n",
    "# inside the 'reset_parameters' method using N(0, 1) distribution.\n",
    "#  \n",
    "# So, the expected magnitude of the embedding vector is sqrt(embedding_dim) and the expected magnitude\n",
    "# of the positional embedding (more about this in step_8_positional_encoding.ipynb) is roughly (assuming \n",
    "# uniform distribution for sinsuodial positional encodings which is not corrrect but gives an easier \n",
    "# estimate) sqrt(embedding_dim / 3). So, they are already on the same scale and embeddings don't need \n",
    "# to be scaled to bring them to the same scale. Use ChatGPT / Gemini to get an explanation how the \n",
    "# expected magnitudes are calculated in the respective cases which gave me a reasonable answer.\n",
    "#\n",
    "# This blog (https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec) just \n",
    "# says that this scaling is done to magnify the contribution of word embeddings when word embeddings are\n",
    "# added to the positional encodings. This is true, however, I have seen people mentioning on the internet\n",
    "# that scaling did not have any visible impact on their models (To be verified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0, 123, 345, 455,   7,   1,   2,   2],\n",
      "        [  0,  56,  98, 234,   9,  56,   1,   2]], dtype=torch.int32) \n",
      "\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "sample_input = torch.tensor(data=[[0, 123, 345, 455, 7, 1, 2, 2], [0, 56, 98, 234, 9, 56, 1, 2]], dtype=torch.int)\n",
    "print(sample_input, \"\\n\")\n",
    "print(sample_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings(\n",
      "  (look_up_table): Embedding(500, 20)\n",
      ")\n",
      "<class '__main__.Embeddings'>\n"
     ]
    }
   ],
   "source": [
    "transformer_embedding_layer = Embeddings(vocab_size=500, embedding_dim=20)\n",
    "print(transformer_embedding_layer) \n",
    "print(type(transformer_embedding_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -1.8577,  -0.3983,  -3.8971,  -9.7414,   5.9951,  -1.6411,  -2.4170,\n",
      "           -0.6763,   5.1841,   2.2469,   3.3569,   2.3509,   6.1423,  -3.4343,\n",
      "           -5.5828,  -1.1291,  -9.1931,   1.9975,  -5.6461,  -3.1570],\n",
      "         [  0.6649,  -6.8562,  -1.0972,  -3.3827,   4.4044,   2.6300,   1.7047,\n",
      "           -1.0038,  -7.0174,  -6.7603,   2.1979,   5.4500,   1.0113,   3.9205,\n",
      "            0.4894,   0.2233,   7.3648,   6.1077,  -2.5670,  -7.5822],\n",
      "         [ -6.9634,  -1.0228,   6.5550,   6.1711, -10.7813,  -4.3164,   0.8786,\n",
      "            4.4746,   0.0207,  -2.9096,   3.8563,  -3.9088,  -3.1286,  -6.4031,\n",
      "            0.1279,  -0.9704,  -3.5322,   4.8648,   2.2500,   3.4233],\n",
      "         [  2.9664,   3.0572,  -0.9320,   1.5378,   3.3464,  -0.9655,  -3.4692,\n",
      "            2.1214,   6.1003,  -0.4259, -11.8368,   0.6232,  -3.9156,  -0.4642,\n",
      "           -0.0155,   1.9090,   0.0717,  -0.5324,   8.4863,  -3.9943],\n",
      "         [ -1.4025,   4.5092,   0.5611,   2.6926,   0.9760,  -1.7043,  11.6612,\n",
      "           -6.0308,   5.1574,  -4.5691,   5.3819,  -1.8580,   2.5227,   4.4433,\n",
      "            7.2958,   3.6333,  -3.1602,   8.6726,   0.5656,  -1.2703],\n",
      "         [ -2.3602,  -2.5841,   6.6814,  -9.1966,   0.9268,   2.2988,  -4.6765,\n",
      "            1.1132,   8.7837,  -3.2770,  10.5357,   1.1113,  -8.5583,   1.1536,\n",
      "           -1.2978,  -4.9522,   1.2461,  -2.6085,  -4.9607,   1.6276],\n",
      "         [  0.7741,  -1.4002,  -3.4909,  -0.9405,  -1.9867,  -2.1711,  -4.5953,\n",
      "            5.0860,  -5.3920,   4.0276,  -0.5967,   5.8668,  -2.7088,   0.3831,\n",
      "            1.9304,   1.3601,   3.4473,  -3.6145,   0.8885,   3.9232],\n",
      "         [  0.7741,  -1.4002,  -3.4909,  -0.9405,  -1.9867,  -2.1711,  -4.5953,\n",
      "            5.0860,  -5.3920,   4.0276,  -0.5967,   5.8668,  -2.7088,   0.3831,\n",
      "            1.9304,   1.3601,   3.4473,  -3.6145,   0.8885,   3.9232]],\n",
      "\n",
      "        [[ -1.8577,  -0.3983,  -3.8971,  -9.7414,   5.9951,  -1.6411,  -2.4170,\n",
      "           -0.6763,   5.1841,   2.2469,   3.3569,   2.3509,   6.1423,  -3.4343,\n",
      "           -5.5828,  -1.1291,  -9.1931,   1.9975,  -5.6461,  -3.1570],\n",
      "         [ -1.5513,  -5.9549, -12.6229,  -1.0734,   3.7631,   6.6362,  -8.9424,\n",
      "           -5.4508,   3.8286,  -1.8149,   0.8885,  -2.5472,   2.7280,  -6.8885,\n",
      "            8.8319,   1.0562,   1.1990,   3.6934,  -0.5424,   0.0343],\n",
      "         [ -3.1545,  -1.3094,  -0.4211, -12.7681,  -3.5214,  -2.7735,  -0.9608,\n",
      "            6.0012,   6.0019,  -6.7518,  -4.4230,   0.4349,   4.2298,  -1.6808,\n",
      "           -2.5514,   2.1225,  -3.0857,   3.0369,   3.3547,   3.5667],\n",
      "         [  3.4746,   0.9262,  -5.7660,   3.3048,   7.9731,  -1.4964,  -5.3210,\n",
      "            0.8285,   3.1810,  -2.0054,  -1.4440,  -7.1490,   4.5933,   5.9826,\n",
      "            2.2135,  -0.6272,  -1.9387,  -1.6666,   0.0900,  -4.9638],\n",
      "         [  1.1046,  -5.0957,   1.8176, -12.0354,  -4.0477,  -0.5455,  -1.2138,\n",
      "            0.9929,   2.9751,  -3.2790,  -3.4396,  -2.5182,   0.5830,  -7.3090,\n",
      "           -0.1702,  -1.4974,  -0.0157,   4.5783,  -1.7452,   3.4112],\n",
      "         [ -1.5513,  -5.9549, -12.6229,  -1.0734,   3.7631,   6.6362,  -8.9424,\n",
      "           -5.4508,   3.8286,  -1.8149,   0.8885,  -2.5472,   2.7280,  -6.8885,\n",
      "            8.8319,   1.0562,   1.1990,   3.6934,  -0.5424,   0.0343],\n",
      "         [ -2.3602,  -2.5841,   6.6814,  -9.1966,   0.9268,   2.2988,  -4.6765,\n",
      "            1.1132,   8.7837,  -3.2770,  10.5357,   1.1113,  -8.5583,   1.1536,\n",
      "           -1.2978,  -4.9522,   1.2461,  -2.6085,  -4.9607,   1.6276],\n",
      "         [  0.7741,  -1.4002,  -3.4909,  -0.9405,  -1.9867,  -2.1711,  -4.5953,\n",
      "            5.0860,  -5.3920,   4.0276,  -0.5967,   5.8668,  -2.7088,   0.3831,\n",
      "            1.9304,   1.3601,   3.4473,  -3.6145,   0.8885,   3.9232]]],\n",
      "       grad_fn=<MulBackward0>) \n",
      "\n",
      "torch.Size([2, 8, 20])\n"
     ]
    }
   ],
   "source": [
    "# Notice that the shape of the input is (2, 8) and the shape of the output is (2, 8, 20).\n",
    "# For every position (i, j), an embedding of size 20 is added in the last dimension. \n",
    "transformer_embedding_layer_output = transformer_embedding_layer.forward(input=sample_input)\n",
    "print(transformer_embedding_layer_output, \"\\n\")\n",
    "print(transformer_embedding_layer_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
