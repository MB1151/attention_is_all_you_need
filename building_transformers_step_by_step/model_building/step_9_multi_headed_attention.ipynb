{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) What is Attention?\n",
    "# 2) What is Multi Headed Attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources to go through before continuing with this notebook:\n",
    "#\n",
    "# 1) https://www.youtube.com/watch?v=ySEx_Bqxvvo&t=827s\n",
    "#       -- First discusses RNNs and then moved to Attention Mechanism.\n",
    "#       -- MUST WATCH -- MUST WATCH -- MUST WATCH -- MUST WATCH -- MUST WATCH\n",
    "# 2) https://jalammar.github.io/illustrated-transformer/\n",
    "#       -- Explains the entire transformer model. We only need to focus on attention and \n",
    "#          Multi Headed Attention for this notebook.\n",
    "# 3) https://peterbloem.nl/blog/transformers\n",
    "#       -- Explains the transformer model with a bit more detail about the implementation.\n",
    "#       -- Also, explains how the multi headed attention can be implemented using a single\n",
    "#          set of matrices for Q, K, and V instead of creating h (Number of heads) matrices. \n",
    "#          This can be quite confusing and needs time to understand.\n",
    "# 4) https://www.youtube.com/watch?v=wjZofJX0v4M\n",
    "#       -- Gives good intution about all the basics needed to understand Attention. In\n",
    "#          particular, the intution for word embededdings and how they work is very good.\n",
    "#       -- However, this video is fast paced and it helps if you already have some idea\n",
    "#          about the concepts from the other resources above.\n",
    "# 5) https://drive.google.com/file/d/1465IgQ-jYDLFqfBZJ2NcyHwqfeJtkRmF/view?usp=drive_link\n",
    "#       -- Shows visually how the input is transformed by the Multi-Headed Attention layer to\n",
    "#          generate the output.\n",
    "#       -- Please go through this resource before continuing with the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: THIS IS A LONG NOTEBOOK. PLEASE TAKE YOUR TIME TO UNDERSTAND THE CONCEPTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating input to experiment with the Multi-Headed Attention\n",
    "def generate_batch_of_input_data(batch_size: int, seq_len: int, d_model: int) -> Tensor:\n",
    "    return torch.randn(batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 3, 8])\n",
      "input:  tensor([[[-1.0202, -0.2347,  0.3195,  0.3942,  0.4593, -2.6094, -0.5602,\n",
      "          -0.9243],\n",
      "         [-0.6834,  0.0312, -1.2677,  0.4416, -0.3943, -0.6785, -1.0569,\n",
      "          -0.1173],\n",
      "         [ 1.4697,  0.1980,  0.3201,  0.2304, -1.1114, -0.3340,  1.0294,\n",
      "          -0.4959]],\n",
      "\n",
      "        [[-1.1367,  0.6126,  0.4391,  0.3038,  1.5510, -0.0983, -1.5240,\n",
      "          -0.3695],\n",
      "         [-0.6542,  0.6223, -0.5636, -1.5257, -1.9707, -0.6709,  0.3991,\n",
      "           0.8756],\n",
      "         [ 0.1549,  1.3908, -1.0694,  1.3774, -0.1948, -0.3535,  0.4294,\n",
      "           0.4716]]])\n"
     ]
    }
   ],
   "source": [
    "# Input contains 2 sequences of length 3 (3 tokens) and each token embedding in the sequence is of size 8.\n",
    "# d_model will be the size of the token embeddings in the actual model which is 512.\n",
    "input = generate_batch_of_input_data(batch_size=2, seq_len=3, d_model=8)\n",
    "print(\"shape: \", input.shape)\n",
    "print(\"input: \", input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_k:  4\n"
     ]
    }
   ],
   "source": [
    "# We are using 2 heads in the Multi-Headed Attention.\n",
    "num_heads: int = 2\n",
    "# The size of the word embeddings in the input.\n",
    "d_model: int = 8\n",
    "# This is the size of the query, key and value vectors for a token in a single head.\n",
    "d_k: int = d_model // num_heads\n",
    "print(\"d_k: \", d_k)\n",
    "# This is the number of tokens per sentence.\n",
    "seq_len = 3    # Referred to as 'n{t}' in the 'Input_Transformation_In_Multi_Headed_Attention.pdf'.\n",
    "# This is the batch size i.e., the input batch contains 2 sentences.\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_creator:  Linear(in_features=8, out_features=8, bias=True)\n",
      "key_creator:  Linear(in_features=8, out_features=8, bias=True)\n",
      "value_creator:  Linear(in_features=8, out_features=8, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Refer to 'understanding_nn_linear.ipynb' notebook (Add link to the notebook) to learn about how Linear Layer works.\n",
    "# A linear layer to generate the query vectors for each token in the sequence.\n",
    "query_creator = nn.Linear(in_features=d_model, out_features=d_model, bias=True)\n",
    "print(\"query_creator: \", query_creator)\n",
    "# A linear layer to generate the querie vectors for each token in the sequence.\n",
    "key_creator = nn.Linear(in_features=d_model, out_features=d_model, bias=True)\n",
    "print(\"key_creator: \", key_creator)\n",
    "# A linear layer to generate the querie vectors for each token in the sequence.\n",
    "value_creator = nn.Linear(in_features=d_model, out_features=d_model, bias=True)\n",
    "print(\"value_creator: \", value_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 3, 8])\n",
      "queries:  tensor([[[-0.2101, -0.3526,  0.2313, -0.2076, -0.6798, -0.0843, -0.4807,\n",
      "           0.4135],\n",
      "         [ 0.3132, -0.7180, -0.4983, -0.1015, -0.0967,  0.1530,  0.2631,\n",
      "          -0.0825],\n",
      "         [ 0.0373,  0.0739,  0.2784,  0.5304,  0.2237, -0.7869, -0.9946,\n",
      "           0.8966]],\n",
      "\n",
      "        [[ 0.8011, -0.2928,  0.1292, -0.6189, -0.1394,  0.5197,  0.2627,\n",
      "           0.4689],\n",
      "         [ 0.4967, -0.3740,  0.0132, -0.6353, -0.8985,  0.1501, -0.5515,\n",
      "          -0.3980],\n",
      "         [ 0.2963, -0.3409, -0.2049, -0.0806,  0.6988,  0.1840, -0.6014,\n",
      "           0.3676]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# A single linear layer (a single matrix multiplication) is going to generate the queries for all the heads.\n",
    "#\n",
    "# input: [batch_size, seq_len, d_model]\n",
    "# queries: [batch_size, seq_len, d_model]\n",
    "# \n",
    "# Lets run through a toy example to explain the query creation.\n",
    "#\n",
    "# For the toy example:\n",
    "# d_model   = 4\n",
    "# num_heads = 2\n",
    "# d_k       = d_model // num_heads = 2\n",
    "# seq_len  = 3\n",
    "#\n",
    "# Lets ignore the batch dimension for now. \n",
    "# \n",
    "# The input will look something like this:\n",
    "# input = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]] \n",
    "# [1, 2, 3, 4] --> input vector for first token in the sequence.\n",
    "# [5, 6, 7, 8] --> input vector for second token in the sequence.\n",
    "# [9, 10, 11, 12] --> input vector for third token in the sequence.\n",
    "# \n",
    "# Assume query_creator is an identity function for this example i.e., Q = I (Identity Matrix). So, the input is same as output.\n",
    "# The output of query_creator will look something like this:\n",
    "# output = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]\n",
    "# \n",
    "# This output contains the queries for all the heads for all the tokens in the sequence.\n",
    "# [1, 2, 3, 4] --> [1, 2] | [3, 4] --> Logical division of the query vector for the first token.\n",
    "# [1, 2] --> query vector for the first token in the first head.\n",
    "# [3, 4] --> query vector for the first token in the second head.\n",
    "#\n",
    "# [5, 6, 7, 8] --> [5, 6] | [7, 8] --> Logical division of the query vector for the second token.\n",
    "# [5, 6] --> query vector for the second token in the first head.\n",
    "# [7, 8] --> query vector for the second token in the second head.\n",
    "#\n",
    "# [9, 10, 11, 12] --> [9, 10] | [11, 12] --> Logical division of the query vector for the third token.\n",
    "# [9, 10] --> query vector for the third token in the first head.\n",
    "# [11, 12] --> query vector for the third token in the second head.\n",
    "#\n",
    "# We basically are reducing the size of the query vector for each head so that the overall size is\n",
    "# still the same as d_model.\n",
    "#\n",
    "# The above example is for a single sequence. The actual input will have a batch dimension as well.\n",
    "# The exact same process is repeated for every sequence (or 2D matrix) in the batch.\n",
    "queries = query_creator(input)\n",
    "print(\"shape: \", queries.shape)\n",
    "print(\"queries: \", queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 3, 8])\n",
      "keys:  tensor([[[-0.1720, -0.0974, -0.4074, -0.3137, -0.7831, -1.3113,  0.1043,\n",
      "          -0.3691],\n",
      "         [-0.3618, -0.0844, -0.5938, -0.0148, -0.2940, -0.6218,  0.5029,\n",
      "          -0.4131],\n",
      "         [-0.1265,  0.2863,  0.0526,  0.3498, -0.4518, -0.1813, -0.0158,\n",
      "          -0.1033]],\n",
      "\n",
      "        [[-0.0239, -1.1511, -0.4069, -1.3810,  0.2647, -0.6113, -0.0314,\n",
      "           0.5024],\n",
      "         [-1.4270,  0.5460,  0.1839,  0.6636, -0.1671, -0.5133,  0.8250,\n",
      "          -0.3704],\n",
      "         [-0.1410, -0.6418, -0.3549, -0.5290, -0.7715, -0.4057,  0.6064,\n",
      "           0.0792]]], grad_fn=<ViewBackward0>)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([2, 3, 8])\n",
      "values:  tensor([[[ 0.5662,  1.2474,  0.6110,  0.3846,  0.7495,  0.0327, -0.5575,\n",
      "          -0.4441],\n",
      "         [ 0.3909,  0.2873,  0.0760,  0.7672,  0.7780,  0.0809, -0.5373,\n",
      "          -0.4977],\n",
      "         [-0.3288, -0.8719,  1.3444, -0.0021,  0.5524,  0.3307,  0.4051,\n",
      "          -0.3111]],\n",
      "\n",
      "        [[ 0.7716,  0.8189, -0.1598,  0.7462, -0.2320,  0.1364, -0.8177,\n",
      "          -0.1223],\n",
      "         [-0.4825,  0.3338,  0.0198,  0.0347,  0.6211,  0.2088,  0.4031,\n",
      "          -0.8069],\n",
      "         [-0.0621, -0.6016,  1.0996,  0.6104, -0.0697,  0.9557, -0.4313,\n",
      "          -0.1889]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Key and Value creation is similar to Query creation. We just use a different linear layer for each i.e.,\n",
    "# key_creator for keys creation and value_creator for values creation respectively.\n",
    "keys = key_creator(input)\n",
    "print(\"shape: \", keys.shape)\n",
    "print(\"keys: \", keys)\n",
    "print(\"-\" * 150)\n",
    "\n",
    "values = value_creator(input)\n",
    "print(\"shape: \", values.shape)\n",
    "print(\"values: \", values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 3, 2, 4])\n",
      "queries:  tensor([[[[-0.2101, -0.3526,  0.2313, -0.2076],\n",
      "          [-0.6798, -0.0843, -0.4807,  0.4135]],\n",
      "\n",
      "         [[ 0.3132, -0.7180, -0.4983, -0.1015],\n",
      "          [-0.0967,  0.1530,  0.2631, -0.0825]],\n",
      "\n",
      "         [[ 0.0373,  0.0739,  0.2784,  0.5304],\n",
      "          [ 0.2237, -0.7869, -0.9946,  0.8966]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8011, -0.2928,  0.1292, -0.6189],\n",
      "          [-0.1394,  0.5197,  0.2627,  0.4689]],\n",
      "\n",
      "         [[ 0.4967, -0.3740,  0.0132, -0.6353],\n",
      "          [-0.8985,  0.1501, -0.5515, -0.3980]],\n",
      "\n",
      "         [[ 0.2963, -0.3409, -0.2049, -0.0806],\n",
      "          [ 0.6988,  0.1840, -0.6014,  0.3676]]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Refer to 'understanding_tenson_manipulations_part_6.ipynb' (Add link to the notebook) to \n",
    "# understand more about the 'view' and 'reshape' operations.\n",
    "#\n",
    "# Now, lets separate the queries, keys and values for each head. We already have a 3D tensor.\n",
    "# Now, we will obtain a 4D tensor, where each 3D tensor holds the queries for all the heads for a  \n",
    "# single sequence (or sentence).\n",
    "#\n",
    "# queries             : [batch_size, seq_len, d_model]\n",
    "# transformed_queries : [batch_size, seq_len, num_heads, d_k]\n",
    "#\n",
    "# It is important to know how the elements are rearranged by this operation. So, please spend some time\n",
    "# to understand the 'view' operation in detail. This will keep coming up in the Multi-Headed Attention.\n",
    "queries = queries.view(batch_size, seq_len, num_heads, d_k)\n",
    "print(\"shape: \", queries.shape)\n",
    "print(\"queries: \", queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 3, 2, 4])\n",
      "keys:  tensor([[[[-0.1720, -0.0974, -0.4074, -0.3137],\n",
      "          [-0.7831, -1.3113,  0.1043, -0.3691]],\n",
      "\n",
      "         [[-0.3618, -0.0844, -0.5938, -0.0148],\n",
      "          [-0.2940, -0.6218,  0.5029, -0.4131]],\n",
      "\n",
      "         [[-0.1265,  0.2863,  0.0526,  0.3498],\n",
      "          [-0.4518, -0.1813, -0.0158, -0.1033]]],\n",
      "\n",
      "\n",
      "        [[[-0.0239, -1.1511, -0.4069, -1.3810],\n",
      "          [ 0.2647, -0.6113, -0.0314,  0.5024]],\n",
      "\n",
      "         [[-1.4270,  0.5460,  0.1839,  0.6636],\n",
      "          [-0.1671, -0.5133,  0.8250, -0.3704]],\n",
      "\n",
      "         [[-0.1410, -0.6418, -0.3549, -0.5290],\n",
      "          [-0.7715, -0.4057,  0.6064,  0.0792]]]], grad_fn=<ViewBackward0>)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([2, 3, 2, 4])\n",
      "values:  tensor([[[[ 0.5662,  1.2474,  0.6110,  0.3846],\n",
      "          [ 0.7495,  0.0327, -0.5575, -0.4441]],\n",
      "\n",
      "         [[ 0.3909,  0.2873,  0.0760,  0.7672],\n",
      "          [ 0.7780,  0.0809, -0.5373, -0.4977]],\n",
      "\n",
      "         [[-0.3288, -0.8719,  1.3444, -0.0021],\n",
      "          [ 0.5524,  0.3307,  0.4051, -0.3111]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7716,  0.8189, -0.1598,  0.7462],\n",
      "          [-0.2320,  0.1364, -0.8177, -0.1223]],\n",
      "\n",
      "         [[-0.4825,  0.3338,  0.0198,  0.0347],\n",
      "          [ 0.6211,  0.2088,  0.4031, -0.8069]],\n",
      "\n",
      "         [[-0.0621, -0.6016,  1.0996,  0.6104],\n",
      "          [-0.0697,  0.9557, -0.4313, -0.1889]]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Keys and Values are also transformed in the same way as queries.\n",
    "keys = keys.view(batch_size, seq_len, num_heads, d_k)\n",
    "print(\"shape: \", keys.shape)\n",
    "print(\"keys: \", keys)\n",
    "print(\"-\" * 150)\n",
    "\n",
    "values = values.view(batch_size, seq_len, num_heads, d_k)\n",
    "print(\"shape: \", values.shape)\n",
    "print(\"values: \", values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 2, 3, 4])\n",
      "queries:  tensor([[[[-0.2101, -0.3526,  0.2313, -0.2076],\n",
      "          [ 0.3132, -0.7180, -0.4983, -0.1015],\n",
      "          [ 0.0373,  0.0739,  0.2784,  0.5304]],\n",
      "\n",
      "         [[-0.6798, -0.0843, -0.4807,  0.4135],\n",
      "          [-0.0967,  0.1530,  0.2631, -0.0825],\n",
      "          [ 0.2237, -0.7869, -0.9946,  0.8966]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8011, -0.2928,  0.1292, -0.6189],\n",
      "          [ 0.4967, -0.3740,  0.0132, -0.6353],\n",
      "          [ 0.2963, -0.3409, -0.2049, -0.0806]],\n",
      "\n",
      "         [[-0.1394,  0.5197,  0.2627,  0.4689],\n",
      "          [-0.8985,  0.1501, -0.5515, -0.3980],\n",
      "          [ 0.6988,  0.1840, -0.6014,  0.3676]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Refer to 'understanding_tensor_manipulations_part_6.ipynb' (link to the notebook) to understand \n",
    "# more about the transpose operation.\n",
    "#\n",
    "# queries           : [batch_size, seq_len, num_heads, d_k]\n",
    "# transposed_queries: [batch_size, num_heads, seq_len, d_k]\n",
    "#\n",
    "# Matrix multiplication is performed (by Pytorch) on the last two dimensions of the tensors. We calculate the \n",
    "# attention scores for each token by every other token in the sentence. So, for each token in a sentence, we \n",
    "# need to compute the dot product of the query vector with the key vectors of all other tokens (including the \n",
    "# current token). So, we need to rearrange the queries and keys so that the tokens iterate on dimension 2 and \n",
    "# token vectors (or embeddings) iterate on dimension 3. This is why we transpose all the tensors here.\n",
    "queries = queries.transpose(dim0=1, dim1=2)\n",
    "print(\"shape: \", queries.shape)\n",
    "print(\"queries: \", queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 2, 3, 4])\n",
      "keys:  tensor([[[[-0.1720, -0.0974, -0.4074, -0.3137],\n",
      "          [-0.3618, -0.0844, -0.5938, -0.0148],\n",
      "          [-0.1265,  0.2863,  0.0526,  0.3498]],\n",
      "\n",
      "         [[-0.7831, -1.3113,  0.1043, -0.3691],\n",
      "          [-0.2940, -0.6218,  0.5029, -0.4131],\n",
      "          [-0.4518, -0.1813, -0.0158, -0.1033]]],\n",
      "\n",
      "\n",
      "        [[[-0.0239, -1.1511, -0.4069, -1.3810],\n",
      "          [-1.4270,  0.5460,  0.1839,  0.6636],\n",
      "          [-0.1410, -0.6418, -0.3549, -0.5290]],\n",
      "\n",
      "         [[ 0.2647, -0.6113, -0.0314,  0.5024],\n",
      "          [-0.1671, -0.5133,  0.8250, -0.3704],\n",
      "          [-0.7715, -0.4057,  0.6064,  0.0792]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shape:  torch.Size([2, 2, 3, 4])\n",
      "values:  tensor([[[[ 0.5662,  1.2474,  0.6110,  0.3846],\n",
      "          [ 0.3909,  0.2873,  0.0760,  0.7672],\n",
      "          [-0.3288, -0.8719,  1.3444, -0.0021]],\n",
      "\n",
      "         [[ 0.7495,  0.0327, -0.5575, -0.4441],\n",
      "          [ 0.7780,  0.0809, -0.5373, -0.4977],\n",
      "          [ 0.5524,  0.3307,  0.4051, -0.3111]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7716,  0.8189, -0.1598,  0.7462],\n",
      "          [-0.4825,  0.3338,  0.0198,  0.0347],\n",
      "          [-0.0621, -0.6016,  1.0996,  0.6104]],\n",
      "\n",
      "         [[-0.2320,  0.1364, -0.8177, -0.1223],\n",
      "          [ 0.6211,  0.2088,  0.4031, -0.8069],\n",
      "          [-0.0697,  0.9557, -0.4313, -0.1889]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Keys and Values are also transformed in the same way as queries.\n",
    "keys = keys.transpose(dim0=1, dim1=2)\n",
    "print(\"shape: \", keys.shape)\n",
    "print(\"keys: \", keys)\n",
    "print(\"-\" * 150)\n",
    "\n",
    "values = values.transpose(dim0=1, dim1=2)\n",
    "print(\"shape: \", values.shape)\n",
    "print(\"values: \", values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have queries, keys and values for each head in the Multi-Headed Attention. The next step is to \n",
    "# calculate the attention scores for each token within each head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 2, 4, 3])\n",
      "keys:  tensor([[[[-0.1720, -0.3618, -0.1265],\n",
      "          [-0.0974, -0.0844,  0.2863],\n",
      "          [-0.4074, -0.5938,  0.0526],\n",
      "          [-0.3137, -0.0148,  0.3498]],\n",
      "\n",
      "         [[-0.7831, -0.2940, -0.4518],\n",
      "          [-1.3113, -0.6218, -0.1813],\n",
      "          [ 0.1043,  0.5029, -0.0158],\n",
      "          [-0.3691, -0.4131, -0.1033]]],\n",
      "\n",
      "\n",
      "        [[[-0.0239, -1.4270, -0.1410],\n",
      "          [-1.1511,  0.5460, -0.6418],\n",
      "          [-0.4069,  0.1839, -0.3549],\n",
      "          [-1.3810,  0.6636, -0.5290]],\n",
      "\n",
      "         [[ 0.2647, -0.1671, -0.7715],\n",
      "          [-0.6113, -0.5133, -0.4057],\n",
      "          [-0.0314,  0.8250,  0.6064],\n",
      "          [ 0.5024, -0.3704,  0.0792]]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# queries: [batch_size, num_heads, seq_len, d_k]\n",
    "# keys   : [batch_size, num_heads, seq_len, d_k]\n",
    "# \n",
    "# To calculate the attention scores, we need to compute [queries * keys^{Transpose}] for each sentence. \n",
    "# * here represents matrix multiplication. So, we transpose keys and then perform the matrix multiplication.\n",
    "#\n",
    "# transposed_keys: [batch_size, num_heads, d_k, seq_len]\n",
    "keys = keys.transpose(dim0=2, dim1=3)\n",
    "print(\"shape: \", keys.shape)\n",
    "print(\"keys: \", keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 2, 3, 3])\n",
      "attention_scores: \n",
      " tensor([[[[ 0.0207, -0.0143, -0.0674],\n",
      "          [ 0.1255,  0.1223, -0.1535],\n",
      "          [-0.1467, -0.0965,  0.1083]],\n",
      "\n",
      "         [[ 0.2201, -0.0801,  0.1437],\n",
      "          [-0.0335,  0.0499,  0.0102],\n",
      "          [ 0.2111, -0.2235, -0.0176]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5600, -0.8450,  0.1783],\n",
      "          [ 0.6453, -0.6661,  0.2507],\n",
      "          [ 0.2900, -0.3500,  0.1462]],\n",
      "\n",
      "         [[-0.0637, -0.1002,  0.0466],\n",
      "          [-0.2561, -0.1172,  0.1331],\n",
      "          [ 0.1381, -0.4218, -0.4746]]]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# queries: [batch_size, num_heads, seq_len, d_k]\n",
    "# keys   : [batch_size, num_heads, d_k, seq_len]\n",
    "#\n",
    "# We scale the scores by the square root of the dimension of the vectors (d_k) to ensure that the scores don't \n",
    "# grow too large. We later apply softmax on these scores to normalize them. So, during the gradient calculation, \n",
    "# if the values after applying softmax are too large, the gradients will be too small. To avoid this, we scale \n",
    "# the scores to bring them down.\n",
    "#\n",
    "# attention_scores: [batch_size, num_heads, seq_len, seq_len]\n",
    "#\n",
    "# Lets ignore the batch dimension for now.\n",
    "# Each 2D matrix in the attention_scores tensor will contain the attention scores for each token\n",
    "# in the sequence with every other token in the sequence.\n",
    "# Example: \n",
    "# [0.0207,  -0.0143,  -0.0674] --> Attention scores for the first token in the sequence.\n",
    "# \n",
    "# 0.0207  --> Attention score for the first token with the first token.\n",
    "# -0.0143 --> Attention score for the first token with the second token.\n",
    "# -0.0674 --> Attention score for the first token with the third token.\n",
    "# \n",
    "# ofcourse, the numbers will change if you run this cell again.\n",
    "attention_scores = torch.matmul(queries, keys) / math.sqrt(d_k)\n",
    "print(\"shape: \", attention_scores.shape)\n",
    "print(\"attention_scores: \\n\", attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 1, 3, 3])\n",
      "mask_random: \n",
      " tensor([[[[False,  True,  True],\n",
      "          [ True, False, False],\n",
      "          [ True,  True, False]]],\n",
      "\n",
      "\n",
      "        [[[ True, False,  True],\n",
      "          [False, False, False],\n",
      "          [ True, False, False]]]])\n"
     ]
    }
   ],
   "source": [
    "# You can skip this cell (and the next one) and come back to this after completing the entire \n",
    "# notebook by ignoring masks. We mask the attention_scores if a mask is provided.\n",
    "# We have 1 mask for 1 sequence in the batch. We will create a random mask for now for experimentation.\n",
    "# Please note that this random mask might not even follow all the rules that the actual mask should \n",
    "# follow. Refer to 'step_5_data_batching_and_masking.ipynb' to understand more about creating masks \n",
    "# for the transformer inputs.\n",
    "#\n",
    "# attention_scores: [batch_size, num_heads, seq_len, seq_len]\n",
    "# So, we also need the mask to have 4 dimensions. Every head uses the same mask since it is \n",
    "# essentially the same sentence being used in all the heads.\n",
    "# Remember that 'True' in 'mask_random' means the value should not be masked and 'False' means the \n",
    "# value should be masked.\n",
    "mask_random = torch.randint(low=-50, high=50, size=(batch_size, 1, seq_len, seq_len)) < 0\n",
    "print(\"shape: \", mask_random.shape)\n",
    "print(\"mask_random: \\n\", mask_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 2, 3, 3])\n",
      "attention_scores: \n",
      " tensor([[[[-1.0000e+09, -1.4259e-02, -6.7406e-02],\n",
      "          [ 1.2546e-01, -1.0000e+09, -1.0000e+09],\n",
      "          [-1.4671e-01, -9.6459e-02, -1.0000e+09]],\n",
      "\n",
      "         [[-1.0000e+09, -8.0123e-02,  1.4368e-01],\n",
      "          [-3.3508e-02, -1.0000e+09, -1.0000e+09],\n",
      "          [ 2.1106e-01, -2.2352e-01, -1.0000e+09]]],\n",
      "\n",
      "\n",
      "        [[[ 5.6001e-01, -1.0000e+09,  1.7827e-01],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [ 2.8996e-01, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-6.3654e-02, -1.0000e+09,  4.6578e-02],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [ 1.3805e-01, -1.0000e+09, -1.0000e+09]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# The same mask is applied to all the heads of a single sentence. Notice that the\n",
    "# corresponding values (with values as False) in the attention_scores tensor are \n",
    "# set to '-1e9'. Please do not set this to '-inf' as it will cause the softmax to\n",
    "# return 'nan' values in some cases --> You can try setting it to '-inf' and see \n",
    "# what happens.\n",
    "attention_scores = attention_scores.masked_fill(mask_random == False, float('-1e9'))\n",
    "print(\"shape: \", attention_scores.shape)\n",
    "print(\"attention_scores: \\n\", attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 2, 3, 3])\n",
      "attention_scores:  tensor([[[[0.0000, 0.5133, 0.4867],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [0.4874, 0.5126, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.4443, 0.5557],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [0.6070, 0.3930, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5943, 0.0000, 0.4057],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4725, 0.0000, 0.5275],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# The attention scores for the tokens which have been masked are set to zero by the softmax function.\n",
    "# When we say set to zero, we mean the value is so small that it is practically zero. This means \n",
    "# these tokens won't contribute to the value calculation. \n",
    "attention_scores = attention_scores.softmax(dim=-1)\n",
    "print(\"shape: \", attention_scores.shape)\n",
    "print(\"attention_scores: \", attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 2, 3, 4])\n",
      "attention_heads:  tensor([[[[ 0.0406, -0.2769,  0.6934,  0.3928],\n",
      "          [ 0.5662,  1.2474,  0.6110,  0.3846],\n",
      "          [ 0.4764,  0.7553,  0.3367,  0.5807]],\n",
      "\n",
      "         [[ 0.6527,  0.2197, -0.0136, -0.3940],\n",
      "          [ 0.7495,  0.0327, -0.5575, -0.4441],\n",
      "          [ 0.7607,  0.0516, -0.5495, -0.4652]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4334,  0.2426,  0.3511,  0.6911],\n",
      "          [ 0.0757,  0.1837,  0.3198,  0.4638],\n",
      "          [ 0.7716,  0.8189, -0.1598,  0.7462]],\n",
      "\n",
      "         [[-0.1464,  0.5686, -0.6139, -0.1575],\n",
      "          [ 0.1065,  0.4336, -0.2820, -0.3727],\n",
      "          [-0.2320,  0.1364, -0.8177, -0.1223]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# We now have the attention scores for each token in each head. The next step is to calculate the \n",
    "# weighted sum of the values using these scores. These are called the attention heads.\n",
    "#\n",
    "# attention_scores: [batch_size, num_heads, seq_len, seq_len]\n",
    "# values          : [batch_size, num_heads, seq_len, d_k]\n",
    "#\n",
    "# Lets consider 2 Matrices Mat1 and Mat2 and Output = Mat1 * Mat2.\n",
    "# Mat1: [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "# Mat2: [[10, 11, 12], [13, 14, 15], [16, 17, 18]]\n",
    "# \n",
    "# Output[0] = Mat1[0] * Mat2 = [1, 2, 3] * [[10, 11, 12], [13, 14, 15], [16, 17, 18]]\n",
    "#          = 1 * [10, 11, 12] + 2 * [13, 14, 15] + 3 * [16, 17, 18]\n",
    "#\n",
    "# So, the first row of the output matrix will be the dot product of the first row of Mat1 with\n",
    "# all the rows of Mat2.\n",
    "#\n",
    "# Output[1] = 4 * [10, 11, 12] + 5 * [13, 14, 15] + 6 * [16, 17, 18]\n",
    "# Output[2] = 7 * [10, 11, 12] + 8 * [13, 14, 15] + 9 * [16, 17, 18]\n",
    "# \n",
    "# For our case, 1, 2, 3 are the attention scores and [10, 11, 12], [13, 14, 15], [16, 17, 18]\n",
    "# are the corresponding value vectors.\n",
    "#\n",
    "# Each row in the last dimension of the values tensor contain the values for the tokens in the \n",
    "# sequence. So, the weighted sum of the values for a token in the sequence is calculated by \n",
    "# multiplying the attention scores for that token with the values for all the tokens in the \n",
    "# sequence.\n",
    "#\n",
    "# attention_heads: [batch_size, num_heads, seq_len, d_k]\n",
    "attention_heads = torch.matmul(attention_scores, values)\n",
    "print(\"shape: \", attention_heads.shape)\n",
    "print(\"attention_heads: \", attention_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 3, 2, 4])\n",
      "attention_heads:  tensor([[[[ 0.0406, -0.2769,  0.6934,  0.3928],\n",
      "          [ 0.6527,  0.2197, -0.0136, -0.3940]],\n",
      "\n",
      "         [[ 0.5662,  1.2474,  0.6110,  0.3846],\n",
      "          [ 0.7495,  0.0327, -0.5575, -0.4441]],\n",
      "\n",
      "         [[ 0.4764,  0.7553,  0.3367,  0.5807],\n",
      "          [ 0.7607,  0.0516, -0.5495, -0.4652]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4334,  0.2426,  0.3511,  0.6911],\n",
      "          [-0.1464,  0.5686, -0.6139, -0.1575]],\n",
      "\n",
      "         [[ 0.0757,  0.1837,  0.3198,  0.4638],\n",
      "          [ 0.1065,  0.4336, -0.2820, -0.3727]],\n",
      "\n",
      "         [[ 0.7716,  0.8189, -0.1598,  0.7462],\n",
      "          [-0.2320,  0.1364, -0.8177, -0.1223]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# We now have attention heads for each token in each head. The next step is to concatenate the\n",
    "# attention heads for each token from all the heads. Before concatenating, we need to rearrange\n",
    "# the dimensions of the attention heads tensor appropriately. This is a preparation step for to \n",
    "# apply the concatenation operation in the next step.\n",
    "#\n",
    "# attention_heads           : [batch_size, num_heads, seq_len, d_k]\n",
    "# transposed_attention_heads: [batch_size, seq_len, num_heads, d_k]\n",
    "attention_heads = attention_heads.transpose(dim0=1, dim1=2)\n",
    "print(\"shape: \", attention_heads.shape)\n",
    "print(\"attention_heads: \", attention_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_contiguous:  False\n",
      "shape:  torch.Size([2, 3, 8])\n",
      "attention_heads:  tensor([[[ 0.0406, -0.2769,  0.6934,  0.3928,  0.6527,  0.2197, -0.0136,\n",
      "          -0.3940],\n",
      "         [ 0.5662,  1.2474,  0.6110,  0.3846,  0.7495,  0.0327, -0.5575,\n",
      "          -0.4441],\n",
      "         [ 0.4764,  0.7553,  0.3367,  0.5807,  0.7607,  0.0516, -0.5495,\n",
      "          -0.4652]],\n",
      "\n",
      "        [[ 0.4334,  0.2426,  0.3511,  0.6911, -0.1464,  0.5686, -0.6139,\n",
      "          -0.1575],\n",
      "         [ 0.0757,  0.1837,  0.3198,  0.4638,  0.1065,  0.4336, -0.2820,\n",
      "          -0.3727],\n",
      "         [ 0.7716,  0.8189, -0.1598,  0.7462, -0.2320,  0.1364, -0.8177,\n",
      "          -0.1223]]], grad_fn=<UnsafeViewBackward0>)\n",
      "is_contiguous:  True\n"
     ]
    }
   ],
   "source": [
    "# We now have attention heads in the required shape. The next step is to concatenate the attention\n",
    "# heads for each token from all the heads to get a single attention head vector per token.\n",
    "# \n",
    "# attentions_heads: [batch_size, seq_len, num_heads, d_k]\n",
    "# concatenated_attention_heads: [batch_size, seq_len, d_model]\n",
    "#\n",
    "# Since the tensor is not contiguous, we use reshape instead of view. view might fail if the \n",
    "# tensor is not contiguous.\n",
    "print(\"is_contiguous: \", attention_heads.is_contiguous())\n",
    "attention_heads = attention_heads.reshape(batch_size, seq_len, d_model)\n",
    "print(\"shape: \", attention_heads.shape)\n",
    "print(\"attention_heads: \", attention_heads)\n",
    "print(\"is_contiguous: \", attention_heads.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_creator:  Linear(in_features=8, out_features=8, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# We finally pass our concatenated attention heads through a linear layer to get the output of the\n",
    "# multi-headed attention layer.\n",
    "output_creator = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "print(\"output_creator: \", output_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([2, 3, 8])\n",
      "multi_head_attention_output:  tensor([[[ 0.5358,  0.0076,  0.2317,  0.2802, -0.4445,  0.3625, -0.0881,\n",
      "           0.4434],\n",
      "         [ 0.6170, -0.0066, -0.5055,  0.6572,  0.0556,  0.0129, -0.3898,\n",
      "           0.8623],\n",
      "         [ 0.5143,  0.0054, -0.3420,  0.5535, -0.0222, -0.0262, -0.3684,\n",
      "           0.8703]],\n",
      "\n",
      "        [[ 0.0982, -0.3812, -0.0171,  0.1120, -0.1386,  0.3311, -0.1053,\n",
      "           0.6196],\n",
      "         [ 0.3642, -0.3084,  0.0067,  0.1829, -0.1968,  0.3673, -0.0892,\n",
      "           0.5677],\n",
      "         [-0.0052, -0.3488, -0.3012,  0.1479,  0.1463, -0.0018, -0.2324,\n",
      "           0.7958]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# attention_heads: [batch_size, seq_len, d_model]\n",
    "# multi_head_attention_output: [batch_size, seq_len, d_model]\n",
    "multi_head_attention_output = output_creator(attention_heads)\n",
    "print(\"shape: \", multi_head_attention_output.shape)\n",
    "print(\"multi_head_attention_output: \", multi_head_attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Headed Attention In Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a copy (deepcopy) of the module and returns ModuleList containing the copies.\n",
    "def clone_module(module: nn.Module, num_clones: int) -> nn.ModuleList:\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(num_clones)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_attention_heads(queries: Tensor, keys: Tensor, values: Tensor, mask: Optional[Tensor]=None, dropout_layer: Optional[nn.Module]=None) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Calculates the attention scores for each token in the sequence with every other token in the sequence.\n",
    "       Applues the mask if provided and then normalizes the scores using softmax. It then calculates the \n",
    "       attention heads for each token in the sequence.\n",
    "\n",
    "    Args:\n",
    "        queries (Tensor): [batch_size, num_heads, seq_len, d_k]\n",
    "        keys (Tensor): [batch_size, num_heads, seq_len, d_k]\n",
    "        values (Tensor): [batch_size, num_heads, seq_len, d_k]\n",
    "        mask (Optional[Tensor], optional): [batch_size, 1, seq_len, seq_len]. Defaults to None.\n",
    "        dropout_layer (Optional[nn.Module], optional): probability with which the values are dropped on dropout layer. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: Returns the attention heads and the attention scores.\n",
    "                               attention_heads: [batch_size, num_heads, seq_len, d_k]\n",
    "                               attention_scores: [batch_size, num_heads, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    # Size of the vectors for each token for each head in the sequence.\n",
    "    d_k = queries.shape[-1]\n",
    "    # Calculate the attention scores for each token in the sequence with every other token in the sequence.\n",
    "    attention_scores = torch.matmul(queries, keys.transpose(dim0=2, dim1=3)) / math.sqrt(d_k)\n",
    "    # Mask the attention scores if a mask is provided. Mask is used in two different ways:\n",
    "    # 1) To prevent the model from attending to the padding tokens --> This applies for both src and tgt sentences.\n",
    "    # 2) To prevent the model from attending to the future tokens in the sequence --> This applies only for tgt sentences.\n",
    "    if mask is not None:\n",
    "        # Please do not set the masked values to float('-inf') as it sometimes (not in everycase) causes softmax to return nan.\n",
    "        attention_scores = attention_scores.masked_fill(mask == False, float('-1e9'))\n",
    "    # Normalize the attention scores using softmax.\n",
    "    attention_scores = attention_scores.softmax(dim=-1)\n",
    "    # Apply dropout regularization to prevent overfitting problems.\n",
    "    if dropout_layer is not None:\n",
    "        dropout_layer(attention_scores)\n",
    "    # Calculate the attention heads for each token in the sequence. The head for each token is calculated by\n",
    "    # taking the weighted average (averaged by attention scores) of the values for all the tokens in the \n",
    "    # sequence for the token of interest.\n",
    "    attention_heads = torch.matmul(attention_scores, values)\n",
    "    return attention_heads, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are just going to combine everything above and put it into a class. This class will be used\n",
    "# to create the Multi-Headed Attention layer in the Transformer model.\n",
    "# Refer to 'using_modules.ipynb' (Add link to the notebook) to understand more about Pytorch modules.\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_model: int, dropout_prob: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads.\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        # We use dropout to prevent overfitting.\n",
    "        self.dropout_layer = nn.Dropout(p=dropout_prob)\n",
    "        # Creating the linear layers that generate queries, keys and values for each token in the sequence.\n",
    "        # Also, creating an additional linear layer to generate the output of the Multi-Headed Attention from concatenated attention heads.\n",
    "        self.linear_layers = clone_module(module=nn.Linear(in_features=d_model, out_features=d_model), num_clones=4)\n",
    "\n",
    "\n",
    "    def forward(self, query_input: Tensor, key_input: Tensor, value_input: Tensor, mask: Optional[Tensor]=None) -> Tensor:\n",
    "        \"\"\"Forward pass of the Multi-Headed Attention layer. \n",
    "\n",
    "        Args:\n",
    "            query (Tensor): Input to be used for query creation.\n",
    "                            query_input: [batch_size, seq_len, d_model]\n",
    "            key (Tensor): Input to be used for key creation.\n",
    "                          key_input  : [batch_size, seq_len, d_model]\n",
    "            value (Tensor): Input to be used for value creation.\n",
    "                            value_input: [batch_size, seq_len, d_model]\n",
    "            mask (Tensor): Mask to be applied to the attention scores. Default is None. Same mask will \n",
    "                           be applied to all the heads in the Multi-Headed Attention layer.\n",
    "                           mask: [batch_size, 1, seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Mutli-Headed Attention Output: Output of the Multi-Headed Attention layer. Generates one output vector \n",
    "                                           for each token in the sequence. Does this for each sequence in the batch.\n",
    "                                           output: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Generates the queries, keys and values for each token in the sequence.\n",
    "        # shape of queries, keys, values: [batch_size, seq_len, d_model]\n",
    "        queries, keys, values = [linear_layer(input) for linear_layer, input in zip(self.linear_layers, (query_input, key_input, value_input))]\n",
    "        batch_size = query_input.shape[0]\n",
    "        seq_len = query_input.shape[1]\n",
    "        # Separating the queries, keys and values for each head into a separate vector. The vectors for each token in all the heads\n",
    "        # are concatenated when they are created using the linear_layers above.\n",
    "        # Shape for queries, keys, values after view: [batch_size, seq_len, num_heads, d_k]\n",
    "        # Shape for queries, key, values after transpose: [batch_size, num_heads, seq_len, d_k]\n",
    "        queries, keys, values = [data.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(dim0=1, dim1=2) for data in (queries, keys, values)]\n",
    "        # Calculate the attention heads for each token in the sequence.\n",
    "        # attention_heads: [batch_size, num_heads, seq_len, d_k]\n",
    "        attention_heads, attention_scores = construct_attention_heads(queries=queries, keys=keys, values=values, mask=mask, dropout_layer=self.dropout_layer)\n",
    "        # Concatenate the attention heads for each token from all the heads.\n",
    "        # attention_heads: [batch_size, seq_len, d_model]\n",
    "        attention_heads = attention_heads.transpose(dim0=1, dim1=2).reshape(batch_size, seq_len, self.d_model)\n",
    "        # Generate the output of the Multi-Headed Attention layer.\n",
    "        return self.linear_layers[-1](attention_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadedAttention(\n",
      "  (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "  (linear_layers): ModuleList(\n",
      "    (0-3): 4 x Linear(in_features=8, out_features=8, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "multi_headed_attention_layer = MultiHeadedAttention(num_heads=2, d_model=8, dropout_prob=0.1)\n",
    "print(multi_headed_attention_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([2, 3, 8])\n",
      "input:  tensor([[[-0.3415,  1.7753,  0.0380,  0.1188,  0.4282,  0.2899, -0.4513,\n",
      "           0.3557],\n",
      "         [-0.6632, -1.2565, -0.1667, -1.9468, -0.0124, -0.7232,  0.2141,\n",
      "          -0.5374],\n",
      "         [-0.2168,  0.7195, -2.5153,  1.2357, -0.7998, -0.4453,  0.7830,\n",
      "           1.4223]],\n",
      "\n",
      "        [[-1.2573,  1.0951,  0.9087, -0.7431, -0.2753,  1.4350,  1.1018,\n",
      "          -0.7295],\n",
      "         [ 0.3248, -0.9626, -0.2803, -1.0175,  0.2620, -0.6605, -1.0310,\n",
      "           0.5023],\n",
      "         [ 1.0583, -1.7163, -0.7393,  1.1486,  1.1051, -1.3013,  0.2931,\n",
      "          -1.4220]]])\n",
      "output shape:  torch.Size([2, 3, 8])\n",
      "output:  tensor([[[-0.1944,  0.1598,  0.3522,  0.1647, -0.2321,  0.1088, -0.2963,\n",
      "           0.4579],\n",
      "         [ 0.1834,  0.0292,  0.2885,  0.3071, -0.1512,  0.2481, -0.1918,\n",
      "           0.3766],\n",
      "         [-0.3755, -0.0380,  0.4772,  0.3606, -0.0559,  0.1052, -0.1906,\n",
      "           0.7190]],\n",
      "\n",
      "        [[ 0.3711, -0.1907,  0.3214,  0.0076, -0.2554, -0.1442,  0.0702,\n",
      "           0.0829],\n",
      "         [ 0.2284,  0.0658,  0.0729,  0.1279, -0.4080,  0.3001,  0.0303,\n",
      "           0.1767],\n",
      "         [ 0.3237,  0.0749, -0.1161,  0.1101, -0.5722,  0.5245,  0.2183,\n",
      "           0.2050]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"input shape: \", input.shape)\n",
    "print(\"input: \", input)\n",
    "transformer_multi_headed_attention_output = multi_headed_attention_layer(query_input=input, key_input=input, value_input=input)\n",
    "print(\"output shape: \", transformer_multi_headed_attention_output.shape)\n",
    "print(\"output: \", transformer_multi_headed_attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
