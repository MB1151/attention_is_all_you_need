{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) How all the building blocks of the transformer fit together to make a machine translation model?\n",
    "# 2) Verfiy that the number of parameters within the built model is the same as number of parameters\n",
    "#    expected within out model architecture ie.,\n",
    "#           --> assert parameter_count (built_model) == parameter_count (count_manually). \n",
    "#\n",
    "# A LOT OF THE CODE IN THIS NOTEBOOK IS COPIED FROM THE PREVIOUS NOTEBOOKS. I WILL NOT BE EXPLAINING \n",
    "# THOSE PARTS IN THIS NOTEBOOK AGAIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources to help understand this notebook:\n",
    "#\n",
    "# 1) https://nlp.seas.harvard.edu/annotated-transformer/\n",
    "#       --  The very best resource to understand the transformer code.\n",
    "# 2) http://jalammar.github.io/illustrated-transformer/\n",
    "#       -- Another great resource to understand the architecture of the transformer model.\n",
    "# 3) https://www.youtube.com/watch?v=8krd5qKVw-Q\n",
    "#       -- Gives an intuitive explanation of Xavier Initialization.\n",
    "# 4) https://www.deeplearning.ai/ai-notes/initialization/index.html\n",
    "#       -- Gives an in-depth mathematical explanation of Xavier Initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../Data/Images/TranslationTransformerModel.png\" alt=\"Translation Transformer Model\" width=\"600\" height=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credits: The above image is taken from this blog post: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transformer model is a stack of N encoder-decoder layers.\n",
    "# In our implementation, each of the ENCODER boxes shown in the above image is referred to as an \n",
    "# EncoderLayer and each of the DECODER boxes is referred to as a DecoderLayer. The stack of 6 ENCODER \n",
    "# boxes (EncoderLayers) is referred to as an Encoder and the stack of 6 DECODER boxes (DecoderLayers) \n",
    "# is referred to as a Decoder. The entire model is referred to as a Transformer.\n",
    "#\n",
    "# The bottom EncoderLayer receives the tokenized src sentence embeddings and the bottom DecoderLayer \n",
    "# receives the tokenized tgt sentence embeddings. The sentences are tokenized in the Data Preparation\n",
    "# phase which is not part of the Transformer model below. The tokens are converted into embeddings, \n",
    "# aggregated with the positional encodings and passed to the bottom EncoderLayer and bottom DecoderLayer \n",
    "# as inputs.\n",
    "#\n",
    "# The output of the ENCODER (the last EncoderLayer) is passed to each of the DecoderLayers for src \n",
    "# attention calculation. The output of the DECODER (the last DecoderLayer) is passed to a linear \n",
    "# layer which projects the Decoder output to the vocab space (target vocab space). This output is then \n",
    "# passed to the softmax layer to get the token output probabilities which is the output of the \n",
    "# Transformer model as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../Data/Images/EncoderDecoder.png\" alt=\"EncoderDecoder\" width=\"550\" height=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credits: The above image is taken from this blog post: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants to be used in the model.\n",
    "# Size of the embedding vectors in the model. This is 512 in the transformer paper.\n",
    "d_model = 8\n",
    "# Number of layers in the Encoder in the Encoder and Decoder. The transformer paper also uses 6. The \n",
    "# number of layers could be different in Encoder and Decoder but we use the same value for both.\n",
    "num_layers = 6\n",
    "# Probability with which to drop data in the transformer model. We will use the same dropout_prob\n",
    "# throughout the model.\n",
    "dropout_prob = 0.1\n",
    "# Number of attention heads in each of the multi-head attention layers in the model. The transformer\n",
    "# paper uses 8.\n",
    "num_heads = 8\n",
    "# Number of neurons in the hidden layer (that expands the input) in the feed forward neural network.\n",
    "# The transformer paper uses 2048.\n",
    "d_feed_forward = 16\n",
    "# Number of sentences in each batch of data.\n",
    "batch_size = 2\n",
    "# Number of tokens in the src vocab. This is the number of unique words in the src language (English).\n",
    "src_vocab_size = 6\n",
    "# Number of tokens in the tgt vocab. This is the number of unique words in the tgt language (Telugu).\n",
    "tgt_vocab_size = 6\n",
    "# Number of tokens in each of the sentences in the batch. We will pad the sentences to make them all\n",
    "# of the same length. This is the length of the longest sentence in the batch.\n",
    "seq_len = 4\n",
    "# Maximum number of tokens in the sentences among all the batches in the entire dataset.\n",
    "max_seq_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVERYTHING IN THIS CELL HAS BEEN EXPLAINED IN DETAIL IN THE PREVIOUS NOTEBOOKS. PLEASE REFER TO THE EARLIER\n",
    "# NOTEBOOKS TO UNDERSTAND THE CODE IN THIS CELL. YOU CAN SKIP (JUST RUN IT BLINDLY) THIS CELL AND MOVE TO THE \n",
    "# NEXT CELL DIRECTLY. \n",
    "\n",
    "# Refer to 'step_8_word_embeddings.ipynb' notebook to learn more about the Embeddings class.\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        \"\"\"Creates the embedding layer that serves as a look-up table for the tokens in the transformer model.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary i.e., number of distinct tokens in the vocabulary.\n",
    "            embedding_dim (int): The size of the embedding vector to be generated for each token.\n",
    "        \"\"\"\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.look_up_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    # The input is be a '2D' tensor where each '1D' tensor within the '2D' tensor is the list\n",
    "    # of indices corresponding to the tokens in the vocab.\n",
    "    # [[0, 123, 3455, 4556, 7, 1, 2, 2], [0, 56, 98, 6234, 909, 56, 1, 2]]\n",
    "    # 0 - <SOS>, 1 - <eos>, 2 - <pad>\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"Converts the input tensor of token indices to their corresponding embedding vectors.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): The input tensor of token indices.\n",
    "                            shape: [batch_size, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The tensor of embedding vectors for the corresponding input tokens.\n",
    "                    shape: [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        # There is no reasoning as to why the original 'attention_is_all_you_need' paper scaled the\n",
    "        # embeddings using 'math.sqrt(embedding_dim)'. A few blogs attempted to explain this reasoning,\n",
    "        # but I haven't found anything with solid reasoning.\n",
    "        return self.look_up_table(input) * math.sqrt(self.embedding_dim)\n",
    "\n",
    "\n",
    "# Refer to 'step_10_positional_encoding.ipynb' notebook to learn more about the PositionalEncoding class.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # d_model is the same as encoding_size.\n",
    "    def __init__(self, encoding_size: int, dropout_prob: float, max_len: int = 5000):\n",
    "        \"\"\"Creates the positional encodings.\n",
    "\n",
    "        Args:\n",
    "            encoding_size (int): Size of the positional encoding vector that represents the position of the token.\n",
    "            dropout_prob (float): Probability of an element to be zeroed or dropped.\n",
    "            max_len (int, optional): Largest position for which the positional encoding vector is generated. Defaults to 5000.\n",
    "                                     By default, it generates positional encodings for the first 5000 positions.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Refer to step_8_drop_out.ipynb notebook (link to the notebook) to understand more about dropout.\n",
    "        self.dropout = nn.Dropout(p=dropout_prob, inplace=False)\n",
    "        # Compute the positional encodings in log space.\n",
    "        positional_encoding = torch.zeros(size=(max_len, encoding_size), dtype=torch.float)\n",
    "        positional_encoding_numerators = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        numerators_in_exponent = torch.arange(0, encoding_size, 2, dtype=torch.float)\n",
    "        positional_encoding_denominators = torch.exp(numerators_in_exponent * (-math.log(10000.0) / encoding_size))\n",
    "        positional_encoding[:, 0::2] = torch.sin(positional_encoding_numerators * positional_encoding_denominators)\n",
    "        positional_encoding[:, 1::2] = torch.cos(positional_encoding_numerators * positional_encoding_denominators)\n",
    "        # Refer to understanding_tensor_manipulations_part_1.ipynb notebook (link to the notebook) to\n",
    "        # understand more about unsqueeze operation in pytorch.\n",
    "        # In transformer model, we receive 3D tensors as input to this module. Each 1D tensor\n",
    "        # in the last dimension is an embedding for the token. Each 2D tensor is a sentence.\n",
    "        # The entire 3D tensor is a batch of sentences. To work with 3D tensors in the forward\n",
    "        # method, we convert the positional encoding to a 3D tensor.\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        # Refer to using_modules.ipynb (link to the notebook) to understand more about buffers in pytorch.\n",
    "        # This tells the module to not update the positional encoding tensor during the training. It is \n",
    "        # not a trainable parameter but it is still part of the state of the model.\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "    \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"Adds the positional encodings to the input tensor.\n",
    "        Args:\n",
    "            input (Tensor): The input tensor containing the embeddings of the tokens.\n",
    "                            shape: [batch_size, sentence_length, d_model]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Input with the positional encodings added to it.\n",
    "                    shape: [batch_size, sentence_length, d_model]\n",
    "                    d_model is the same as encoding_size.\n",
    "        \"\"\"\n",
    "        # Refer to understanding_tensor_manipulations_part_5.ipynb notebook (link to the notebook) to \n",
    "        # understand more about broadcasting in python.\n",
    "        # The input tensor is a 3D tensor of shape (batch_size, sentence_length, encoding_size).\n",
    "        # We add (uses broadcasting) the positional encoding to the input tensor to get the final tensor.\n",
    "        # positional_encoding: (1, max_len, encoding_size) --> (1, sentence_length, encoding_size) \n",
    "        #       -- Extracts the positional encodings for the sentence_length from the positional_encoding \n",
    "        #          tensor.\n",
    "        # (batch_size, sentence_length, encoding_size) --> input\n",
    "        # (batch_size, sentence_length, encoding_size) --> Resultant tensor shape after broadcasting.\n",
    "        # requires_grad_(False) is not needed since the positional encoding is already registered\n",
    "        # as a Buffer and not a trainable parameter. It is just included for clarity.\n",
    "        input = input + self.positional_encoding[:, :input.size(1)].requires_grad_(False)\n",
    "        return self.dropout(input)\n",
    "    \n",
    "\n",
    "# Creates a copy (deepcopy) of the module and returns ModuleList containing the copies.\n",
    "def clone_module(module: nn.Module, num_clones: int) -> nn.ModuleList:\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(num_clones)])\n",
    "\n",
    "\n",
    "# Refer to 'step_11_multi_headed_attention.ipynb' notebook to understand how this function works.\n",
    "def construct_attention_heads(queries: Tensor, keys: Tensor, values: Tensor, mask: Optional[Tensor]=None, dropout_layer: Optional[nn.Module]=None) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Calculates the attention scores for each token in the sequence with every other token in the sequence.\n",
    "       Applues the mask if provided and then normalizes the scores using softmax. It then calculates the \n",
    "       attention heads for each token in the sequence.\n",
    "\n",
    "    Args:\n",
    "        queries (Tensor): [batch_size, num_heads, seq_len, d_k]\n",
    "        keys (Tensor): [batch_size, num_heads, seq_len, d_k]\n",
    "        values (Tensor): [batch_size, num_heads, seq_len, d_k]\n",
    "        mask (Optional[Tensor], optional): [batch_size, 1, seq_len, seq_len]. Defaults to None.\n",
    "        dropout_layer (Optional[nn.Module], optional): probability with which the values are dropped on dropout layer. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: Returns the attention heads and the attention scores.\n",
    "                               attention_heads: [batch_size, num_heads, seq_len, d_k]\n",
    "                               attention_scores: [batch_size, num_heads, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    # Size of the vectors for each token for each head in the sequence.\n",
    "    d_k = queries.shape[-1]\n",
    "    # Calculate the attention scores for each token in the sequence with every other token in the sequence.\n",
    "    attention_scores = torch.matmul(queries, keys.transpose(dim0=2, dim1=3)) / math.sqrt(d_k)\n",
    "    # Mask the attention scores if a mask is provided. Mask is used in two different ways:\n",
    "    # 1) To prevent the model from attending to the padding tokens --> This applies for both src and tgt sentences.\n",
    "    # 2) To prevent the model from attending to the future tokens in the sequence --> This applies only for tgt sentences.\n",
    "    if mask is not None:\n",
    "        # Please do not set the masked values to float('-inf') as it sometimes (not in everycase) causes softmax to return nan.\n",
    "        attention_scores = attention_scores.masked_fill(mask == False, float('-1e9'))\n",
    "    # Normalize the attention scores using softmax.\n",
    "    attention_scores = attention_scores.softmax(dim=-1)\n",
    "    # Apply dropout regularization to prevent overfitting problems.\n",
    "    if dropout_layer is not None:\n",
    "        dropout_layer(attention_scores)\n",
    "    # Calculate the attention heads for each token in the sequence. The head for each token is calculated by\n",
    "    # taking the weighted average (averaged by attention scores) of the values for all the tokens in the \n",
    "    # sequence for the token of interest.\n",
    "    attention_heads = torch.matmul(attention_scores, values)\n",
    "    return attention_heads, attention_scores\n",
    "\n",
    "\n",
    "# Refer to 'step_11_multi_headed_attention.ipynb' notebook to understand how this class works.\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_model: int, dropout_prob: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads.\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        # We use dropout to prevent overfitting.\n",
    "        self.dropout_layer = nn.Dropout(p=dropout_prob)\n",
    "        # Creating the linear layers that generate queries, keys and values for each token in the sequence.\n",
    "        # Also, creating an additional linear layer to generate the output of the Multi-Headed Attention from concatenated attention heads.\n",
    "        self.linear_layers = clone_module(module=nn.Linear(in_features=d_model, out_features=d_model), num_clones=4)\n",
    "\n",
    "\n",
    "    def forward(self, query_input: Tensor, key_input: Tensor, value_input: Tensor, mask: Optional[Tensor]=None) -> Tensor:\n",
    "        \"\"\"Forward pass of the Multi-Headed Attention layer. \n",
    "\n",
    "        Args:\n",
    "            query (Tensor): Input to be used for query creation.\n",
    "                            query_input: [batch_size, seq_len, d_model]\n",
    "            key (Tensor): Input to be used for key creation.\n",
    "                          key_input  : [batch_size, seq_len, d_model]\n",
    "            value (Tensor): Input to be used for value creation.\n",
    "                            value_input: [batch_size, seq_len, d_model]\n",
    "            mask (Tensor): Mask to be applied to the attention scores. Default is None. Same mask will \n",
    "                           be applied to all the heads in the Multi-Headed Attention layer.\n",
    "                           mask: [batch_size, 1, seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Mutli-Headed Attention Output: Output of the Multi-Headed Attention layer. Generates one output vector \n",
    "                                           for each token in the sequence. Does this for each sequence in the batch.\n",
    "                                           output: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Generates the queries, keys and values for each token in the sequence.\n",
    "        # shape of queries, keys, values: [batch_size, seq_len, d_model]\n",
    "        queries, keys, values = [linear_layer(input) for linear_layer, input in zip(self.linear_layers, (query_input, key_input, value_input))]\n",
    "        batch_size = query_input.shape[0]\n",
    "        seq_len = query_input.shape[1]\n",
    "        # Separating the queries, keys and values for each head into a separate vector. The vectors for each token in all the heads\n",
    "        # are concatenated when they are created using the linear_layers above.\n",
    "        # Shape for queries, keys, values after view: [batch_size, seq_len, num_heads, d_k]\n",
    "        # Shape for queries, key, values after transpose: [batch_size, num_heads, seq_len, d_k]\n",
    "        queries, keys, values = [data.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(dim0=1, dim1=2) for data in (queries, keys, values)]\n",
    "        # Calculate the attention heads for each token in the sequence.\n",
    "        # attention_heads: [batch_size, num_heads, seq_len, d_k]\n",
    "        attention_heads, attention_scores = construct_attention_heads(queries=queries, keys=keys, values=values, mask=mask, dropout_layer=self.dropout_layer)\n",
    "        # Concatenate the attention heads for each token from all the heads.\n",
    "        # attention_heads: [batch_size, seq_len, d_model]\n",
    "        attention_heads = attention_heads.transpose(dim0=1, dim1=2).reshape(batch_size, seq_len, self.d_model)\n",
    "        # Generate the output of the Multi-Headed Attention layer.\n",
    "        return self.linear_layers[-1](attention_heads)\n",
    "    \n",
    "\n",
    "# Refer to 'step_12_feed_forward_neural_network.ipynb' notebook to understand how this class works.\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, d_model: int, d_feed_forward: int, dropout_prob: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear_layer_1 = nn.Linear(in_features=d_model, out_features=d_feed_forward)\n",
    "        self.linear_layer_2 = nn.Linear(in_features=d_feed_forward, out_features=d_model)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"Passes the input through the Feed Forward Neural Network and returns the output \n",
    "           of the neural network.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): The output of the Multi-Headed Attention layer.\n",
    "                            shape: [batch_size, seq_len, d_model]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output of the Feed Forward Neural Network.\n",
    "                    shape: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # We first expand the input to higher dimension. We apply the ReLU activation function in this layer.\n",
    "        intermediate_output = self.linear_layer_1(input).relu()\n",
    "        # Dropout layer to prevent overfitting\n",
    "        intermediate_output = self.dropout_layer(intermediate_output)\n",
    "        # We then compress the input back to its original dimension. There is no specific intuitive explanation \n",
    "        # as to why this is done. It is just shown to be working practically in neural networks in general and \n",
    "        # in this paper in particular.\n",
    "        return self.linear_layer_2(intermediate_output)\n",
    "    \n",
    "\n",
    "# Refer to 'step_14_encoder.ipynb' to understand how this class works.\n",
    "class SubLayerWrapper(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout_prob: float):\n",
    "        \"\"\"This class is a wrapper around the MultiHeadedAttention and PositionwiseFeedForward classes.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimension of the vectors used in the Attention model.\n",
    "            dropout_prob (float): probability with which nodes can be dropped.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, input: Tensor, sublayer: nn.Module) -> Tensor:\n",
    "        \"\"\"It applies the operation on the input, applies dropout, adds the input back to the transformed \n",
    "           input, does normalization and returns the output.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Input to be transformer by the sublayer.\n",
    "                            input: [batch_size, seq_len, d_model]\n",
    "            sublayer (nn.Module): sublayer could be either MultiHeadedAttention or PositionwiseFeedForward.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: Output of the sublayer transformation.\n",
    "                    output: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        return self.layer_norm(input + self.dropout(sublayer(input)))\n",
    "\n",
    "\n",
    "# Refer to 'step_14_encoder.ipynb' to understand how this class works.\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, self_attention: MultiHeadedAttention, feed_forward: FeedForwardNN, d_model: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout_prob = dropout_prob\n",
    "        # These modules are now the child modules of the EncoderLayer and will be registered as parameters of the EncoderLayer.\n",
    "        self.self_attention = self_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer_wrappers = clone_module(module=SubLayerWrapper(d_model=self.d_model, dropout_prob=self.dropout_prob), num_clones=2)\n",
    "\n",
    "    def forward(self, input: Tensor, mask: Tensor) -> Tensor:\n",
    "        \"\"\"This method is the forward pass of the EncoderLayer class.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Source sentence provided as input to the EncoderLayer. These are the embeddings of the source \n",
    "                            sentence for the first EncoderLayer.\n",
    "                            SHAPE: [batch_size, seq_len, d_model]\n",
    "            mask (Tensor): Boolean mask to be applied to the input during attention scores calculation.\n",
    "                           SHAPE: [batch_size, 1, seq_len, seq_len]\n",
    "        Returns:\n",
    "            Tensor: Output of the EncoderLayer.\n",
    "                    SHAPE: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # We are just saving the function call to the self_attention method in a variable and passing the\n",
    "        # lambda function (contained within the variable) to the sublayer_wrappers[0] to execute it when \n",
    "        # needed.\n",
    "        output = self.sublayer_wrappers[0](input, lambda input: self.self_attention(query_input=input, key_input=input, value_input=input, mask=mask))\n",
    "        return self.sublayer_wrappers[1](output, self.feed_forward)\n",
    "\n",
    "\n",
    "# Refer to 'step_14_encoder.ipynb' to understand how this class works.\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_layer: EncoderLayer, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = clone_module(module=encoder_layer, num_clones=num_layers)\n",
    "        self.layer_norm = nn.LayerNorm(encoder_layer.d_model)\n",
    "\n",
    "    def forward(self, input: Tensor, mask: Optional[Tensor]=None) -> Tensor:\n",
    "        \"\"\"This method is the forward pass of the Encoder class. The output of the current EncoderLayer is\n",
    "           passed as input to the next EncoderLayer. We have 6 identical EncoderLayers stacked on top of \n",
    "           each other. The output of the last EncoderLayer is passed through a Layer Normalization layer\n",
    "           and returned as the final output of the Encoder\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Input to the Encoder i.e., embeddings of the tokenized src sequences.\n",
    "                            input: [batch_size, seq_len, d_model]\n",
    "            mask (Optional[Tensor], optional): Boolean mask to be applied during attention scores calculation.\n",
    "                                               mask: [batch_size, 1, seq_len, seq_len]. Defaults to None.\n",
    "                            \n",
    "        Returns:\n",
    "            Tensor: Output of the Encoder i.e., encoded src sentences.\n",
    "                    output: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        output = input\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            # Pass the output of the previous EncoderLayer to the current EncoderLayer.\n",
    "            output = encoder_layer(input=output, mask=mask)\n",
    "        return self.layer_norm(output)\n",
    "\n",
    "\n",
    "# Refer to 'step_15_decoder.ipynb' to understand how this class works.\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, self_attention: MultiHeadedAttention, src_attention: MultiHeadedAttention, feed_forward: FeedForwardNN, d_model: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout_prob = dropout_prob\n",
    "        # These modules are now the child modules of the DecoderLayer and will be registered as parameters of the DecoderLayer.\n",
    "        self.self_attention = self_attention\n",
    "        self.src_attention = src_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer_wrappers = clone_module(module=SubLayerWrapper(d_model=d_model, dropout_prob=dropout_prob), num_clones=3)\n",
    "\n",
    "    def forward(self, input: Tensor, encoded_src: Tensor, tgt_mask: Tensor, src_mask: Optional[Tensor]=None) -> Tensor:\n",
    "        \"\"\"This method is the forward pass of the DecoderLayer class.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Target sentence provided as input to the DecoderLayer. These are the embeddings of the target \n",
    "                            sentence for the first DecoderLayer.\n",
    "                            SHAPE: [batch_size, seq_len, d_model]\n",
    "            encoded_src (Tensor): Encoded source sentence. This is the output of the Encoder. This is used to calculate the\n",
    "                                  source attention scores for the target sentence. \n",
    "                                  SHAPE: [batch_size, seq_len, d_model] \n",
    "            tgt_mask (Tensor): Mask to prevent the future tokens in the target sentence to attend to the previous tokens and\n",
    "                               also to prevent padding tokens from attending to any other token except other padding tokens.\n",
    "                               SHAPE: [batch_size, 1, seq_len, seq_len]\n",
    "            src_mask (Optional[Tensor], optional): Mask to prevent the the padding tokens to attend to the tokens in the tgt sentence. \n",
    "                                                   Defaults to None.\n",
    "                                                   SHAPE: [batch_size, 1, seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Returns the output of the DecoderLayer. This is the output of the Positionwise FeedForward Neural Network.\n",
    "                    SHAPE: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # First sublayer: Self-Attention on the target sentence. Hence, it uses the tgt_mask.\n",
    "        self_attention_output = self.sublayer_wrappers[0](input=input, sublayer=lambda input: self.self_attention(query_input=input, key_input=input, value_input=input, mask=tgt_mask)) \n",
    "        # To give intuition about src_attention, I have a query for a token in the target sentence. I want to know whether \n",
    "        # some token in the source sentence is important for me to predict the output for the token in the target sentence. \n",
    "        # So, I go to the source sentence and get the values for all the tokens in the source sentence. I then calculate \n",
    "        # the attention scores between the query (in tgt) and the keys (in src). I then calculate the attention heads for \n",
    "        # the token in the target sentence using the attention scores. This is what is done in the below line. Note that \n",
    "        # referring to statement 'the keys and values are from the source' doesn't mean that you get keys and values \n",
    "        # explicitly. It means we use the encoded data from the source sentence to calculate the queries and keys for \n",
    "        # this transformation.\n",
    "        # Second sublayer: Attention on the source sentence. Hence, it uses the src_mask.\n",
    "        src_attention_output = self.sublayer_wrappers[1](input=self_attention_output, sublayer=lambda self_attention_output: self.src_attention(query_input=self_attention_output, key_input=encoded_src, value_input=encoded_src, mask=src_mask))\n",
    "        # Third sublayer: Positionwise FeedForward Neural Network\n",
    "        return self.sublayer_wrappers[2](input=src_attention_output, sublayer=self.feed_forward)\n",
    "\n",
    "\n",
    "# Refer to 'step_15_decoder.ipynb' to understand how this class works.\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_layer: DecoderLayer, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.decoder_layers = clone_module(module=decoder_layer, num_clones=num_layers)\n",
    "        self.layer_norm = nn.LayerNorm(decoder_layer.d_model)\n",
    "\n",
    "    def forward(self, input: Tensor, encoded_src: Tensor, tgt_mask: Tensor, src_mask: Optional[Tensor]=None) -> Tensor:\n",
    "        \"\"\"This method is the forward pass of the Decoder class. The output of the current DecoderLayer is\n",
    "           passed as input to the next DecoderLayer. We have 6 identical DecoderLayers stacked on top of \n",
    "           each other. The output of the Encoder (last EncoderLayer) is also passed as input to the \n",
    "           first DecoderLayer. The output of the last DecoderLayer is passed through a Layer Normalization \n",
    "           layer and returned as the final output of the Decoder.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Input to the Decoder i.e., embeddings of the tokenized tgt sequences.\n",
    "                            input: [batch_size, seq_len, d_model]\n",
    "            encoded_src (Tensor): output of the encoder i.e., encoded src sequences.\n",
    "            tgt_mask (Tensor): Boolean mask to be applied during self attention scores calculation.\n",
    "                               tgt_mask: [batch_size, 1, seq_len, seq_len].\n",
    "            src_mask (Optional[Tensor], optional): Boolean mask to be applied during src attention scores calculation.\n",
    "                                                   tgt_mask: [batch_size, 1, seq_len, seq_len]. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output of the Decoder.\n",
    "                    output: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        output = input\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            # Pass the output of the previous DecoderLayer to the current DecoderLayer.\n",
    "            output = decoder_layer(input=output, encoded_src=encoded_src, tgt_mask=tgt_mask, src_mask=src_mask)\n",
    "        return self.layer_norm(output)\n",
    "\n",
    "\n",
    "# Refer to 'step_14_token_predictor.ipynb' to understand how this class works.\n",
    "class TokenPredictor(nn.Module):\n",
    "    def __init__(self, d_model: int, tgt_vocab_size: int):\n",
    "        super(TokenPredictor, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = tgt_vocab_size\n",
    "        self.linear = nn.Linear(in_features=d_model, out_features=tgt_vocab_size)\n",
    "        # The non-module variables are not added to the list of parameters of the model.\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, decoder_output: Tensor) -> Tensor:\n",
    "        \"\"\"The forward pass of the output generator. Calculates the probability distribution over the \n",
    "           vocabulary. Each token vector has a corresponding probability distribution over the \n",
    "           vocabulary since we predict one token per output.\n",
    "\n",
    "        Args:\n",
    "            decoder_output (Tensor): Output of the Decoder.\n",
    "                                     shape: [batch_size, seq_len, d_model]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Log probability distribution over the vocabulary. \n",
    "                    shape: [batch_size, seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        # Project the decoder output to the vocab_size dimensional space.\n",
    "        logits = self.linear(decoder_output)\n",
    "        # Convert the logits to a probability distribution over the vocabulary. All the entires in the\n",
    "        # output tensor are negative since we are using log softmax. The log softmax is used to make\n",
    "        # the training more numerically stable. However, the maximum value is still the same as the \n",
    "        # maximum value of the original softmax output.\n",
    "        return self.log_softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer model which is the main learning part of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now create the Transformer model by logically combining all the components we have created so far.\n",
    "class MachineTranslationModel(nn.Module):\n",
    "    \"\"\"Model that combines the Encoder, Decoder and the TokenPredictor to create a machine translation Transformer model.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_feed_forward: int, dropout_prob: float, num_heads: int, src_vocab_size: int, tgt_vocab_size: int, num_layers: int, max_seq_len: int):\n",
    "        \"\"\"Initializes the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): size of the embedding vectors in the model.\n",
    "            d_feed_forward (int): Number of neurons in the hidden layer of the feed forward neural network.\n",
    "            dropout_prob (float): probability with which to drop data in the transformer model.\n",
    "            num_heads (int): number of attention heads in each of the multi-head attention layers in the model.\n",
    "            src_vocab_size (int): size of the source vocabulary.\n",
    "            tgt_vocab_size (int): size of the target vocabulary.\n",
    "            num_layers (int): number of layers in the Encoder and Decoder.\n",
    "            max_seq_len (int): Maximum length of the sequence that is ever input to the model.\n",
    "        \"\"\"\n",
    "        super(MachineTranslationModel, self).__init__()\n",
    "        self.src_embedding = Embeddings(vocab_size=src_vocab_size, embedding_dim=d_model)\n",
    "        self.tgt_embedding = Embeddings(vocab_size=tgt_vocab_size, embedding_dim=d_model)\n",
    "        self.positional_encoding = PositionalEncoding(encoding_size=d_model, dropout_prob=dropout_prob, max_len=max_seq_len)\n",
    "        multi_headed_attention = MultiHeadedAttention(num_heads=num_heads, d_model=d_model, dropout_prob=dropout_prob)\n",
    "        feed_forward_nn = FeedForwardNN(d_model=d_model, d_feed_forward=d_feed_forward, dropout_prob=dropout_prob)\n",
    "        encoder_layer = EncoderLayer(self_attention=copy.deepcopy(multi_headed_attention), \n",
    "                                     feed_forward=copy.deepcopy(feed_forward_nn), \n",
    "                                     d_model=d_model, \n",
    "                                     dropout_prob=dropout_prob)\n",
    "        decoder_layer = DecoderLayer(self_attention=copy.deepcopy(multi_headed_attention), \n",
    "                                     src_attention=copy.deepcopy(multi_headed_attention),\n",
    "                                     feed_forward=copy.deepcopy(feed_forward_nn), \n",
    "                                     d_model=d_model, \n",
    "                                     dropout_prob=dropout_prob)\n",
    "        self.encoder = Encoder(encoder_layer=encoder_layer, num_layers=num_layers)\n",
    "        self.decoder = Decoder(decoder_layer=decoder_layer, num_layers=num_layers)\n",
    "        self.token_predictor = TokenPredictor(d_model=d_model, tgt_vocab_size=tgt_vocab_size)\n",
    "        self.initialize_model_parameters()\n",
    "\n",
    "    def initialize_model_parameters(self):\n",
    "        \"\"\"Initializes the parameters of the model using the Xavier Uniform initialization.\"\"\"\n",
    "        for params in self.parameters():\n",
    "            # This is to ensure the only the weights are initialized and not the biases. biases usually have only\n",
    "            # one dimension and the weights have more than one dimension.\n",
    "            if params.dim() > 1:\n",
    "                nn.init.xavier_uniform_(params)\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor, tgt_mask: Tensor) -> Tensor:\n",
    "        \"\"\"The forward pass of the Transformer model. The source sentences are passed through the Encoder and the target\n",
    "           sentences are passed through the Decoder. The output of the Decoder is passed through the token predictor to\n",
    "           get the probability distribution over the target vocabulary.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Source sentences (English) containing the token ids corresponding to the indices in the src vocabulary. \n",
    "                          Example input looks like [[0, 4, 55, 67, 1, 2, 2], [0, 42, 585, 967, 19, 26, 1]]\n",
    "                          SHAPE: [batch_size, seq_len]\n",
    "            tgt (Tensor): Target sentences (Telugu) containing the token ids corresponding to the indices in the tgt vocabulary. \n",
    "                          Example input looks like [[0, 3, 5, 677, 81, 1, 2], [0, 7, 67, 190, 3245, 1]]\n",
    "                          SHAPE: [batch_size, seq_len - 1]\n",
    "            src_mask (Tensor): Mask to be applied to the source sentences in each of the attention heads.\n",
    "                               src_mask: [batch_size, 1, seq_len, seq_len]\n",
    "            tgt_mask (Tensor): Mask to be applied to the target sentences in each of the attention heads.\n",
    "                               tgt_mask: [batch_size, 1, seq_len - 1, seq_len - 1]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Log probability distribution over the tokens in the target vocabulary (Telugu vocabulary).\n",
    "                    SHAPE: [batch_size, seq_len - 1, tgt_vocab_size]\n",
    "        \"\"\"\n",
    "        # Pass the source sentences through the encoder to get the encoded source token vectors.\n",
    "        encoded_src = self.encode(src=src, src_mask=src_mask)\n",
    "        # Pass the target sentence through the decoder to get the encoded target token vectors.\n",
    "        decoded_tgt = self.decode(tgt=tgt, tgt_mask=tgt_mask, encoded_src=encoded_src, src_mask=src_mask)\n",
    "        return self.generate_tgt_token_prob_distributions(decoded_tgt=decoded_tgt)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"Encodes the source sentences (English).\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): A batch of source sentences containing the token ids corresponding to the indices in the src vocabulary.\n",
    "                          SHAPE: [batch_size, seq_len]\n",
    "            src_mask (Tensor): Mask to be applied to the source sentences in each of the attention heads. Same mask will be \n",
    "                               applied to the sentence in all the attention heads.\n",
    "                               SHAPE: [batch_size, 1, seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Encoded source sentences. Each token in the source sentence is represented by a vector that encodes\n",
    "                    all the information about the token and its relationship with other tokens in the sentence.\n",
    "                    SHAPE: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Get the embeddings for the source sentences.\n",
    "        src_embeddings = self.src_embedding(src)\n",
    "        # Add the positional encodings to the embeddings.\n",
    "        src_embeddings = self.positional_encoding(src_embeddings)\n",
    "        # Pass the source sentence through the encoder.\n",
    "        encoded_src = self.encoder(input=src_embeddings, mask=src_mask)\n",
    "        return encoded_src\n",
    "\n",
    "    def decode(self, tgt: Tensor, tgt_mask: Tensor, encoded_src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"Encodes the target sentences (Telugu).\n",
    "\n",
    "        Args:\n",
    "            tgt (Tensor): A batch of target sentences containing the token ids corresponding to the indices in the tgt vocabulary.\n",
    "                          SHAPE: [batch_size, seq_len - 1]\n",
    "            tgt_mask (Tensor): Mask to be applied to the target sentences in each of the attention heads. Same mask will be \n",
    "                               applied to the sentence in all the attention heads.\n",
    "                               SHAPE: [batch_size, 1, seq_len - 1, seq_len - 1]\n",
    "            encoded_src (Tensor): The encoded token representations of the source sentences. This is used to calculate the\n",
    "                                  source attention scores for the target sentence.\n",
    "                                  SHAPE: [batch_size, seq_len, d_model]\n",
    "            src_mask (Tensor): Mask to be applied to the source sentences in each of the attention heads. Same mask will be \n",
    "                               applied to the sentence in all the attention heads.\n",
    "                               SHAPE: [batch_size, 1, seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Encoded (or Decoded if that makes more sense to you) target sentences. Each token in the target \n",
    "                    sentence is represented by a vector that encodes all the information about the token and its \n",
    "                    relationship with other tokens in the target sentence and the corresponding source sentence.\n",
    "        \"\"\"\n",
    "        # Get the embeddings for the target sentences.\n",
    "        tgt_embeddings = self.tgt_embedding(tgt)\n",
    "        # Add the positional encodings to the embeddings.\n",
    "        tgt_embeddings = self.positional_encoding(tgt_embeddings)\n",
    "        # Pass the target sentence through the decoder.\n",
    "        decoded_tgt = self.decoder(input=tgt_embeddings, encoded_src=encoded_src, tgt_mask=tgt_mask, src_mask=src_mask)\n",
    "        return decoded_tgt\n",
    "\n",
    "    def generate_tgt_token_prob_distributions(self, decoded_tgt: Tensor) -> Tensor:\n",
    "        # Convert the output of the decoder to the probability distribution over the target vocabulary. This will be\n",
    "        # used to calculate the loss in the training phase.\n",
    "        return self.token_predictor(decoded_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  src\n",
      "shape:  torch.Size([2, 4])\n",
      "tensor([[5, 1, 4, 0],\n",
      "        [0, 0, 0, 0]])\n",
      "----------------------------------------------------------------------------------------\n",
      "name:  src_mask\n",
      "shape:  torch.Size([2, 1, 4, 4])\n",
      "tensor([[[[False, False, False, False],\n",
      "          [False, False, False,  True],\n",
      "          [False,  True,  True, False],\n",
      "          [False,  True,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[False, False, False, False],\n",
      "          [False,  True,  True,  True],\n",
      "          [ True, False, False,  True],\n",
      "          [False, False, False, False]]]])\n",
      "----------------------------------------------------------------------------------------\n",
      "name:  tgt\n",
      "shape:  torch.Size([2, 4])\n",
      "tensor([[3, 5, 0, 1],\n",
      "        [0, 5, 4, 0]])\n",
      "----------------------------------------------------------------------------------------\n",
      "name:  tgt_mask\n",
      "shape:  torch.Size([2, 1, 4, 4])\n",
      "tensor([[[[ True, False, False, False],\n",
      "          [False, False, False,  True],\n",
      "          [ True, False, False, False],\n",
      "          [ True,  True, False,  True]]],\n",
      "\n",
      "\n",
      "        [[[False,  True,  True, False],\n",
      "          [ True,  True, False,  True],\n",
      "          [ True,  True, False, False],\n",
      "          [False,  True, False,  True]]]])\n",
      "----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL CONTAINS SOME HELPER FUNCTIONS TO GENERATE RANDOM DATA FOR TESTING THE MODEL.\n",
    "\n",
    "# Generates the (src, tgt) data and prints them for visibility.\n",
    "def PrintTensor(input: Tensor, name: str):\n",
    "    print(\"name: \", name)\n",
    "    print(\"shape: \", input.shape)\n",
    "    print(input)\n",
    "    print(\"-\" * 150)\n",
    "\n",
    "# The true data will have a specific format that is expected by the model. That part is not considered\n",
    "# in this random generation. The random generation is just to test the model and not to train it.\n",
    "def generate_batch_of_input_data(batch_size: int, seq_len: int, vocab_size: int) -> Tensor:\n",
    "    return torch.randint(low=0, high=vocab_size, size=(batch_size, seq_len))\n",
    "\n",
    "# The true data will have masks that are corelated with the data. That part is not considered in this\n",
    "# random generation. The random generation of masks is just to test the model and not to train it.\n",
    "def construct_random_mask(batch_size: int, seq_len: int) -> Tensor:\n",
    "    # If some index is set to False, then it will be masked out.\n",
    "    mask = torch.randn(size=(batch_size, 1, seq_len, seq_len)) > 0.5\n",
    "    return mask.bool()\n",
    "\n",
    "src = generate_batch_of_input_data(batch_size=batch_size, seq_len=seq_len, vocab_size=src_vocab_size)\n",
    "PrintTensor(input=src, name=\"src\")\n",
    "src_mask = construct_random_mask(batch_size=batch_size, seq_len=seq_len)\n",
    "PrintTensor(input=src_mask, name=\"src_mask\")\n",
    "tgt = generate_batch_of_input_data(batch_size=batch_size, seq_len=seq_len, vocab_size=tgt_vocab_size)\n",
    "PrintTensor(input=tgt, name=\"tgt\")\n",
    "tgt_mask = construct_random_mask(batch_size=batch_size, seq_len=seq_len)\n",
    "PrintTensor(input=tgt_mask, name=\"tgt_mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MachineTranslationTransformer(\n",
      "  (src_embedding): Embeddings(\n",
      "    (look_up_table): Embedding(6, 8)\n",
      "  )\n",
      "  (tgt_embedding): Embeddings(\n",
      "    (look_up_table): Embedding(6, 8)\n",
      "  )\n",
      "  (positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0-5): 6 x EncoderLayer(\n",
      "        (self_attention): MultiHeadedAttention(\n",
      "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "          (linear_layers): ModuleList(\n",
      "            (0-3): 4 x Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (feed_forward): FeedForwardNN(\n",
      "          (linear_layer_1): Linear(in_features=8, out_features=16, bias=True)\n",
      "          (linear_layer_2): Linear(in_features=16, out_features=8, bias=True)\n",
      "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (sublayer_wrappers): ModuleList(\n",
      "          (0-1): 2 x SubLayerWrapper(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (decoder_layers): ModuleList(\n",
      "      (0-5): 6 x DecoderLayer(\n",
      "        (self_attention): MultiHeadedAttention(\n",
      "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "          (linear_layers): ModuleList(\n",
      "            (0-3): 4 x Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (src_attention): MultiHeadedAttention(\n",
      "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "          (linear_layers): ModuleList(\n",
      "            (0-3): 4 x Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (feed_forward): FeedForwardNN(\n",
      "          (linear_layer_1): Linear(in_features=8, out_features=16, bias=True)\n",
      "          (linear_layer_2): Linear(in_features=16, out_features=8, bias=True)\n",
      "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (sublayer_wrappers): ModuleList(\n",
      "          (0-2): 3 x SubLayerWrapper(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (output_generator): OutputGenerator(\n",
      "    (linear): Linear(in_features=8, out_features=6, bias=True)\n",
      "    (log_softmax): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the MachineTranslationTransformer model.\n",
    "machine_translation_transformer = MachineTranslationModel(d_model=d_model, \n",
    "                                                          d_feed_forward=d_feed_forward,\n",
    "                                                          dropout_prob=dropout_prob, \n",
    "                                                          num_heads=num_heads, \n",
    "                                                          src_vocab_size=src_vocab_size, \n",
    "                                                          tgt_vocab_size=tgt_vocab_size, \n",
    "                                                          num_layers=num_layers,\n",
    "                                                          max_seq_len=max_seq_len)\n",
    "print(machine_translation_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  decoded_tgt\n",
      "shape:  torch.Size([2, 4, 6])\n",
      "tensor([[[-1.9138, -0.8584, -2.6599, -2.8548, -1.2727, -3.8601],\n",
      "         [-1.4200, -1.2505, -4.1406, -4.0404, -0.8449, -4.7314],\n",
      "         [-1.0043, -1.0392, -2.4634, -3.2804, -2.0178, -3.7191],\n",
      "         [-1.1923, -0.9592, -2.4992, -3.4596, -1.8626, -3.1144]],\n",
      "\n",
      "        [[-2.7122, -0.6663, -1.8501, -2.1413, -2.4697, -2.8027],\n",
      "         [-3.3400, -1.3180, -3.3497, -2.2045, -0.6202, -4.2917],\n",
      "         [-3.4366, -1.2331, -3.0217, -1.9720, -0.7687, -3.6909],\n",
      "         [-2.7717, -1.1688, -3.5477, -2.7665, -0.6677, -3.8104]]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Pass the input data through the model to get the output.\n",
    "decoded_tgt = machine_translation_transformer(src=src, tgt=tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "PrintTensor(input=decoded_tgt, name=\"decoded_tgt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets verify if the number of parameters in the model is the same as what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_params:  9206\n",
      "total_params_with_grad:  9206\n",
      "total_params_without_grad:  0\n"
     ]
    }
   ],
   "source": [
    "# Finding out the number of parameters in the build model.\n",
    "total_params = sum(params.numel() for params in machine_translation_transformer.parameters())\n",
    "print(\"total_params: \", total_params)\n",
    "total_params_with_grad = sum(params.numel() for params in machine_translation_transformer.parameters() if params.requires_grad)\n",
    "print(\"total_params_with_grad: \", total_params_with_grad)\n",
    "total_params_without_grad = sum(params.numel() for params in machine_translation_transformer.parameters() if not params.requires_grad)\n",
    "print(\"total_params_without_grad: \", total_params_without_grad)\n",
    "assert total_params == total_params_with_grad + total_params_without_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_embedding.look_up_table.weight   48\n",
      "tgt_embedding.look_up_table.weight   48\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.0.weight   64\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.0.bias   8\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.1.weight   64\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.1.bias   8\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.2.weight   64\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.2.bias   8\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.3.weight   64\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.3.bias   8\n",
      "encoder.encoder_layers.0.feed_forward.linear_layer_1.weight   128\n",
      "encoder.encoder_layers.0.feed_forward.linear_layer_1.bias   16\n",
      "encoder.encoder_layers.0.feed_forward.linear_layer_2.weight   128\n",
      "encoder.encoder_layers.0.feed_forward.linear_layer_2.bias   8\n",
      "encoder.encoder_layers.0.sublayer_wrappers.0.layer_norm.weight   8\n",
      "encoder.encoder_layers.0.sublayer_wrappers.0.layer_norm.bias   8\n",
      "encoder.encoder_layers.0.sublayer_wrappers.1.layer_norm.weight   8\n",
      "encoder.encoder_layers.0.sublayer_wrappers.1.layer_norm.bias   8\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.0.weight   64\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.0.bias   8\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.1.weight   64\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.1.bias   8\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.2.weight   64\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.2.bias   8\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.3.weight   64\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.3.bias   8\n",
      "encoder.encoder_layers.1.feed_forward.linear_layer_1.weight   128\n",
      "encoder.encoder_layers.1.feed_forward.linear_layer_1.bias   16\n",
      "encoder.encoder_layers.1.feed_forward.linear_layer_2.weight   128\n",
      "encoder.encoder_layers.1.feed_forward.linear_layer_2.bias   8\n",
      "encoder.encoder_layers.1.sublayer_wrappers.0.layer_norm.weight   8\n",
      "encoder.encoder_layers.1.sublayer_wrappers.0.layer_norm.bias   8\n",
      "encoder.encoder_layers.1.sublayer_wrappers.1.layer_norm.weight   8\n",
      "encoder.encoder_layers.1.sublayer_wrappers.1.layer_norm.bias   8\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.0.weight   64\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.0.bias   8\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.1.weight   64\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.1.bias   8\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.2.weight   64\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.2.bias   8\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.3.weight   64\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.3.bias   8\n",
      "encoder.encoder_layers.2.feed_forward.linear_layer_1.weight   128\n",
      "encoder.encoder_layers.2.feed_forward.linear_layer_1.bias   16\n",
      "encoder.encoder_layers.2.feed_forward.linear_layer_2.weight   128\n",
      "encoder.encoder_layers.2.feed_forward.linear_layer_2.bias   8\n",
      "encoder.encoder_layers.2.sublayer_wrappers.0.layer_norm.weight   8\n",
      "encoder.encoder_layers.2.sublayer_wrappers.0.layer_norm.bias   8\n",
      "encoder.encoder_layers.2.sublayer_wrappers.1.layer_norm.weight   8\n",
      "encoder.encoder_layers.2.sublayer_wrappers.1.layer_norm.bias   8\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.0.weight   64\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.0.bias   8\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.1.weight   64\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.1.bias   8\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.2.weight   64\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.2.bias   8\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.3.weight   64\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.3.bias   8\n",
      "encoder.encoder_layers.3.feed_forward.linear_layer_1.weight   128\n",
      "encoder.encoder_layers.3.feed_forward.linear_layer_1.bias   16\n",
      "encoder.encoder_layers.3.feed_forward.linear_layer_2.weight   128\n",
      "encoder.encoder_layers.3.feed_forward.linear_layer_2.bias   8\n",
      "encoder.encoder_layers.3.sublayer_wrappers.0.layer_norm.weight   8\n",
      "encoder.encoder_layers.3.sublayer_wrappers.0.layer_norm.bias   8\n",
      "encoder.encoder_layers.3.sublayer_wrappers.1.layer_norm.weight   8\n",
      "encoder.encoder_layers.3.sublayer_wrappers.1.layer_norm.bias   8\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.0.weight   64\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.0.bias   8\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.1.weight   64\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.1.bias   8\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.2.weight   64\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.2.bias   8\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.3.weight   64\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.3.bias   8\n",
      "encoder.encoder_layers.4.feed_forward.linear_layer_1.weight   128\n",
      "encoder.encoder_layers.4.feed_forward.linear_layer_1.bias   16\n",
      "encoder.encoder_layers.4.feed_forward.linear_layer_2.weight   128\n",
      "encoder.encoder_layers.4.feed_forward.linear_layer_2.bias   8\n",
      "encoder.encoder_layers.4.sublayer_wrappers.0.layer_norm.weight   8\n",
      "encoder.encoder_layers.4.sublayer_wrappers.0.layer_norm.bias   8\n",
      "encoder.encoder_layers.4.sublayer_wrappers.1.layer_norm.weight   8\n",
      "encoder.encoder_layers.4.sublayer_wrappers.1.layer_norm.bias   8\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.0.weight   64\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.0.bias   8\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.1.weight   64\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.1.bias   8\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.2.weight   64\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.2.bias   8\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.3.weight   64\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.3.bias   8\n",
      "encoder.encoder_layers.5.feed_forward.linear_layer_1.weight   128\n",
      "encoder.encoder_layers.5.feed_forward.linear_layer_1.bias   16\n",
      "encoder.encoder_layers.5.feed_forward.linear_layer_2.weight   128\n",
      "encoder.encoder_layers.5.feed_forward.linear_layer_2.bias   8\n",
      "encoder.encoder_layers.5.sublayer_wrappers.0.layer_norm.weight   8\n",
      "encoder.encoder_layers.5.sublayer_wrappers.0.layer_norm.bias   8\n",
      "encoder.encoder_layers.5.sublayer_wrappers.1.layer_norm.weight   8\n",
      "encoder.encoder_layers.5.sublayer_wrappers.1.layer_norm.bias   8\n",
      "encoder.layer_norm.weight   8\n",
      "encoder.layer_norm.bias   8\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.0.feed_forward.linear_layer_1.weight   128\n",
      "decoder.decoder_layers.0.feed_forward.linear_layer_1.bias   16\n",
      "decoder.decoder_layers.0.feed_forward.linear_layer_2.weight   128\n",
      "decoder.decoder_layers.0.feed_forward.linear_layer_2.bias   8\n",
      "decoder.decoder_layers.0.sublayer_wrappers.0.layer_norm.weight   8\n",
      "decoder.decoder_layers.0.sublayer_wrappers.0.layer_norm.bias   8\n",
      "decoder.decoder_layers.0.sublayer_wrappers.1.layer_norm.weight   8\n",
      "decoder.decoder_layers.0.sublayer_wrappers.1.layer_norm.bias   8\n",
      "decoder.decoder_layers.0.sublayer_wrappers.2.layer_norm.weight   8\n",
      "decoder.decoder_layers.0.sublayer_wrappers.2.layer_norm.bias   8\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.1.feed_forward.linear_layer_1.weight   128\n",
      "decoder.decoder_layers.1.feed_forward.linear_layer_1.bias   16\n",
      "decoder.decoder_layers.1.feed_forward.linear_layer_2.weight   128\n",
      "decoder.decoder_layers.1.feed_forward.linear_layer_2.bias   8\n",
      "decoder.decoder_layers.1.sublayer_wrappers.0.layer_norm.weight   8\n",
      "decoder.decoder_layers.1.sublayer_wrappers.0.layer_norm.bias   8\n",
      "decoder.decoder_layers.1.sublayer_wrappers.1.layer_norm.weight   8\n",
      "decoder.decoder_layers.1.sublayer_wrappers.1.layer_norm.bias   8\n",
      "decoder.decoder_layers.1.sublayer_wrappers.2.layer_norm.weight   8\n",
      "decoder.decoder_layers.1.sublayer_wrappers.2.layer_norm.bias   8\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.2.feed_forward.linear_layer_1.weight   128\n",
      "decoder.decoder_layers.2.feed_forward.linear_layer_1.bias   16\n",
      "decoder.decoder_layers.2.feed_forward.linear_layer_2.weight   128\n",
      "decoder.decoder_layers.2.feed_forward.linear_layer_2.bias   8\n",
      "decoder.decoder_layers.2.sublayer_wrappers.0.layer_norm.weight   8\n",
      "decoder.decoder_layers.2.sublayer_wrappers.0.layer_norm.bias   8\n",
      "decoder.decoder_layers.2.sublayer_wrappers.1.layer_norm.weight   8\n",
      "decoder.decoder_layers.2.sublayer_wrappers.1.layer_norm.bias   8\n",
      "decoder.decoder_layers.2.sublayer_wrappers.2.layer_norm.weight   8\n",
      "decoder.decoder_layers.2.sublayer_wrappers.2.layer_norm.bias   8\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.3.feed_forward.linear_layer_1.weight   128\n",
      "decoder.decoder_layers.3.feed_forward.linear_layer_1.bias   16\n",
      "decoder.decoder_layers.3.feed_forward.linear_layer_2.weight   128\n",
      "decoder.decoder_layers.3.feed_forward.linear_layer_2.bias   8\n",
      "decoder.decoder_layers.3.sublayer_wrappers.0.layer_norm.weight   8\n",
      "decoder.decoder_layers.3.sublayer_wrappers.0.layer_norm.bias   8\n",
      "decoder.decoder_layers.3.sublayer_wrappers.1.layer_norm.weight   8\n",
      "decoder.decoder_layers.3.sublayer_wrappers.1.layer_norm.bias   8\n",
      "decoder.decoder_layers.3.sublayer_wrappers.2.layer_norm.weight   8\n",
      "decoder.decoder_layers.3.sublayer_wrappers.2.layer_norm.bias   8\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.4.feed_forward.linear_layer_1.weight   128\n",
      "decoder.decoder_layers.4.feed_forward.linear_layer_1.bias   16\n",
      "decoder.decoder_layers.4.feed_forward.linear_layer_2.weight   128\n",
      "decoder.decoder_layers.4.feed_forward.linear_layer_2.bias   8\n",
      "decoder.decoder_layers.4.sublayer_wrappers.0.layer_norm.weight   8\n",
      "decoder.decoder_layers.4.sublayer_wrappers.0.layer_norm.bias   8\n",
      "decoder.decoder_layers.4.sublayer_wrappers.1.layer_norm.weight   8\n",
      "decoder.decoder_layers.4.sublayer_wrappers.1.layer_norm.bias   8\n",
      "decoder.decoder_layers.4.sublayer_wrappers.2.layer_norm.weight   8\n",
      "decoder.decoder_layers.4.sublayer_wrappers.2.layer_norm.bias   8\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.5.feed_forward.linear_layer_1.weight   128\n",
      "decoder.decoder_layers.5.feed_forward.linear_layer_1.bias   16\n",
      "decoder.decoder_layers.5.feed_forward.linear_layer_2.weight   128\n",
      "decoder.decoder_layers.5.feed_forward.linear_layer_2.bias   8\n",
      "decoder.decoder_layers.5.sublayer_wrappers.0.layer_norm.weight   8\n",
      "decoder.decoder_layers.5.sublayer_wrappers.0.layer_norm.bias   8\n",
      "decoder.decoder_layers.5.sublayer_wrappers.1.layer_norm.weight   8\n",
      "decoder.decoder_layers.5.sublayer_wrappers.1.layer_norm.bias   8\n",
      "decoder.decoder_layers.5.sublayer_wrappers.2.layer_norm.weight   8\n",
      "decoder.decoder_layers.5.sublayer_wrappers.2.layer_norm.bias   8\n",
      "decoder.layer_norm.weight   8\n",
      "decoder.layer_norm.bias   8\n",
      "output_generator.linear.weight   48\n",
      "output_generator.linear.bias   6\n"
     ]
    }
   ],
   "source": [
    "# Prints out all the layers and the number of parameters in each layer.\n",
    "for name, params in machine_translation_transformer.named_parameters():\n",
    "    print(name, \" \", params.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the number of trainable parameters in the transformer model manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of parameters associated with the model:  9206\n"
     ]
    }
   ],
   "source": [
    "# Lets try to count the number of parameters in the model manually by going through each component and counting \n",
    "# parameters. \n",
    "# The number of parameters associated with the Embeddings class for src sentences. We have 1 embedding vector \n",
    "# per token in the source vocabulary. We have 6 tokens and each token is represented by an 8-dimensional vector. \n",
    "# So, the total number of parameters associated with the Embeddings class for src sentences is 6 * 8 = 48.\n",
    "num_src_embedding_params = src_vocab_size * d_model\n",
    "# The number of parameters associated with the Embeddings class for tgt sentences. We have 1 embedding vector\n",
    "# per token in the target vocabulary. We have 6 tokens and each token is represented by an 8-dimensional vector.\n",
    "# So, the total number of parameters associated with the Embeddings class for tgt sentences is 6 * 8 = 48.\n",
    "num_tgt_embedding_params = tgt_vocab_size * d_model\n",
    "# There are no parameters associated with the PositionalEncoding class. These are calculated based on a predefined\n",
    "# formula and are not learned during the training process.\n",
    "num_positional_encoding_params = 0\n",
    "# Now, lets calculate the number of parameters associated with the Encoder class. The Encoder class has 6 \n",
    "# identical EncoderLayers stacked on top of each other. Lets calculate the number of parameters associated with \n",
    "# each EncoderLayer class. The EncoderLayer class has MultiHeadedAttention and FeedForwardNN classes as its child \n",
    "# classes. Each MultiHeadedAttention class has 4 linear layers (query, key, value and output). Note that a single \n",
    "# linear layer is used to calculate the queries, keys, values and outputs for all the heads. So, we don't need to \n",
    "# do this calculation for each head separately. Lets take the linear layer associated with the query calculation. \n",
    "# The input to this linear layer is a 8-dimensional vector (d_model) and the output is also an 8-dimensional vector \n",
    "# (d_model=8). So, the number of parameters in this linear layer associated with the weight matrix is 8 * 8 = 64. \n",
    "# We also have bias terms (d_model=8) associated with this linear layer. So, the total number of parameters \n",
    "# associated with the query linear layer is 64 + 8 = 72.\n",
    "num_encoder_query_params = d_model * d_model + d_model\n",
    "num_encoder_key_params = d_model * d_model + d_model\n",
    "num_encoder_value_params = d_model * d_model + d_model\n",
    "num_encoder_attention_output_params = d_model * d_model + d_model\n",
    "# Now lets calculate the number of parameters associated with FeedForward neural network class in the EncoderLayer. \n",
    "# The first linear layer in the feed forward expands the input to a higher dimension (d_model to d_feed_forward). \n",
    "# The input to this linear layer is a 8-dimensional vector (d_model) and the output is a 16-dimensional vector \n",
    "# (d_feed_forward). So, the number of parameters in this linear layer associated with the weight matrix is \n",
    "# 8 * 16 = 128. We also have bias terms (d_feed_forward=16) associated with this linear layer. So, the total number \n",
    "# of parameters associated with the first linear layer in the feed forward neural network is 128 + 16 = 144. The \n",
    "# second linear layer in the feed forward neural network compresses the input back to its original dimension \n",
    "# (d_feed_forward to d_model). The input to this linear layer is a 16-dimensional vector (d_feed_forward) and the \n",
    "# output is an 8-dimensional vector (d_model). So, the number of parameters in this linear layer associated with \n",
    "# the weight matrix is 16 * 8 = 128. We also have bias terms (d_model=8) associated with this linear layer. So, \n",
    "# the total number of parameters associated with the second linear layer in the feed forward neural network is \n",
    "# 128 + 8 = 136.\n",
    "num_encoder_feed_forward_linear_layer_1_params = d_model * d_feed_forward + d_feed_forward\n",
    "num_encoder_feed_forward_linear_layer_2_params = d_feed_forward * d_model + d_model\n",
    "# The output of MultiHeadedAttention and FeedForward neural network is normalized using Layer Normalization. Layer\n",
    "# Normalization is applied along the last dimension of the input tensor (input to Layer Normalization). Each of the\n",
    "# features is scaled independently with the learned paramaters. So, the number of parameters is the number of \n",
    "# features in the last dimension multiplied by 2 (1 parameter for gamma and 1 parameter for beta per feature). Both \n",
    "# the output of MultiHeadedAttention and FeedForward neural network have the same size in the last dimension which \n",
    "# is 8 (d_model). So, the number of parameters associated with Layer Normalization layer that is applied after \n",
    "# MultiHeadedAttention is 8 (gamma) + 8 (beta) = 16. Similarly, the number of parameters associated with Layer\n",
    "# Normalization layer that is applied after FeedForward neural network is 8 (gamma) + 8 (beta) = 16.\n",
    "num_encoder_attention_layer_norm_params = d_model + d_model\n",
    "num_encoder_feed_forward_layer_norm_params = d_model + d_model\n",
    "# The total number of parameters associated with a single EncoderLayer is sum of the above 8 variables = 600.\n",
    "num_encoder_layer_params = num_encoder_query_params + num_encoder_key_params + num_encoder_value_params + num_encoder_attention_output_params + num_encoder_feed_forward_linear_layer_1_params + num_encoder_feed_forward_linear_layer_2_params + num_encoder_attention_layer_norm_params + num_encoder_feed_forward_layer_norm_params\n",
    "# We also apply Layer Normalization to the output of the last EncoderLayer and pass this as the output of the Encoder.\n",
    "# This layer again has same number of parameters associated with it as other Layer Normalization layers i.e.,\n",
    "# 8 (gamma) + 8 (beta) = 16\n",
    "num_encoder_layer_layer_norm_params = d_model + d_model\n",
    "# The transformer model has 6 EncoderLayers stacked on top of each other. So, the total number of parameters \n",
    "# associated with Encoder is (6 * 600) + 16 = 3616\n",
    "num_total_encoder_params = (num_layers * num_encoder_layer_params) + num_encoder_layer_layer_norm_params\n",
    "# Now, lets calculate the number of parameters associated with the DecoderLayer and Decoder.\n",
    "# The method to calculate number of parameters in the DecoderLayer is very similar to how it was done for the \n",
    "# EncoderLayer. DecoderLayer just contains 1 additional MultiHeadedAttention Layer (for source attention) and 1\n",
    "# additional Layer Normalization layer associated with this source attention layer.\n",
    "# Same as in EncoderLayer ==> Linear Layer parameters ==> 64 (weights) + 8 (bias) = 72\n",
    "num_decoder_self_attention_query_params = d_model * d_model + d_model\n",
    "num_decoder_self_attention_key_params = d_model * d_model + d_model\n",
    "num_decoder_self_attention_value_params = d_model * d_model + d_model\n",
    "num_decoder_self_attention_output_params = d_model * d_model + d_model\n",
    "# The next 4 variables correspond to the 1 additional MultiHeadedAttention layer (src attention) present in the \n",
    "# DecoderLayer.\n",
    "# Same as in EncoderLayer ==> Linear Layer parameters ==> 64 (weights) + 8 (bias) = 72\n",
    "num_decoder_src_attention_query_params = d_model * d_model + d_model\n",
    "num_decoder_src_attention_key_params = d_model * d_model + d_model\n",
    "num_decoder_src_attention_value_params = d_model * d_model + d_model\n",
    "num_decoder_src_attention_output_params = d_model * d_model + d_model\n",
    "# The FeedForward neural network is exactly the same as in EncoderLayer.\n",
    "# Same as in EncoderLayer ==> 8 * 16 + 16 = 144\n",
    "num_decoder_feed_forward_linear_layer_1_params = d_model * d_feed_forward + d_feed_forward\n",
    "# Same as in EncoderLayer ==> 8 * 16 + 8 = 136\n",
    "num_decoder_feed_forward_linear_layer_2_params = d_model * d_feed_forward + d_model\n",
    "# We have 1 additional LayerNormalization layer associated with the source attention. However, its architecture and\n",
    "# the parameters are the same as in EncoderLayer.\n",
    "# Same as in EncoderLayer ==> 8 (gamma) + 8 (beta) = 16\n",
    "num_decoder_self_attention_layer_norm_params = d_model + d_model\n",
    "num_decoder_src_attention_layer_norm_params = d_model + d_model\n",
    "num_decoder_feed_forward_layer_norm_params = d_model + d_model\n",
    "# The total number of parameters associated with a single DecoderLayer is the sum of the above 13 variables = \n",
    "num_decoder_layer_params = num_decoder_self_attention_query_params + num_decoder_self_attention_key_params + num_decoder_self_attention_value_params + num_decoder_self_attention_output_params + num_decoder_src_attention_query_params + num_decoder_src_attention_key_params + num_decoder_src_attention_value_params + num_decoder_src_attention_output_params + num_decoder_feed_forward_linear_layer_1_params + num_decoder_feed_forward_linear_layer_2_params + num_decoder_self_attention_layer_norm_params + num_decoder_src_attention_layer_norm_params + num_decoder_feed_forward_layer_norm_params\n",
    "# We also apply Layer Normalization to the output of the last DecoderLayer and pass this as the output of the Decoder. \n",
    "# This layer again has same number of parameters associated with it as other Layer Normalization layers i.e.,\n",
    "# 8 (gamma) + 8 (beta) = 16\n",
    "num_decoder_layer_layer_norm_params = d_model + d_model\n",
    "# The transformer model has 6 EncoderLayers stacked on top of each other. So, the total number of parameters \n",
    "# associated with Encoder is (6 * 904) + 16 = 5440\n",
    "num_total_decoder_params = (num_layers * num_decoder_layer_params) + num_decoder_layer_layer_norm_params\n",
    "# The output of the decoder is passed to a linear layer that projects the output to the target vocabulary space.\n",
    "# These parameters are associated with the TokenPredictor layer in the transformer above. The input to the linear\n",
    "# layer are 8-dimensional vectors (d_model) and output of the linear layers are 6-dimensional vectors (tgt_vocab_size).\n",
    "# So, the number of parameters associated with the TokenPredictor layer is 8 * 6 (weights) + 6 (bias) = 54\n",
    "num_vocab_projection_params = d_model * tgt_vocab_size + tgt_vocab_size\n",
    "# Finally, the total number of parameters in the model is the number of parameters associated with the Embeddings plus\n",
    "# the number of parameters in the Encoder plus the number of parameters in the Decoder plus the number of parameters\n",
    "# in the TokenPredictor.\n",
    "num_total_model_params = num_src_embedding_params + num_tgt_embedding_params + num_total_encoder_params + num_total_decoder_params + num_vocab_projection_params\n",
    "print(\"Total Number of parameters associated with the model: \", num_total_model_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, the number of parameters in the model we built is the same as the number of parameters expected within the model.\n",
    "# Usually, counting the number of parameters could make several bugs in the code visible and a good check to implement\n",
    "# for any machine learning model.\n",
    "assert total_params_with_grad == num_total_model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
