{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) How all the building blocks of the transformer fit together to build a machine translation model?\n",
    "# 2) Verfiy that the number of parameters within the built model is the same as number of parameters\n",
    "#    expected within out model architecture ie.,\n",
    "#           --> assert parameter_count (built_model) == parameter_count (count_manually). \n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# A LOT OF THE CODE IN THIS NOTEBOOK IS COPIED FROM THE PREVIOUS NOTEBOOKS. I WILL NOT BE EXPLAINING \n",
    "# THOSE PARTS IN THIS NOTEBOOK AGAIN.\n",
    "# ------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources to help understand this notebook:\n",
    "#\n",
    "# 1) https://nlp.seas.harvard.edu/annotated-transformer/\n",
    "#       --  The very best resource to understand the transformer implementation.\n",
    "# 2) http://jalammar.github.io/illustrated-transformer/\n",
    "#       -- Another great resource to understand the architecture of the transformer model.\n",
    "# 3) https://www.youtube.com/watch?v=8krd5qKVw-Q\n",
    "#       -- Gives an intuitive explanation of Xavier Initialization.\n",
    "# 4) https://www.deeplearning.ai/ai-notes/initialization/index.html\n",
    "#       -- Gives an in-depth mathematical explanation of Xavier Initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import torch\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from typing import Callable, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../Data/Images/TranslationTransformerModel.png\" alt=\"Translation Transformer Model\" width=\"600\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "credits: The above image is taken from this blog post: [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transformer model is a stack of N encoder-decoder layers.\n",
    "# In our implementation, each of the ENCODER boxes shown in the above image is referred to as an 'EncoderLayer' and \n",
    "# each of the DECODER boxes is referred to as a 'DecoderLayer'. The stack of 6 ENCODER boxes (EncoderLayers) is \n",
    "# referred to as an Encoder and the stack of 6 DECODER boxes (DecoderLayers) is referred to as a Decoder. The entire \n",
    "# model is referred to as a Transformer.\n",
    "#\n",
    "# The bottom most EncoderLayer receives the tokenized src sentence embeddings and the bottom most DecoderLayer \n",
    "# receives the tokenized tgt sentence embeddings. The sentences are tokenized in the Data Preparation phase which is \n",
    "# not part of the Transformer model below. The tokens are converted into embeddings, aggregated with the positional \n",
    "# encodings and passed to the bottom most EncoderLayer and bottom most DecoderLayer as inputs.\n",
    "#\n",
    "# The output of the ENCODER (the last EncoderLayer) is passed to each of the DecoderLayers for src attention \n",
    "# calculation. The output of the DECODER (the last DecoderLayer) is passed to a linear layer which projects the \n",
    "# Decoder output to the vocab space (target vocab space). This output is then passed to the softmax layer to get the \n",
    "# token output probabilities which is the output of the Transformer model as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../Data/Images/EncoderDecoder.png\" alt=\"EncoderDecoder\" width=\"550\" height=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "credits: The above image is taken from this blog post: [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants to be used in the model.\n",
    "# Size of the embedding vectors in the model. This is 512 in the transformer paper.\n",
    "d_model = 8\n",
    "# Number of layers in the Encoder in the Encoder and Decoder. The transformer paper also uses 6. The \n",
    "# number of layers could be different in Encoder and Decoder but we use the same value for both.\n",
    "num_layers = 6\n",
    "# Probability with which to drop data in the transformer model. We will use the same dropout_prob\n",
    "# throughout the model.\n",
    "dropout_prob = 0.1\n",
    "# Number of attention heads in each of the multi-head attention layers in the model. The transformer\n",
    "# paper uses 8.\n",
    "num_heads = 8\n",
    "# Number of neurons in the hidden layer (that expands the input) in the feed forward neural network.\n",
    "# The transformer paper uses 2048.\n",
    "d_feed_forward = 16\n",
    "# Number of sentences in each batch of data.\n",
    "batch_size = 2\n",
    "# Number of tokens in the src vocab. This is the number of unique words in the src language (English).\n",
    "src_vocab_size = 6\n",
    "# Number of tokens in the tgt vocab. This is the number of unique words in the tgt language (Telugu).\n",
    "tgt_vocab_size = 6\n",
    "# Number of tokens in each of the sentences in the batch. We will pad the sentences to make them all\n",
    "# of the same length. This is the length of the longest sentence in the batch.\n",
    "seq_len = 4\n",
    "# Maximum number of tokens in the sentences among all the batches in the entire dataset.\n",
    "max_seq_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVERYTHING IN THIS CELL HAS BEEN EXPLAINED IN DETAIL IN THE PREVIOUS NOTEBOOKS. PLEASE REFER TO THE EARLIER\n",
    "# NOTEBOOKS TO UNDERSTAND THE CODE IN THIS CELL. YOU CAN SKIP (JUST RUN IT BLINDLY) THIS CELL AND MOVE TO THE \n",
    "# NEXT CELL DIRECTLY.\n",
    "# \n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# JUST RUN THIS CELL BLINDLY | JUST RUN THIS CELL BLINDLY | JUST RUN THIS CELL BLINDLY | JUST RUN THIS CELL BLINDLY \n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Refer to 'step_6_token_embeddings.ipynb' notebook to learn more about the Embeddings class.\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        \"\"\"Creates the embedding layer that serves as a look-up table for the tokens in the transformer model.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary i.e., number of distinct tokens in the vocabulary.\n",
    "            embedding_dim (int): The size of the embedding vector to be generated for each token.\n",
    "        \"\"\"\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.look_up_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    # The input is a '2D' tensor where each '1D' tensor within the '2D' tensor is the list\n",
    "    # of indices corresponding to the tokens in the vocab.\n",
    "    # [[0, 123, 3455, 4556, 7, 1, 2, 2], [0, 56, 98, 6234, 909, 56, 1, 2]]\n",
    "    # 0 - <sos>, 1 - <eos>, 2 - <pad>\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"Returns the embedding vectors for the corresponding token indices in the input tensor.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): The input tensor containing token indices.\n",
    "                            shape: [batch_size, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The tensor of embedding vectors for the corresponding input token ids.\n",
    "                    shape: [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        # There is no reasoning as to why the original 'attention_is_all_you_need' paper scaled the\n",
    "        # embeddings using 'math.sqrt(embedding_dim)'. A few blogs attempted to explain this reasoning,\n",
    "        # but I haven't found any correct explanation with solid reasoning.\n",
    "        return self.look_up_table(input) * math.sqrt(self.embedding_dim)\n",
    "\n",
    "\n",
    "# Refer to 'step_8_positional_encoding.ipynb' notebook to learn more about the PositionalEncoding class.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, encoding_size: int, dropout_prob: float, max_len: int=max_seq_len):\n",
    "        \"\"\"Creates the positional encodings.\n",
    "\n",
    "        Args:\n",
    "            encoding_size (int): Size of the positional encoding vector that represents the position of the token.\n",
    "            dropout_prob (float): Probability of an element to be zeroed or dropped.\n",
    "            max_len (int): Largest position for which the positional encoding vector is generated. Defaults to \n",
    "                           max_seq_len (10). By default, it generates positional encodings for the first \n",
    "                           max_seq_len (10) positions.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Refer to step_7_drop_out.ipynb notebook (link to the notebook) to understand more about dropout.\n",
    "        self.dropout = nn.Dropout(p=dropout_prob, inplace=False)\n",
    "        # Compute the positional encodings in log space.\n",
    "        positional_encoding = torch.zeros(size=(max_len, encoding_size), dtype=torch.float)\n",
    "        positional_encoding_numerators = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        numerators_in_exponent = torch.arange(0, encoding_size, 2, dtype=torch.float)\n",
    "        positional_encoding_denominators = torch.exp(numerators_in_exponent * (-math.log(10000.0) / encoding_size))\n",
    "        positional_encoding[:, 0::2] = torch.sin(positional_encoding_numerators * positional_encoding_denominators)\n",
    "        positional_encoding[:, 1::2] = torch.cos(positional_encoding_numerators * positional_encoding_denominators)\n",
    "        # Refer to understanding_tensor_manipulations_part_1.ipynb notebook (add link to the notebook) to\n",
    "        # understand more about unsqueeze operation in pytorch.\n",
    "        # In transformer model, we receive 3D tensors as input to this module. Each 1D tensor\n",
    "        # in the last dimension is an embedding for the token. Each 2D tensor is a sentence.\n",
    "        # The entire 3D tensor is a batch of sentences. To work with 3D tensors in the forward\n",
    "        # method, we convert the positional encoding to a 3D tensor.\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        # Refer to using_modules.ipynb (link to the notebook) to understand more about buffers in pytorch.\n",
    "        # Essentially, This tells the module to not update the positional encoding tensor during the training. \n",
    "        # It is not a trainable parameter but it is still part of the state of the model.\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "    \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"Adds the positional encodings to the input tensor.\n",
    "        Args:\n",
    "            input (Tensor): The input tensor containing the embeddings of the tokens.\n",
    "                            shape: [batch_size, sequence_length, d_model]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Input with the positional encodings added to it.\n",
    "                    shape: [batch_size, sequence_length, d_model]\n",
    "        \"\"\"\n",
    "        # Refer to understanding_tensor_manipulations_part_5.ipynb notebook (add link to the notebook) to \n",
    "        # understand more about broadcasting in python.\n",
    "        # The input tensor is a 3D tensor of shape (batch_size, sequence_length, encoding_size).\n",
    "        # We add (uses broadcasting) the positional encoding to the input tensor to get the final tensor.\n",
    "        # positional_encoding: (1, max_len, encoding_size) --> (1, sequence_length, encoding_size) \n",
    "        #       -- Extracts the positional encodings for the sequence_length from the positional_encoding \n",
    "        #          tensor.\n",
    "        # (batch_size, sequence_length, encoding_size) --> input\n",
    "        # (batch_size, sequence_length, encoding_size) --> Resultant positional encoding tensor after broadcasting.\n",
    "        # requires_grad_(False) is not needed since the positional encoding is already registered\n",
    "        # as a Buffer and not a trainable parameter. It is just included for clarity.\n",
    "        input = input + self.positional_encoding[:, :input.size(1)].requires_grad_(False)\n",
    "        return self.dropout(input)\n",
    "    \n",
    "\n",
    "# Creates a copy (deepcopy) of the module and returns ModuleList containing the copies.\n",
    "def clone_module(module: nn.Module, num_clones: int) -> nn.ModuleList:\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(num_clones)])\n",
    "\n",
    "\n",
    "# Refer to 'step_9_multi_headed_attention.ipynb' notebook to understand how this function works.\n",
    "def construct_attention_heads(queries: Tensor, \n",
    "                              keys: Tensor, \n",
    "                              values: Tensor, \n",
    "                              mask: Optional[Tensor]=None, \n",
    "                              dropout_layer: Optional[nn.Module]=None) -> Tensor:\n",
    "    \"\"\"Calculates the attention scores for each token in the sequence with every other token in the sequence.\n",
    "       Applies the mask if provided and then normalizes the scores using softmax. It then calculates the \n",
    "       attention heads for each token in the sequence.\n",
    "\n",
    "    Args:\n",
    "        queries (Tensor): [batch_size, num_heads, seq_len, d_k]\n",
    "        keys (Tensor): [batch_size, num_heads, seq_len, d_k]\n",
    "        values (Tensor): [batch_size, num_heads, seq_len, d_k]\n",
    "        mask (Optional[Tensor]): [batch_size, 1, 1, src_seq_len] if the mask is for the source sequences.\n",
    "                                 [batch_size, 1, tgt_seq_len, tgt_seq_len] if the mask is for the target sequences.\n",
    "                                 Defaults to None.\n",
    "        dropout_layer (Optional[nn.Module], optional): probability with which the values are dropped on dropout \n",
    "                                                       layer. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Returns the attention heads.\n",
    "                SHAPE: [batch_size, num_heads, seq_len, d_k]\n",
    "    \"\"\"\n",
    "    # Size of the vectors for each token for each head in the sequence.\n",
    "    d_k = queries.shape[-1]\n",
    "    # Calculate the attention scores for each token in the sequence with every other token in the sequence.\n",
    "    attention_scores = torch.matmul(queries, keys.transpose(dim0=2, dim1=3)) / math.sqrt(d_k)\n",
    "    # Mask the attention scores if a mask is provided. Mask is used in two different ways:\n",
    "    # 1) To prevent the model from attending to the padding tokens --> This applies for both src and tgt sequences.\n",
    "    # 2) To prevent the model from attending to the future tokens in the sequence --> This applies only for tgt sequences.\n",
    "    if mask is not None:\n",
    "        # Please do not set the masked values to float('-inf') as it sometimes (not in everycase) causes softmax to return nan.\n",
    "        attention_scores = attention_scores.masked_fill(mask == False, float('-1e9'))\n",
    "    # Normalize the attention scores using softmax.\n",
    "    attention_scores = attention_scores.softmax(dim=-1)\n",
    "    # Apply dropout regularization to prevent overfitting problems.\n",
    "    if dropout_layer is not None:\n",
    "        attention_scores = dropout_layer(attention_scores)\n",
    "    # The result of this matrix multiplication is the attention_heads.\n",
    "    # Calculate the attention heads for each token in the sequence. The head for each token is calculated by\n",
    "    # taking the weighted average (averaged by attention scores) of the values for all the tokens in the \n",
    "    # sequence for the token of interest. \n",
    "    return torch.matmul(attention_scores, values)\n",
    "\n",
    "\n",
    "# Refer to 'step_9_multi_headed_attention.ipynb' notebook to understand how this function works.\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_model: int, dropout_prob: float=dropout_prob):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads.\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        # We use dropout to prevent overfitting.\n",
    "        self.dropout_layer = nn.Dropout(p=dropout_prob, inplace=False)\n",
    "        # Creating the linear layers that generate queries, keys and values for each token in the sequence.\n",
    "        # Also, creating an additional linear layer to generate the output of the Multi-Headed Attention from concatenated attention heads.\n",
    "        self.linear_layers = clone_module(module=nn.Linear(in_features=d_model, out_features=d_model), num_clones=4)\n",
    "\n",
    "\n",
    "    def forward(self, query_input: Tensor, key_input: Tensor, value_input: Tensor, mask: Optional[Tensor]=None) -> Tensor:\n",
    "        \"\"\"Forward pass of the Multi-Headed Attention layer. \n",
    "\n",
    "        Args:\n",
    "            query_input (Tensor): Input to be used for query creation.\n",
    "                                  SHAPE: [batch_size, seq_len, d_model]\n",
    "            key_input (Tensor): Input to be used for key creation.\n",
    "                                SHAPE: [batch_size, seq_len, d_model]\n",
    "            value_input (Tensor): Input to be used for value creation.\n",
    "                                  SHAPE: [batch_size, seq_len, d_model]\n",
    "            mask (Tensor): Mask to be applied to the attention scores. Default is None. Same mask will \n",
    "                           be applied to all the heads in the Multi-Headed Attention layer.\n",
    "                           mask: [batch_size, 1, 1, src_seq_len] if the mask is for the source sequences.\n",
    "                           mask: [batch_size, 1, tgt_seq_len, tgt_seq_len] if the mask is for the target sequences. \n",
    "                           Note that src_seq_len and tgt_seq_len are the number of tokens in the source and target sequences\n",
    "                           respectively and they are likely different.\n",
    "\n",
    "        Returns:\n",
    "            Mutli-Headed Attention Output: Output of the Multi-Headed Attention layer. Generates one output vector \n",
    "                                           for each token in the sequence. Does this for each sequence in the batch.\n",
    "                                           SHAPE: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Generates the queries, keys and values for each token in the sequence.\n",
    "        # shape of queries, keys, values: [batch_size, seq_len, d_model]\n",
    "        queries, keys, values = [linear_layer(input) for linear_layer, input in zip(self.linear_layers, (query_input, key_input, value_input))]\n",
    "        batch_size = query_input.shape[0]\n",
    "        # Using '-1' in the view function is to infer the size of the dimension from the original tensor. This is important because\n",
    "        # the 'seq_len' for the keys, values comes from Encoder output (i.e., src sequences) and the 'seq_len' for the queries comes\n",
    "        # from decoder input (i.e., tgt sequences) in source attention. The src_sequence size and tgt_sequence size are likely \n",
    "        # different and are being handled with common functionality here. So, we need to infer the size of the dimension from the \n",
    "        # original tensor instead of harcoding it from the query_input tensor. You can try it by hardcoding the seq_len (instead of setting it to -1) \n",
    "        # for keys and values and see the error you get to understand it better (I found out this issue after noticing the errors).\n",
    "        # This separates the queries, keys and values for each head into a separate vector (thus a 4D tensor). The vectors for each \n",
    "        # token in all the heads are concatenated when they are created using the linear_layers above.\n",
    "        # Shape for queries, keys, values after view: [batch_size, seq_len, num_heads, d_k]\n",
    "        # Shape for queries, key, values after transpose: [batch_size, num_heads, seq_len, d_k]\n",
    "        queries, keys, values = [data.view(batch_size, -1, self.num_heads, self.d_k).transpose(dim0=1, dim1=2) for data in (queries, keys, values)]\n",
    "        # Calculate the attention heads for each token in the sequence.\n",
    "        # attention_heads: [batch_size, num_heads, seq_len, d_k]\n",
    "        attention_heads = construct_attention_heads(queries=queries, keys=keys, values=values, mask=mask, dropout_layer=self.dropout_layer)\n",
    "        # Concatenate the attention heads for each token from all the heads.\n",
    "        # attention_heads: [batch_size, seq_len, d_model]\n",
    "        attention_heads = attention_heads.transpose(dim0=1, dim1=2).reshape(batch_size, -1, self.d_model)\n",
    "        # Generate the output of the Multi-Headed Attention layer.\n",
    "        return self.linear_layers[-1](attention_heads)\n",
    "    \n",
    "\n",
    "# Refer to 'step_10_feed_forward_neural_network.ipynb' notebook to understand how this class works.\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, d_model: int, d_feed_forward: int, dropout_prob: float = dropout_prob):\n",
    "        super().__init__()\n",
    "        self.linear_layer_1 = nn.Linear(in_features=d_model, out_features=d_feed_forward)\n",
    "        self.linear_layer_2 = nn.Linear(in_features=d_feed_forward, out_features=d_model)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout_prob, inplace=False)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"Passes the input through the Feed Forward Neural Network and returns the output \n",
    "           of the neural network.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): The output of the Multi-Headed Attention layer.\n",
    "                            shape: [batch_size, seq_len, d_model]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output of the Feed Forward Neural Network.\n",
    "                    shape: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # We first expand the input to higher dimension. We apply the ReLU activation function in this layer.\n",
    "        intermediate_output = self.linear_layer_1(input).relu()\n",
    "        # Dropout layer to prevent overfitting\n",
    "        intermediate_output = self.dropout_layer(intermediate_output)\n",
    "        # We then compress the input back to its original dimension. There is no specific intuitive explanation \n",
    "        # as to why this is done. It is just shown to be working practically in neural networks in general and \n",
    "        # in this paper in particular.\n",
    "        return self.linear_layer_2(intermediate_output)\n",
    "    \n",
    "\n",
    "# Refer to 'step_12_encoder.ipynb' to understand how this class works.\n",
    "class SubLayerWrapper(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout_prob: float):\n",
    "        \"\"\"This class is a wrapper around the MultiHeadedAttention and PositionwiseFeedForward classes.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimension of the vectors used in the Attention model.\n",
    "            dropout_prob (float): probability with which nodes can be dropped.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_prob, inplace=False)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, input: Tensor, sublayer: Callable[[Tensor], Tensor]) -> Tensor:\n",
    "        \"\"\"It applies the operation on the input, applies dropout, adds the input back to the transformed \n",
    "           input, does normalization and returns the output.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Input to be transformer by the sublayer.\n",
    "                            shape: [batch_size, seq_len, d_model]\n",
    "            sublayer (Callable): sublayer is a callable that takes a tensor as input and returns a tensor \n",
    "                                 as output. Could be either a lambda function that calls MultiHeadedAttention \n",
    "                                 or a direct nn.Module which is PositionwiseFeedForward in this case.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: Output of the sublayer transformation.\n",
    "                    shape: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        return input + self.dropout(sublayer(self.layer_norm(input)))\n",
    "\n",
    "\n",
    "# Refer to 'step_12_encoder.ipynb' to understand how this class works.\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 self_attention: MultiHeadedAttention, \n",
    "                 feed_forward: FeedForwardNN, \n",
    "                 d_model: int, \n",
    "                 dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout_prob = dropout_prob\n",
    "        # These modules are now the child modules of the EncoderLayer and will be registered as parameters of the EncoderLayer.\n",
    "        self.self_attention = self_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        # We need two instances of the SubLayerWrapper class. One for the self_attention and the other for the feed_forward.\n",
    "        self.sublayer_wrappers = clone_module(module=SubLayerWrapper(d_model=self.d_model, dropout_prob=self.dropout_prob), num_clones=2)\n",
    "\n",
    "    def forward(self, input: Tensor, mask: Tensor) -> Tensor:\n",
    "        \"\"\"This method is the forward pass of the EncoderLayer class.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Source sequences provided as input to the EncoderLayer. These are the embeddings of the source \n",
    "                            sequences for the first EncoderLayer.\n",
    "                            SHAPE: [batch_size, src_seq_len, d_model]\n",
    "            mask (Tensor): Boolean mask to be applied to the input during attention scores calculation.\n",
    "                           SHAPE: [batch_size, 1, 1, src_seq_len]\n",
    "        Returns:\n",
    "            Tensor: Output of the EncoderLayer.\n",
    "                    SHAPE: [batch_size, src_seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # We are just saving the function call to the self_attention method in a variable and passing the\n",
    "        # lambda function (contained within the variable) to the sublayer_wrappers[0] to execute it when \n",
    "        # needed.\n",
    "        output = self.sublayer_wrappers[0](input, lambda input: self.self_attention(query_input=input, key_input=input, value_input=input, mask=mask))\n",
    "        return self.sublayer_wrappers[1](output, self.feed_forward)\n",
    "    \n",
    "\n",
    "# Refer to 'step_12_encoder.ipynb' to understand how this class works.\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_layer: EncoderLayer, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = clone_module(module=encoder_layer, num_clones=num_layers)\n",
    "        self.layer_norm = nn.LayerNorm(encoder_layer.d_model)\n",
    "\n",
    "    def forward(self, input: Tensor, mask: Optional[Tensor]=None) -> Tensor:\n",
    "        \"\"\"This method is the forward pass of the Encoder class. The output of the current EncoderLayer is\n",
    "           passed as input to the next EncoderLayer. We have 6 identical EncoderLayers stacked on top of \n",
    "           each other. The output of the last EncoderLayer is passed through a Layer Normalization layer\n",
    "           and returned as the final output of the Encoder\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Input to the Encoder i.e., embeddings of the tokenized src sequences.\n",
    "                            input: [batch_size, src_seq_len, d_model]\n",
    "            mask (Optional[Tensor], optional): Boolean mask to be applied during attention scores calculation.\n",
    "                                               mask: [batch_size, 1, 1, src_seq_len]. Defaults to None.\n",
    "                            \n",
    "        Returns:\n",
    "            Tensor: Output of the Encoder i.e., encoded src sequences.\n",
    "                    output: [batch_size, src_seq_len, d_model]\n",
    "        \"\"\"\n",
    "        output = input\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            # Pass the output of the previous EncoderLayer to the current EncoderLayer.\n",
    "            output = encoder_layer(input=output, mask=mask)\n",
    "        return self.layer_norm(output)\n",
    "\n",
    "\n",
    "# Refer to 'step_13_decoder.ipynb' to understand how this class works.\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 self_attention: MultiHeadedAttention, \n",
    "                 src_attention: MultiHeadedAttention, \n",
    "                 feed_forward: FeedForwardNN, \n",
    "                 d_model: int, \n",
    "                 dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout_prob = dropout_prob\n",
    "        # These modules are now the child modules of the DecoderLayer and will be registered as parameters of the DecoderLayer.\n",
    "        self.self_attention = self_attention\n",
    "        self.src_attention = src_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer_wrappers = clone_module(module=SubLayerWrapper(d_model=d_model, dropout_prob=dropout_prob), num_clones=3)\n",
    "\n",
    "    def forward(self, input: Tensor, encoded_src: Tensor, tgt_mask: Tensor, src_mask: Optional[Tensor]=None) -> Tensor:\n",
    "        \"\"\"This method is the forward pass of the DecoderLayer class.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Target sequences provided as input to the DecoderLayer. These are the embeddings of the target \n",
    "                            sequences for the first DecoderLayer.\n",
    "                            SHAPE: [batch_size, tgt_seq_len, d_model]\n",
    "            encoded_src (Tensor): Encoded source sequences. This is the output of the Encoder. This is used to calculate the\n",
    "                                  source attention scores for the target sequences. \n",
    "                                  SHAPE: [batch_size, seq_len, d_model] \n",
    "            tgt_mask (Tensor): Mask to prevent the future tokens in the target sequences to attend to the previous tokens and\n",
    "                               also to prevent padding tokens from attending to any other token except other padding tokens.\n",
    "                               SHAPE: [batch_size, 1, tgt_seq_len, tgt_seq_len]\n",
    "            src_mask (Tensor, optional): Mask to prevent the the padding tokens to attend to the tokens in the tgt sentence. \n",
    "                                         Defaults to None.\n",
    "                                         SHAPE: [batch_size, 1, 1, src_seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Returns the output of the DecoderLayer. This is the output of the Positionwise FeedForward Neural Network.\n",
    "                    SHAPE: [batch_size, tgt_seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # First sublayer: Self-Attention on the target sentence. Hence, it uses the tgt_mask.\n",
    "        self_attention_output = self.sublayer_wrappers[0](input=input, sublayer=lambda input: self.self_attention(query_input=input, key_input=input, value_input=input, mask=tgt_mask)) \n",
    "        # To give intuition about src_attention, I have a query for a token in the target sequence. I want to know whether \n",
    "        # some token in the source sequence is important for me to predict the output for this token in the target sequence. \n",
    "        # So, I go to the source sequence and get the values for all the tokens in the source sequence. I then calculate \n",
    "        # the attention scores between the query (in tgt) and the keys (in src). I then calculate the attention heads for \n",
    "        # the token in the target sequence using the attention scores. This is what is done in the below line. Note that \n",
    "        # referring to statement 'the keys and values are from the source' doesn't mean that you get keys and values \n",
    "        # explicitly. It means we use the encoded data from the source sequences to calculate the queries and keys for \n",
    "        # this transformation.\n",
    "        # Second sublayer: Attention on the source sequences. Hence, it uses the src_mask.\n",
    "        src_attention_output = self.sublayer_wrappers[1](input=self_attention_output, sublayer=lambda self_attention_output: self.src_attention(query_input=self_attention_output, key_input=encoded_src, value_input=encoded_src, mask=src_mask))\n",
    "        # Third sublayer: Positionwise FeedForward Neural Network.\n",
    "        return self.sublayer_wrappers[2](input=src_attention_output, sublayer=self.feed_forward)\n",
    "    \n",
    "\n",
    "# Refer to 'step_13_decoder.ipynb' to understand how this class works.\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_layer: DecoderLayer, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.decoder_layers = clone_module(module=decoder_layer, num_clones=num_layers)\n",
    "        self.layer_norm = nn.LayerNorm(decoder_layer.d_model)\n",
    "\n",
    "    def forward(self, input: Tensor, encoded_src: Tensor, tgt_mask: Tensor, src_mask: Optional[Tensor]=None) -> Tensor:\n",
    "        \"\"\"This method is the forward pass of the Decoder class. The output of the current DecoderLayer is\n",
    "           passed as input to the next DecoderLayer. We have 6 identical DecoderLayers stacked on top of \n",
    "           each other. The output of the Encoder (last EncoderLayer) is also passed as input to the \n",
    "           first DecoderLayer. The output of the last DecoderLayer is passed through a Layer Normalization \n",
    "           layer and returned as the final output of the Decoder.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Input to the Decoder i.e., embeddings of the tokenized tgt sequences.\n",
    "                            SHAPE: [batch_size, tgt_seq_len, d_model]\n",
    "            encoded_src (Tensor): output of the encoder i.e., encoded src sequences.\n",
    "                                  SHAPE: [batch_size, src_seq_len, d_model]\n",
    "            tgt_mask (Tensor): Boolean mask to be applied during self attention scores calculation.\n",
    "                               SHAPE: [batch_size, 1, tgt_seq_len, tgt_seq_len].\n",
    "            src_mask (Tensor, optional): Boolean mask to be applied during src attention scores calculation.\n",
    "                                         SHAPE: [batch_size, 1, 1, src_seq_len]. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output of the Decoder.\n",
    "                    SHAPE: [batch_size, tgt_seq_len, d_model]\n",
    "        \"\"\"\n",
    "        output = input\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            # Pass the output of the previous DecoderLayer to the current DecoderLayer.\n",
    "            output = decoder_layer(input=output, encoded_src=encoded_src, tgt_mask=tgt_mask, src_mask=src_mask)\n",
    "        return self.layer_norm(output)\n",
    "\n",
    "\n",
    "# Refer to 'step_14_token_predictor.ipynb' to understand how this class works.\n",
    "class TokenPredictor(nn.Module):\n",
    "    def __init__(self, d_model: int, tgt_vocab_size: int):\n",
    "        super(TokenPredictor, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = tgt_vocab_size\n",
    "        self.linear = nn.Linear(in_features=d_model, out_features=tgt_vocab_size)\n",
    "        # The non-module variables are not added to the list of parameters of the model.\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, decoder_output: Tensor) -> Tensor:\n",
    "        \"\"\"The forward pass of the token predictor. Calculates the probability distribution over the \n",
    "           vocabulary. Each token vector has a corresponding probability distribution over the \n",
    "           vocabulary since we predict one token per output.\n",
    "\n",
    "        Args:\n",
    "            decoder_output (Tensor): Output of the Decoder.\n",
    "                                     SHAPE: [batch_size, tgt_seq_len - 1, d_model]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Log probability distribution over the vocabulary. \n",
    "                    SHAPE: [batch_size, tgt_seq_len - 1, vocab_size]\n",
    "        \"\"\"\n",
    "        # Project the decoder output to the vocab_size dimensional space.\n",
    "        logits = self.linear(decoder_output)\n",
    "        # Convert the logits to a probability distribution over the vocabulary. All the entries in the\n",
    "        # output tensor are negative since we are using log softmax. The log softmax is used to make\n",
    "        # the training more numerically stable. However, the maximum value in log_softmax is still the \n",
    "        # same as the maximum value of the general softmax output.\n",
    "        return self.log_softmax(logits)\n",
    "    \n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# CELL CONTAINING THE COPIED CODE FROM PREVIOUS NOTEBOOKS ENDS HERE.\n",
    "# CELL CONTAINING THE COPIED CODE FROM PREVIOUS NOTEBOOKS ENDS HERE.\n",
    "# CELL CONTAINING THE COPIED CODE FROM PREVIOUS NOTEBOOKS ENDS HERE.\n",
    "# -------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer model i.e., learning material specific to this notebook starts from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the Machine Translation model is source tensors, source masks, target tensors, and target masks. <br>\n",
    "The output of the Machine Translation model is the a bunch of tensors where each tensor corresponds to the <br>\n",
    "probability distribution over the target vocabulary. These bunch of tensors will be used to predict the exact <br>\n",
    "tokens in later steps and output a readable sentence in the target language. <br>\n",
    "\n",
    "To build a Machine translation model, we need to instantiate all of the modules we have created in earlier <br>\n",
    "notebooks. This instantiation should be done in the initialization function of the machine translation module. <br>\n",
    "The input need to be passed through each of these modules appropriately and the output of the last module is <br>\n",
    "the output of the machine translation model. This is done in the forward function of the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets go step by step and understand what all modules are necesary:\n",
    "# Step 1:\n",
    "#   -- We need embedding vectors for the source and target vocabulary. We made a choice to keep the tokens\n",
    "#      separate for both languages.\n",
    "src_embedding = Embeddings(vocab_size=src_vocab_size, embedding_dim=d_model)\n",
    "tgt_embedding = Embeddings(vocab_size=tgt_vocab_size, embedding_dim=d_model)\n",
    "\n",
    "# Step 2:\n",
    "#   -- We need positional encoding module to find the tensors corresponding to each token position and add\n",
    "#      it to the embedding vectors.\n",
    "src_positional_encoding = PositionalEncoding(encoding_size=d_model, dropout_prob=dropout_prob, max_len=max_seq_len)\n",
    "tgt_positional_encoding = PositionalEncoding(encoding_size=d_model, dropout_prob=dropout_prob, max_len=max_seq_len)\n",
    "\n",
    "# Step 3:\n",
    "#   -- We need Multi Headed Attention module to pass the token vectors through the attention layer in Encoder\n",
    "#      and Decoder separately. We create 1 multi headed attention module and copy it as many times as \n",
    "#      necessary and pass it to Encoder and Decoder accordingly.\n",
    "multi_headed_attention = MultiHeadedAttention(num_heads=num_heads, d_model=d_model, dropout_prob=dropout_prob)\n",
    "\n",
    "# Step 4:\n",
    "#   -- We need Feed Forward module to pass the token vectors through the Feedforward layer that expands and \n",
    "#      compresses the vectors sizes. We again create 1 feed forward module and copy it as many times as \n",
    "#      necessary and pass it to the Encoder and Decoder accordingly.\n",
    "feed_forward_nn = FeedForwardNN(d_model=d_model, d_feed_forward=d_feed_forward, dropout_prob=dropout_prob)\n",
    "\n",
    "# Step 5:\n",
    "#   -- We need the Encoder to encode the source input tokens and use it for decoding in the Decoder.\n",
    "encoder_layer = EncoderLayer(self_attention=copy.deepcopy(multi_headed_attention), \n",
    "                             feed_forward=copy.deepcopy(feed_forward_nn), \n",
    "                             d_model=d_model, \n",
    "                             dropout_prob=dropout_prob)\n",
    "\n",
    "# Step 6:\n",
    "#   -- We need the Decoder that behaves differently during training and inference time.\n",
    "#   -- Training time: Encode the target tokens and predict the next token for every current token in the input.\n",
    "#   -- Testing time: Predict the target tokens using the already predicted tokens.\n",
    "decoder_layer = DecoderLayer(self_attention=copy.deepcopy(multi_headed_attention), \n",
    "                             src_attention=copy.deepcopy(multi_headed_attention),\n",
    "                             feed_forward=copy.deepcopy(feed_forward_nn), \n",
    "                             d_model=d_model, \n",
    "                             dropout_prob=dropout_prob)\n",
    "\n",
    "# Step 7:\n",
    "#   -- We need to convert the output token vectors from the last Decoder to the probability distributions over\n",
    "#      the target vocabulary. We need token predictor module to be able to do this.\n",
    "token_predictor = TokenPredictor(d_model=d_model, tgt_vocab_size=tgt_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xavier Initialization\n",
    "\n",
    "We instantiated all the module necessary for the machine translation model. When the modules are initialized, all <br>\n",
    "the learnable parameters are initialized to random values. This random initialization makes the training slower and <br>\n",
    "harder to converge to a local minimum during training. Initializing the parameters in a specific way is helps us <br>\n",
    "avoid these issues. We use Xavier initialization in this model. To do this we iterate over all the learnable <br>\n",
    "parameters of a module and initialize them using Xavier initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets generate some random source, and target data to experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  src\n",
      "shape:  torch.Size([2, 4])\n",
      "tensor([[2, 0, 0, 0],\n",
      "        [2, 1, 2, 0]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "name:  src_mask\n",
      "shape:  torch.Size([2, 1, 1, 4])\n",
      "tensor([[[[False, False,  True, False]]],\n",
      "\n",
      "\n",
      "        [[[False,  True, False, False]]]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "name:  tgt\n",
      "shape:  torch.Size([2, 4])\n",
      "tensor([[0, 5, 3, 4],\n",
      "        [1, 5, 0, 4]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "name:  tgt_mask\n",
      "shape:  torch.Size([2, 1, 4, 4])\n",
      "tensor([[[[ True, False,  True,  True],\n",
      "          [False, False, False, False],\n",
      "          [ True, False, False,  True],\n",
      "          [False, False,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[ True, False, False, False],\n",
      "          [ True,  True, False,  True],\n",
      "          [False,  True, False, False],\n",
      "          [False, False,  True, False]]]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL CONTAINS SOME HELPER FUNCTIONS TO GENERATE RANDOM DATA FOR TESTING THE MODEL.\n",
    "\n",
    "# Generates the (src, tgt) data and prints them for visibility.\n",
    "def PrintTensor(input: Tensor, name: str):\n",
    "    print(\"name: \", name)\n",
    "    print(\"shape: \", input.shape)\n",
    "    print(input)\n",
    "    print(\"-\" * 150)\n",
    "\n",
    "# The true data will have a specific format that is expected by the model. That part is not considered\n",
    "# in this random generation. The random generation is just to test the model and not to train it.\n",
    "def generate_batch_of_input_data(batch_size: int, seq_len: int, vocab_size: int) -> Tensor:\n",
    "    return torch.randint(low=0, high=vocab_size, size=(batch_size, seq_len))\n",
    "\n",
    "# The true data will have masks that are corelated with the data. That part is not considered in this\n",
    "# random generation. The random generation of masks is just to test the model and not to train it.\n",
    "def construct_target_mask(batch_size: int, seq_len: int) -> Tensor:\n",
    "    # If some index is set to False, then it will be masked out.\n",
    "    mask = torch.randn(size=(batch_size, 1, seq_len, seq_len)) > 0.5\n",
    "    return mask.bool()\n",
    "\n",
    "# The true data will have masks that are corelated with the data. That part is not considered in this\n",
    "# random generation. The random generation of masks is just to test the model and not to train it.\n",
    "def construct_source_mask(batch_size: int, seq_len: int) -> Tensor:\n",
    "    # If some index is set to False, then it will be masked out.\n",
    "    mask = torch.randn(size=(batch_size, 1, 1, seq_len)) > 0.5\n",
    "    return mask.bool()\n",
    "\n",
    "# Please not that these are randomly generated data and do not consider the restrictions that are present\n",
    "# in the actual data. This is just for experimentation.\n",
    "src = generate_batch_of_input_data(batch_size=batch_size, seq_len=seq_len, vocab_size=src_vocab_size)\n",
    "PrintTensor(input=src, name=\"src\")\n",
    "src_mask = construct_source_mask(batch_size=batch_size, seq_len=seq_len)\n",
    "PrintTensor(input=src_mask, name=\"src_mask\")\n",
    "tgt = generate_batch_of_input_data(batch_size=batch_size, seq_len=seq_len, vocab_size=tgt_vocab_size)\n",
    "PrintTensor(input=tgt, name=\"tgt\")\n",
    "tgt_mask = construct_target_mask(batch_size=batch_size, seq_len=seq_len)\n",
    "PrintTensor(input=tgt_mask, name=\"tgt_mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward function logic of the Machine Translation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of src_embedding_vectors: torch.Size([2, 4, 8])\n",
      "src_embedding_vectors: tensor([[[-1.0407, -2.4498, -2.0050, -0.4954,  0.8531, -1.1112, -3.4147,\n",
      "          -1.7112],\n",
      "         [-4.3891, -1.1112,  1.3253,  0.2707,  3.3873, -5.7342,  1.6317,\n",
      "          -1.5066],\n",
      "         [-4.3891, -1.1112,  1.3253,  0.2707,  3.3873, -5.7342,  1.6317,\n",
      "          -1.5066],\n",
      "         [-4.3891, -1.1112,  1.3253,  0.2707,  3.3873, -5.7342,  1.6317,\n",
      "          -1.5066]],\n",
      "\n",
      "        [[-1.0407, -2.4498, -2.0050, -0.4954,  0.8531, -1.1112, -3.4147,\n",
      "          -1.7112],\n",
      "         [-2.7075, -1.1179,  0.4477,  0.0810,  4.1930, -3.6844, -0.5976,\n",
      "          -2.0886],\n",
      "         [-1.0407, -2.4498, -2.0050, -0.4954,  0.8531, -1.1112, -3.4147,\n",
      "          -1.7112],\n",
      "         [-4.3891, -1.1112,  1.3253,  0.2707,  3.3873, -5.7342,  1.6317,\n",
      "          -1.5066]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the embedding vectors for the tokens in the source tensors.\n",
    "src_embedding_vectors = src_embedding(src)\n",
    "print(f\"shape of src_embedding_vectors: {src_embedding_vectors.shape}\")\n",
    "print(f\"src_embedding_vectors: {src_embedding_vectors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of src_embedding_vectors_with_position: torch.Size([2, 4, 8])\n",
      "src_embedding_vectors_with_position:  tensor([[[-1.1563, -1.6109, -0.0000,  0.5607,  0.9479, -0.1235, -3.7941,\n",
      "          -0.7902],\n",
      "         [-3.9418, -0.6343,  0.0000,  1.4063,  3.7748, -5.2603,  1.8141,\n",
      "          -0.0000],\n",
      "         [-3.8665, -1.6970,  1.6933,  1.3897,  3.7859, -5.2605,  1.8152,\n",
      "          -0.5628],\n",
      "         [-4.7200, -2.3346,  1.8009,  1.3622,  3.7970, -5.2608,  1.8163,\n",
      "          -0.5628]],\n",
      "\n",
      "        [[-1.1563, -1.6109, -2.2278,  0.5607,  0.0000, -0.1235, -3.7941,\n",
      "          -0.7902],\n",
      "         [-2.0733, -0.6418,  0.6084,  1.1956,  4.6700, -2.9827, -0.6628,\n",
      "          -1.2096],\n",
      "         [-0.1460, -3.1844, -2.0070,  0.5385,  0.9701, -0.1238, -3.7919,\n",
      "          -0.7902],\n",
      "         [-4.7200, -2.3346,  1.8009,  1.3622,  3.7970, -0.0000,  1.8163,\n",
      "          -0.0000]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Add position information to the source embedding vectors.\n",
    "src_embedding_vectors_with_position = src_positional_encoding(src_embedding_vectors)\n",
    "print(f\"shape of src_embedding_vectors_with_position: {src_embedding_vectors_with_position.shape}\")\n",
    "print(\"src_embedding_vectors_with_position: \", src_embedding_vectors_with_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoded_src: torch.Size([2, 4, 8])\n",
      "encoded_src: tensor([[[-1.0788e+00, -2.4987e+00,  2.4476e-01,  2.3449e-01,  1.7933e+00,\n",
      "           5.0035e-03, -3.9426e+00, -1.3105e+00],\n",
      "         [-4.2757e+00, -1.3240e+00,  3.2981e-01,  1.3144e+00,  4.4227e+00,\n",
      "          -5.3738e+00,  1.4156e+00, -5.6603e-01],\n",
      "         [-4.0959e+00, -2.2578e+00,  2.1010e+00,  1.3444e+00,  4.3172e+00,\n",
      "          -5.3181e+00,  1.6782e+00, -1.2229e+00],\n",
      "         [-5.0831e+00, -3.0675e+00,  2.2088e+00,  1.2976e+00,  4.4506e+00,\n",
      "          -5.2016e+00,  1.4037e+00, -1.1183e+00]],\n",
      "\n",
      "        [[-1.0684e+00, -2.2803e+00, -2.5831e+00,  2.8138e-01,  3.1579e-01,\n",
      "           2.1795e-01, -4.6701e+00, -1.4581e+00],\n",
      "         [-1.9891e+00, -1.4342e+00,  4.1396e-01,  1.0769e+00,  4.9344e+00,\n",
      "          -2.5030e+00, -1.0236e+00, -1.5172e+00],\n",
      "         [-6.0431e-03, -3.9125e+00, -1.9845e+00,  5.1564e-01,  1.0958e+00,\n",
      "          -7.5007e-02, -3.9750e+00, -1.5933e+00],\n",
      "         [-4.7928e+00, -3.0232e+00,  1.8152e+00,  1.3983e+00,  4.1163e+00,\n",
      "          -1.1414e-01,  1.0259e+00, -3.4902e-01]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now, pass the source embedding vectors through the Encoder to get the encoded source sequences.\n",
    "encoded_src = encoder_layer(input=src_embedding_vectors_with_position, mask=src_mask)\n",
    "print(f\"shape of encoded_src: {encoded_src.shape}\")\n",
    "print(f\"encoded_src: {encoded_src}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tgt_embedding_vectors: torch.Size([2, 4, 8])\n",
      "tgt_embedding_vectors: tensor([[[-4.8951, -0.2630,  3.4337,  0.1546, -1.7134,  6.4791,  7.0275,\n",
      "          -1.8557],\n",
      "         [ 4.0292, -0.3557, -3.2353,  4.4224, -1.0516,  2.3604,  0.1306,\n",
      "          -0.3515],\n",
      "         [-1.0533, -2.1132, -5.1767,  5.0605, -0.5604, -4.0133, -0.7780,\n",
      "           4.1544],\n",
      "         [ 1.8254,  4.6595,  1.5401,  3.3508,  1.2806, -2.0570,  1.5998,\n",
      "           2.9175]],\n",
      "\n",
      "        [[-2.2948, -3.6065,  0.2519, -1.1297, -2.9740,  3.1283,  2.7748,\n",
      "          -3.7646],\n",
      "         [ 4.0292, -0.3557, -3.2353,  4.4224, -1.0516,  2.3604,  0.1306,\n",
      "          -0.3515],\n",
      "         [-4.8951, -0.2630,  3.4337,  0.1546, -1.7134,  6.4791,  7.0275,\n",
      "          -1.8557],\n",
      "         [ 1.8254,  4.6595,  1.5401,  3.3508,  1.2806, -2.0570,  1.5998,\n",
      "           2.9175]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the embedding vectors for the target tokens.\n",
    "tgt_embedding_vectors = tgt_embedding(tgt)\n",
    "print(f\"shape of tgt_embedding_vectors: {tgt_embedding_vectors.shape}\")\n",
    "print(f\"tgt_embedding_vectors: {tgt_embedding_vectors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tgt_embedding_vectors_with_position: torch.Size([2, 4, 8])\n",
      "tgt_embedding_vectors_with_position: tensor([[[-5.4390,  0.0000,  3.8153,  1.2829, -1.9037,  8.3101,  7.8083,\n",
      "          -0.9507],\n",
      "         [ 5.4119,  0.2052, -3.4839,  6.0193, -0.0000,  3.7337,  0.1463,\n",
      "           0.7205],\n",
      "         [-0.1600, -2.8104, -5.5311,  6.7118, -0.6004, -3.3483, -0.8622,\n",
      "           5.7271],\n",
      "         [ 0.0000,  4.0772,  2.0396,  4.7845,  1.4562, -1.1750,  1.7809,\n",
      "           4.3527]],\n",
      "\n",
      "        [[-2.5498, -2.8962,  0.2799, -0.1441, -3.3044,  4.5870,  3.0831,\n",
      "          -3.0718],\n",
      "         [ 5.4119,  0.2052, -3.4839,  6.0193, -1.1573,  3.7337,  0.1463,\n",
      "           0.7205],\n",
      "         [-4.4286, -0.7546,  4.0360,  1.2608, -1.8815,  0.0000,  7.8105,\n",
      "          -0.9507],\n",
      "         [ 2.1850,  4.0772,  0.0000,  4.7845,  1.4562, -1.1750,  1.7809,\n",
      "           4.3527]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Add position information to the target embedding vectors.\n",
    "tgt_embedding_vectors_with_position = tgt_positional_encoding(tgt_embedding_vectors)\n",
    "print(f\"shape of tgt_embedding_vectors_with_position: {tgt_embedding_vectors_with_position.shape}\")\n",
    "print(f\"tgt_embedding_vectors_with_position: {tgt_embedding_vectors_with_position}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of decoder_output: torch.Size([2, 4, 8])\n",
      "decoder_output: tensor([[[-6.8903, -1.3032,  3.6175,  0.8016, -0.4512,  7.4971,  7.2742,\n",
      "          -1.8167],\n",
      "         [ 4.3756, -1.7061, -3.8398,  5.0032,  1.0308,  3.3971,  0.0600,\n",
      "          -0.0650],\n",
      "         [-0.8664, -5.0414, -5.2303,  5.9490,  0.1336, -4.6500, -2.4436,\n",
      "           5.2300],\n",
      "         [-0.8941,  1.9210,  1.9445,  4.3427,  1.8343, -2.5898, -0.1082,\n",
      "           3.7365]],\n",
      "\n",
      "        [[-2.9524, -3.6451, -0.0892, -0.2469, -3.8061,  3.0214,  1.5248,\n",
      "          -3.0172],\n",
      "         [ 4.3414, -0.2938, -4.1295,  5.2066, -1.5429,  2.8994, -0.8386,\n",
      "           0.2133],\n",
      "         [-4.8313, -2.0372,  3.7531,  1.7165, -2.4956, -1.3718,  6.2177,\n",
      "          -1.5337],\n",
      "         [ 2.2659,  3.4722, -0.6654,  4.2460,  2.0903, -1.9015,  1.5401,\n",
      "           3.8531]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Pass the target embedding vectors through the Decoder to get the output of the Decoder.\n",
    "decoder_output = decoder_layer(input=tgt_embedding_vectors_with_position, encoded_src=encoded_src, tgt_mask=tgt_mask, src_mask=src_mask)\n",
    "print(f\"shape of decoder_output: {decoder_output.shape}\")\n",
    "print(f\"decoder_output: {decoder_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability Distribution Prediction Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of target_probability_distributions: torch.Size([2, 4, 6])\n",
      "target_probability_distributions: tensor([[[-10.7180,  -4.4458,  -5.5887, -10.0208,  -3.8439,  -0.0376],\n",
      "         [ -2.4640,  -1.3492,  -2.1765,  -0.6614,  -4.6119,  -4.1387],\n",
      "         [ -1.5124,  -6.6308,  -2.7857,  -0.3349,  -7.4957,  -7.3191],\n",
      "         [ -3.7088,  -2.0425,  -1.4831,  -0.6633,  -2.7351,  -3.2484]],\n",
      "\n",
      "        [[ -3.0387,  -3.7377,  -2.1507,  -4.2689,  -2.2149,  -0.3729],\n",
      "         [ -3.4303,  -2.7862,  -2.3919,  -0.2166,  -5.6044,  -5.1828],\n",
      "         [ -5.1930,  -4.8207,  -2.5420,  -6.9290,  -1.0418,  -0.5908],\n",
      "         [ -3.6386,  -1.8577,  -2.1418,  -0.4184,  -3.3989,  -4.7402]]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "target_probability_distributions = token_predictor(decoder_output)\n",
    "print(f\"shape of target_probability_distributions: {target_probability_distributions.shape}\")\n",
    "print(f\"target_probability_distributions: {target_probability_distributions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets combine all the code from above into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now create the Transformer model by logically combining all the components we have created so far.\n",
    "class MachineTranslationModel(nn.Module):\n",
    "    \"\"\"Model that combines the Encoder, Decoder and the TokenPredictor to create a machine translation Transformer model.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 d_model: int, \n",
    "                 d_feed_forward: int, \n",
    "                 dropout_prob: float, \n",
    "                 num_heads: int, \n",
    "                 src_vocab_size: int, \n",
    "                 tgt_vocab_size: int, \n",
    "                 num_layers: int, \n",
    "                 max_seq_len: int):\n",
    "        \"\"\"Initializes the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): size of the embedding vectors in the model.\n",
    "            d_feed_forward (int): Number of neurons in the hidden layer of the feed forward neural network.\n",
    "            dropout_prob (float): probability with which to drop data for regularization in the transformer model.\n",
    "            num_heads (int): number of attention heads in each of the multi-head attention layers in the model.\n",
    "            src_vocab_size (int): size of the source vocabulary.\n",
    "            tgt_vocab_size (int): size of the target vocabulary.\n",
    "            num_layers (int): number of layers in the Encoder and Decoder.\n",
    "            max_seq_len (int): Maximum length of the sequence that is ever input to the model.\n",
    "        \"\"\"\n",
    "        super(MachineTranslationModel, self).__init__()\n",
    "        self.src_embedding = Embeddings(vocab_size=src_vocab_size, embedding_dim=d_model)\n",
    "        self.tgt_embedding = Embeddings(vocab_size=tgt_vocab_size, embedding_dim=d_model)\n",
    "        # We have to create two instances of the PositionalEncoding since PositionalEncoding module has a Dropout layer\n",
    "        # and is applied independently in both the cases.\n",
    "        self.src_positional_encoding = PositionalEncoding(encoding_size=d_model, dropout_prob=dropout_prob, max_len=max_seq_len)\n",
    "        self.tgt_positional_encoding = PositionalEncoding(encoding_size=d_model, dropout_prob=dropout_prob, max_len=max_seq_len)\n",
    "        # Note that multi_headed_attention, feed_forward_nn, encoder_layer and decoder_layer are not child modules of\n",
    "        # the MachineTranslationModel class. They are just variables that are used to create the child modules of the\n",
    "        # MachineTranslationModel class.\n",
    "        multi_headed_attention = MultiHeadedAttention(num_heads=num_heads, d_model=d_model, dropout_prob=dropout_prob)\n",
    "        feed_forward_nn = FeedForwardNN(d_model=d_model, d_feed_forward=d_feed_forward, dropout_prob=dropout_prob)\n",
    "        encoder_layer = EncoderLayer(self_attention=copy.deepcopy(multi_headed_attention), \n",
    "                                     feed_forward=copy.deepcopy(feed_forward_nn), \n",
    "                                     d_model=d_model, \n",
    "                                     dropout_prob=dropout_prob)\n",
    "        decoder_layer = DecoderLayer(self_attention=copy.deepcopy(multi_headed_attention), \n",
    "                                     src_attention=copy.deepcopy(multi_headed_attention),\n",
    "                                     feed_forward=copy.deepcopy(feed_forward_nn), \n",
    "                                     d_model=d_model, \n",
    "                                     dropout_prob=dropout_prob)\n",
    "        # encoder, decoder and token_predictor are the child modules of the MachineTranslationModel class.\n",
    "        self.encoder = Encoder(encoder_layer=encoder_layer, num_layers=num_layers)\n",
    "        self.decoder = Decoder(decoder_layer=decoder_layer, num_layers=num_layers)\n",
    "        self.token_predictor = TokenPredictor(d_model=d_model, tgt_vocab_size=tgt_vocab_size)\n",
    "        self.initialize_model_parameters()\n",
    "\n",
    "    def initialize_model_parameters(self):\n",
    "        \"\"\"Initializes the parameters of the model using the Xavier Uniform initialization.\"\"\"\n",
    "        for params in self.parameters():\n",
    "            # This is to ensure the only the weights are initialized and not the biases. biases usually have only\n",
    "            # one dimension and the weights have more than one dimension. biases are usually initialized to zero.\n",
    "            if params.dim() > 1:\n",
    "                nn.init.xavier_uniform_(params)\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor, tgt_mask: Tensor) -> Tensor:\n",
    "        \"\"\"The forward pass of the Transformer model. The source sentences are passed through the Encoder and the target\n",
    "           sentences are passed through the Decoder. The output of the Decoder is passed through the token predictor to\n",
    "           get the probability distribution over the target vocabulary.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Source sequences (English) containing the token ids corresponding to the indices in the src vocabulary. \n",
    "                          Example input looks like [[0, 4, 55, 67, 1, 2, 2], [0, 42, 585, 967, 19, 26, 1]]\n",
    "                          SHAPE: [batch_size, src_seq_len]\n",
    "            tgt (Tensor): Target sequences (Telugu) containing the token ids corresponding to the indices in the tgt vocabulary. \n",
    "                          Example input looks like [[0, 3, 5, 677, 81, 1, 2], [0, 7, 67, 190, 3245, 1]]\n",
    "                          SHAPE: [batch_size, tgt_seq_len]\n",
    "            src_mask (Tensor): Mask to be applied to the source sequences in each of the attention heads.\n",
    "                               src_mask: [batch_size, 1, src_seq_len, src_seq_len]\n",
    "            tgt_mask (Tensor): Mask to be applied to the target sequences in each of the attention heads.\n",
    "                               tgt_mask: [batch_size, 1, tgt_seq_len - 1, tgt_seq_len - 1]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Log probability distribution over the tokens in the target vocabulary (Telugu vocabulary).\n",
    "                    SHAPE: [batch_size, tgt_seq_len - 1, tgt_vocab_size]\n",
    "        \"\"\"\n",
    "        # Pass the source sentences through the encoder to get the encoded source token vectors.\n",
    "        encoded_src = self.encode(src=src, src_mask=src_mask)\n",
    "        # Pass the target sentence through the decoder to get the encoded target token vectors.\n",
    "        decoded_tgt = self.decode(tgt=tgt, tgt_mask=tgt_mask, encoded_src=encoded_src, src_mask=src_mask)\n",
    "        return self.generate_tgt_token_prob_distributions(decoded_tgt=decoded_tgt)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"Encodes the source sentences (English).\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): A batch of source sequences containing the token ids corresponding to the indices in the src vocabulary.\n",
    "                          SHAPE: [batch_size, src_seq_len]\n",
    "            src_mask (Tensor): Mask to be applied to the source sequences in each of the attention heads. Same mask will be \n",
    "                               applied to the sequence in all the attention heads.\n",
    "                               SHAPE: [batch_size, 1, 1, src_seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Encoded source sequences. Each token in the source sequence is represented by a vector that encodes\n",
    "                    all the information about the token and its relationship with other tokens in the sequence.\n",
    "                    SHAPE: [batch_size, src_seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Get the embeddings for the source sentences.\n",
    "        src_embeddings = self.src_embedding(src)\n",
    "        # Add the positional encodings to the embeddings.\n",
    "        src_embeddings = self.src_positional_encoding(src_embeddings)\n",
    "        # Pass the source sentence through the encoder.\n",
    "        encoded_src = self.encoder(input=src_embeddings, mask=src_mask)\n",
    "        return encoded_src\n",
    "\n",
    "    def decode(self, tgt: Tensor, encoded_src: Tensor, src_mask: Tensor, tgt_mask: Tensor) -> Tensor:\n",
    "        \"\"\"Encodes each token in the target sequence using the information from the source sequences and the \n",
    "        assocations between tokens in the target sequences.\n",
    "\n",
    "        Args:\n",
    "            tgt (Tensor): A batch of target sequences containing the token ids corresponding to the indices in the tgt vocabulary.\n",
    "                          SHAPE: [batch_size, tgt_seq_len]\n",
    "            encoded_src (Tensor): The encoded token representations of the source sequences. This is used to calculate the\n",
    "                                  source attention scores for the target sentence.\n",
    "                                  SHAPE: [batch_size, src_seq_len, d_model]\n",
    "            src_mask (Tensor): Mask to be applied to the source sequences in each of the attention heads. Same mask will be \n",
    "                               applied to the sequence in all the attention heads.\n",
    "                               SHAPE: [batch_size, 1, src_seq_len, src_seq_len]\n",
    "            tgt_mask (Tensor): Mask to be applied to the target sequences in each of the attention heads. Same mask will be \n",
    "                                         applied to the sequence in all the attention heads.\n",
    "                                         SHAPE: [batch_size, 1, tgt_seq_len - 1, tgt_seq_len - 1]\n",
    "                               \n",
    "        Returns:\n",
    "            Tensor: Encoded (or Decoded if that makes more sense to you) target sequences. Each token in the target \n",
    "                    sequence is represented by a vector that encodes all the information about the token and its \n",
    "                    relationship with other tokens in the target sequence and the corresponding source sequences.\n",
    "                    SHAPE: [batch_size, tgt_seq_len - 1, d_model]\n",
    "        \"\"\"\n",
    "        # Get the embeddings for the target sequences.\n",
    "        tgt_embeddings = self.tgt_embedding(tgt)\n",
    "        # Add the positional encodings to the embeddings.\n",
    "        tgt_embeddings = self.tgt_positional_encoding(tgt_embeddings)\n",
    "        # Pass the target sequence through the decoder.\n",
    "        decoded_tgt = self.decoder(input=tgt_embeddings, encoded_src=encoded_src, tgt_mask=tgt_mask, src_mask=src_mask)\n",
    "        return decoded_tgt\n",
    "\n",
    "    def generate_tgt_token_prob_distributions(self, decoded_tgt: Tensor) -> Tensor:\n",
    "        \"\"\"Takes the output of the decoder and generates the probability distribution for each token over the target vocabulary.\n",
    "\n",
    "        Args:\n",
    "            decoded_tgt (Tensor): The output of the decoder. Each token in the target sequence is represented by a vector that\n",
    "                                  encodes all the information about the token and its relationship with other tokens in the \n",
    "                                  target sequence and the corresponding source sequences.\n",
    "                                  SHAPE: [batch_size, tgt_seq_len - 1, d_model]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Log probability distribution over the tokens in the target vocabulary (Telugu vocabulary in this case).\n",
    "        \"\"\"\n",
    "        # Convert the output of the decoder to the probability distribution over the target vocabulary. This will be\n",
    "        # used to calculate the loss in the training phase.\n",
    "        return self.token_predictor(decoded_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MachineTranslationModel(\n",
      "  (src_embedding): Embeddings(\n",
      "    (look_up_table): Embedding(6, 8)\n",
      "  )\n",
      "  (tgt_embedding): Embeddings(\n",
      "    (look_up_table): Embedding(6, 8)\n",
      "  )\n",
      "  (src_positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (tgt_positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0-5): 6 x EncoderLayer(\n",
      "        (self_attention): MultiHeadedAttention(\n",
      "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "          (linear_layers): ModuleList(\n",
      "            (0-3): 4 x Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (feed_forward): FeedForwardNN(\n",
      "          (linear_layer_1): Linear(in_features=8, out_features=16, bias=True)\n",
      "          (linear_layer_2): Linear(in_features=16, out_features=8, bias=True)\n",
      "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (sublayer_wrappers): ModuleList(\n",
      "          (0-1): 2 x SubLayerWrapper(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (decoder_layers): ModuleList(\n",
      "      (0-5): 6 x DecoderLayer(\n",
      "        (self_attention): MultiHeadedAttention(\n",
      "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "          (linear_layers): ModuleList(\n",
      "            (0-3): 4 x Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (src_attention): MultiHeadedAttention(\n",
      "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "          (linear_layers): ModuleList(\n",
      "            (0-3): 4 x Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (feed_forward): FeedForwardNN(\n",
      "          (linear_layer_1): Linear(in_features=8, out_features=16, bias=True)\n",
      "          (linear_layer_2): Linear(in_features=16, out_features=8, bias=True)\n",
      "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (sublayer_wrappers): ModuleList(\n",
      "          (0-2): 3 x SubLayerWrapper(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (token_predictor): TokenPredictor(\n",
      "    (linear): Linear(in_features=8, out_features=6, bias=True)\n",
      "    (log_softmax): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the MachineTranslationTransformer model.\n",
    "machine_translation_transformer = MachineTranslationModel(d_model=d_model, \n",
    "                                                          d_feed_forward=d_feed_forward,\n",
    "                                                          dropout_prob=dropout_prob, \n",
    "                                                          num_heads=num_heads, \n",
    "                                                          src_vocab_size=src_vocab_size, \n",
    "                                                          tgt_vocab_size=tgt_vocab_size, \n",
    "                                                          num_layers=num_layers,\n",
    "                                                          max_seq_len=max_seq_len)\n",
    "print(machine_translation_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  decoded_tgt\n",
      "shape:  torch.Size([2, 4, 6])\n",
      "tensor([[[-2.8528, -1.0213, -4.5495, -1.1803, -2.0719, -1.9770],\n",
      "         [-2.2348, -1.3514, -5.0215, -0.7608, -2.2693, -2.8676],\n",
      "         [-2.8250, -0.5687, -4.7799, -1.3647, -2.7904, -3.0120],\n",
      "         [-2.9324, -1.1844, -5.0353, -0.6135, -3.3147, -2.8737]],\n",
      "\n",
      "        [[-2.4639, -0.3513, -4.3406, -2.1864, -3.0747, -3.2306],\n",
      "         [-2.1477, -0.6888, -4.7810, -1.4282, -2.4217, -3.1191],\n",
      "         [-3.2905, -0.5929, -4.4174, -1.5028, -2.5175, -2.3557],\n",
      "         [-3.1885, -0.5061, -4.4754, -1.2970, -3.2141, -3.4739]]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Pass the input data through the model to get the output.\n",
    "decoded_tgt = machine_translation_transformer(src=src, tgt=tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "PrintTensor(input=decoded_tgt, name=\"decoded_tgt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets verify if the number of parameters in the model is the same as what we expect.\n",
    "\n",
    "Counting the number of parameters manually and comparing it with the number of learnable parameters in the <br>\n",
    "model helps debug lots of issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_params:  9206\n",
      "total_params_with_grad:  9206\n",
      "total_params_without_grad:  0\n"
     ]
    }
   ],
   "source": [
    "# Finding out the number of parameters in the build model.\n",
    "total_params = sum(params.numel() for params in machine_translation_transformer.parameters())\n",
    "print(\"total_params: \", total_params)\n",
    "total_params_with_grad = sum(params.numel() for params in machine_translation_transformer.parameters() if params.requires_grad)\n",
    "print(\"total_params_with_grad: \", total_params_with_grad)\n",
    "total_params_without_grad = sum(params.numel() for params in machine_translation_transformer.parameters() if not params.requires_grad)\n",
    "print(\"total_params_without_grad: \", total_params_without_grad)\n",
    "assert total_params == total_params_with_grad + total_params_without_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_embedding.look_up_table.weight   48\n",
      "tgt_embedding.look_up_table.weight   48\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.0.weight   64\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.0.bias   8\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.1.weight   64\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.1.bias   8\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.2.weight   64\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.2.bias   8\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.3.weight   64\n",
      "encoder.encoder_layers.0.self_attention.linear_layers.3.bias   8\n",
      "encoder.encoder_layers.0.feed_forward.linear_layer_1.weight   128\n",
      "encoder.encoder_layers.0.feed_forward.linear_layer_1.bias   16\n",
      "encoder.encoder_layers.0.feed_forward.linear_layer_2.weight   128\n",
      "encoder.encoder_layers.0.feed_forward.linear_layer_2.bias   8\n",
      "encoder.encoder_layers.0.sublayer_wrappers.0.layer_norm.weight   8\n",
      "encoder.encoder_layers.0.sublayer_wrappers.0.layer_norm.bias   8\n",
      "encoder.encoder_layers.0.sublayer_wrappers.1.layer_norm.weight   8\n",
      "encoder.encoder_layers.0.sublayer_wrappers.1.layer_norm.bias   8\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.0.weight   64\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.0.bias   8\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.1.weight   64\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.1.bias   8\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.2.weight   64\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.2.bias   8\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.3.weight   64\n",
      "encoder.encoder_layers.1.self_attention.linear_layers.3.bias   8\n",
      "encoder.encoder_layers.1.feed_forward.linear_layer_1.weight   128\n",
      "encoder.encoder_layers.1.feed_forward.linear_layer_1.bias   16\n",
      "encoder.encoder_layers.1.feed_forward.linear_layer_2.weight   128\n",
      "encoder.encoder_layers.1.feed_forward.linear_layer_2.bias   8\n",
      "encoder.encoder_layers.1.sublayer_wrappers.0.layer_norm.weight   8\n",
      "encoder.encoder_layers.1.sublayer_wrappers.0.layer_norm.bias   8\n",
      "encoder.encoder_layers.1.sublayer_wrappers.1.layer_norm.weight   8\n",
      "encoder.encoder_layers.1.sublayer_wrappers.1.layer_norm.bias   8\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.0.weight   64\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.0.bias   8\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.1.weight   64\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.1.bias   8\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.2.weight   64\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.2.bias   8\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.3.weight   64\n",
      "encoder.encoder_layers.2.self_attention.linear_layers.3.bias   8\n",
      "encoder.encoder_layers.2.feed_forward.linear_layer_1.weight   128\n",
      "encoder.encoder_layers.2.feed_forward.linear_layer_1.bias   16\n",
      "encoder.encoder_layers.2.feed_forward.linear_layer_2.weight   128\n",
      "encoder.encoder_layers.2.feed_forward.linear_layer_2.bias   8\n",
      "encoder.encoder_layers.2.sublayer_wrappers.0.layer_norm.weight   8\n",
      "encoder.encoder_layers.2.sublayer_wrappers.0.layer_norm.bias   8\n",
      "encoder.encoder_layers.2.sublayer_wrappers.1.layer_norm.weight   8\n",
      "encoder.encoder_layers.2.sublayer_wrappers.1.layer_norm.bias   8\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.0.weight   64\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.0.bias   8\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.1.weight   64\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.1.bias   8\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.2.weight   64\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.2.bias   8\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.3.weight   64\n",
      "encoder.encoder_layers.3.self_attention.linear_layers.3.bias   8\n",
      "encoder.encoder_layers.3.feed_forward.linear_layer_1.weight   128\n",
      "encoder.encoder_layers.3.feed_forward.linear_layer_1.bias   16\n",
      "encoder.encoder_layers.3.feed_forward.linear_layer_2.weight   128\n",
      "encoder.encoder_layers.3.feed_forward.linear_layer_2.bias   8\n",
      "encoder.encoder_layers.3.sublayer_wrappers.0.layer_norm.weight   8\n",
      "encoder.encoder_layers.3.sublayer_wrappers.0.layer_norm.bias   8\n",
      "encoder.encoder_layers.3.sublayer_wrappers.1.layer_norm.weight   8\n",
      "encoder.encoder_layers.3.sublayer_wrappers.1.layer_norm.bias   8\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.0.weight   64\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.0.bias   8\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.1.weight   64\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.1.bias   8\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.2.weight   64\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.2.bias   8\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.3.weight   64\n",
      "encoder.encoder_layers.4.self_attention.linear_layers.3.bias   8\n",
      "encoder.encoder_layers.4.feed_forward.linear_layer_1.weight   128\n",
      "encoder.encoder_layers.4.feed_forward.linear_layer_1.bias   16\n",
      "encoder.encoder_layers.4.feed_forward.linear_layer_2.weight   128\n",
      "encoder.encoder_layers.4.feed_forward.linear_layer_2.bias   8\n",
      "encoder.encoder_layers.4.sublayer_wrappers.0.layer_norm.weight   8\n",
      "encoder.encoder_layers.4.sublayer_wrappers.0.layer_norm.bias   8\n",
      "encoder.encoder_layers.4.sublayer_wrappers.1.layer_norm.weight   8\n",
      "encoder.encoder_layers.4.sublayer_wrappers.1.layer_norm.bias   8\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.0.weight   64\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.0.bias   8\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.1.weight   64\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.1.bias   8\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.2.weight   64\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.2.bias   8\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.3.weight   64\n",
      "encoder.encoder_layers.5.self_attention.linear_layers.3.bias   8\n",
      "encoder.encoder_layers.5.feed_forward.linear_layer_1.weight   128\n",
      "encoder.encoder_layers.5.feed_forward.linear_layer_1.bias   16\n",
      "encoder.encoder_layers.5.feed_forward.linear_layer_2.weight   128\n",
      "encoder.encoder_layers.5.feed_forward.linear_layer_2.bias   8\n",
      "encoder.encoder_layers.5.sublayer_wrappers.0.layer_norm.weight   8\n",
      "encoder.encoder_layers.5.sublayer_wrappers.0.layer_norm.bias   8\n",
      "encoder.encoder_layers.5.sublayer_wrappers.1.layer_norm.weight   8\n",
      "encoder.encoder_layers.5.sublayer_wrappers.1.layer_norm.bias   8\n",
      "encoder.layer_norm.weight   8\n",
      "encoder.layer_norm.bias   8\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.0.self_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.0.src_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.0.feed_forward.linear_layer_1.weight   128\n",
      "decoder.decoder_layers.0.feed_forward.linear_layer_1.bias   16\n",
      "decoder.decoder_layers.0.feed_forward.linear_layer_2.weight   128\n",
      "decoder.decoder_layers.0.feed_forward.linear_layer_2.bias   8\n",
      "decoder.decoder_layers.0.sublayer_wrappers.0.layer_norm.weight   8\n",
      "decoder.decoder_layers.0.sublayer_wrappers.0.layer_norm.bias   8\n",
      "decoder.decoder_layers.0.sublayer_wrappers.1.layer_norm.weight   8\n",
      "decoder.decoder_layers.0.sublayer_wrappers.1.layer_norm.bias   8\n",
      "decoder.decoder_layers.0.sublayer_wrappers.2.layer_norm.weight   8\n",
      "decoder.decoder_layers.0.sublayer_wrappers.2.layer_norm.bias   8\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.1.self_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.1.src_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.1.feed_forward.linear_layer_1.weight   128\n",
      "decoder.decoder_layers.1.feed_forward.linear_layer_1.bias   16\n",
      "decoder.decoder_layers.1.feed_forward.linear_layer_2.weight   128\n",
      "decoder.decoder_layers.1.feed_forward.linear_layer_2.bias   8\n",
      "decoder.decoder_layers.1.sublayer_wrappers.0.layer_norm.weight   8\n",
      "decoder.decoder_layers.1.sublayer_wrappers.0.layer_norm.bias   8\n",
      "decoder.decoder_layers.1.sublayer_wrappers.1.layer_norm.weight   8\n",
      "decoder.decoder_layers.1.sublayer_wrappers.1.layer_norm.bias   8\n",
      "decoder.decoder_layers.1.sublayer_wrappers.2.layer_norm.weight   8\n",
      "decoder.decoder_layers.1.sublayer_wrappers.2.layer_norm.bias   8\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.2.self_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.2.src_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.2.feed_forward.linear_layer_1.weight   128\n",
      "decoder.decoder_layers.2.feed_forward.linear_layer_1.bias   16\n",
      "decoder.decoder_layers.2.feed_forward.linear_layer_2.weight   128\n",
      "decoder.decoder_layers.2.feed_forward.linear_layer_2.bias   8\n",
      "decoder.decoder_layers.2.sublayer_wrappers.0.layer_norm.weight   8\n",
      "decoder.decoder_layers.2.sublayer_wrappers.0.layer_norm.bias   8\n",
      "decoder.decoder_layers.2.sublayer_wrappers.1.layer_norm.weight   8\n",
      "decoder.decoder_layers.2.sublayer_wrappers.1.layer_norm.bias   8\n",
      "decoder.decoder_layers.2.sublayer_wrappers.2.layer_norm.weight   8\n",
      "decoder.decoder_layers.2.sublayer_wrappers.2.layer_norm.bias   8\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.3.self_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.3.src_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.3.feed_forward.linear_layer_1.weight   128\n",
      "decoder.decoder_layers.3.feed_forward.linear_layer_1.bias   16\n",
      "decoder.decoder_layers.3.feed_forward.linear_layer_2.weight   128\n",
      "decoder.decoder_layers.3.feed_forward.linear_layer_2.bias   8\n",
      "decoder.decoder_layers.3.sublayer_wrappers.0.layer_norm.weight   8\n",
      "decoder.decoder_layers.3.sublayer_wrappers.0.layer_norm.bias   8\n",
      "decoder.decoder_layers.3.sublayer_wrappers.1.layer_norm.weight   8\n",
      "decoder.decoder_layers.3.sublayer_wrappers.1.layer_norm.bias   8\n",
      "decoder.decoder_layers.3.sublayer_wrappers.2.layer_norm.weight   8\n",
      "decoder.decoder_layers.3.sublayer_wrappers.2.layer_norm.bias   8\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.4.self_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.4.src_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.4.feed_forward.linear_layer_1.weight   128\n",
      "decoder.decoder_layers.4.feed_forward.linear_layer_1.bias   16\n",
      "decoder.decoder_layers.4.feed_forward.linear_layer_2.weight   128\n",
      "decoder.decoder_layers.4.feed_forward.linear_layer_2.bias   8\n",
      "decoder.decoder_layers.4.sublayer_wrappers.0.layer_norm.weight   8\n",
      "decoder.decoder_layers.4.sublayer_wrappers.0.layer_norm.bias   8\n",
      "decoder.decoder_layers.4.sublayer_wrappers.1.layer_norm.weight   8\n",
      "decoder.decoder_layers.4.sublayer_wrappers.1.layer_norm.bias   8\n",
      "decoder.decoder_layers.4.sublayer_wrappers.2.layer_norm.weight   8\n",
      "decoder.decoder_layers.4.sublayer_wrappers.2.layer_norm.bias   8\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.5.self_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.0.weight   64\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.0.bias   8\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.1.weight   64\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.1.bias   8\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.2.weight   64\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.2.bias   8\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.3.weight   64\n",
      "decoder.decoder_layers.5.src_attention.linear_layers.3.bias   8\n",
      "decoder.decoder_layers.5.feed_forward.linear_layer_1.weight   128\n",
      "decoder.decoder_layers.5.feed_forward.linear_layer_1.bias   16\n",
      "decoder.decoder_layers.5.feed_forward.linear_layer_2.weight   128\n",
      "decoder.decoder_layers.5.feed_forward.linear_layer_2.bias   8\n",
      "decoder.decoder_layers.5.sublayer_wrappers.0.layer_norm.weight   8\n",
      "decoder.decoder_layers.5.sublayer_wrappers.0.layer_norm.bias   8\n",
      "decoder.decoder_layers.5.sublayer_wrappers.1.layer_norm.weight   8\n",
      "decoder.decoder_layers.5.sublayer_wrappers.1.layer_norm.bias   8\n",
      "decoder.decoder_layers.5.sublayer_wrappers.2.layer_norm.weight   8\n",
      "decoder.decoder_layers.5.sublayer_wrappers.2.layer_norm.bias   8\n",
      "decoder.layer_norm.weight   8\n",
      "decoder.layer_norm.bias   8\n",
      "token_predictor.linear.weight   48\n",
      "token_predictor.linear.bias   6\n"
     ]
    }
   ],
   "source": [
    "# Prints out all the layers and the number of parameters in each layer.\n",
    "for name, params in machine_translation_transformer.named_parameters():\n",
    "    print(name, \" \", params.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the number of trainable parameters in the transformer model manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of parameters associated with the model:  9206\n"
     ]
    }
   ],
   "source": [
    "# Lets try to count the number of parameters in the model manually by going through each component and counting \n",
    "# parameters. \n",
    "# The number of parameters associated with the Embeddings class for src sentences. We have 1 embedding vector \n",
    "# per token in the source vocabulary. We have 6 tokens and each token is represented by an 8-dimensional vector. \n",
    "# So, the total number of parameters associated with the Embeddings class for src sentences is 6 * 8 = 48.\n",
    "num_src_embedding_params = src_vocab_size * d_model\n",
    "# The number of parameters associated with the Embeddings class for tgt sentences. We have 1 embedding vector\n",
    "# per token in the target vocabulary. We have 6 tokens and each token is represented by an 8-dimensional vector.\n",
    "# So, the total number of parameters associated with the Embeddings class for tgt sentences is 6 * 8 = 48.\n",
    "num_tgt_embedding_params = tgt_vocab_size * d_model\n",
    "# There are no parameters associated with the PositionalEncoding class. These are calculated based on a predefined\n",
    "# formula and are not learned during the training process.\n",
    "num_positional_encoding_params = 0\n",
    "# Now, lets calculate the number of parameters associated with the Encoder class. The Encoder class has 6 \n",
    "# identical EncoderLayers stacked on top of each other. Lets calculate the number of parameters associated with \n",
    "# each EncoderLayer class. The EncoderLayer class has MultiHeadedAttention and FeedForwardNN classes as its child \n",
    "# classes. Each MultiHeadedAttention class has 4 linear layers (query, key, value and output). Note that a single \n",
    "# linear layer is used to calculate the queries, keys, values and outputs for all the heads. So, we don't need to \n",
    "# do this calculation for each head separately. Lets take the linear layer associated with the query calculation. \n",
    "# The input to this linear layer is a 8-dimensional vector (d_model) and the output is also an 8-dimensional vector \n",
    "# (d_model=8). So, the number of parameters in this linear layer associated with the weight matrix is 8 * 8 = 64. \n",
    "# We also have bias terms (d_model=8) associated with this linear layer. So, the total number of parameters \n",
    "# associated with the query linear layer is 64 + 8 = 72.\n",
    "num_encoder_query_params = d_model * d_model + d_model\n",
    "num_encoder_key_params = d_model * d_model + d_model\n",
    "num_encoder_value_params = d_model * d_model + d_model\n",
    "num_encoder_attention_output_params = d_model * d_model + d_model\n",
    "# Now lets calculate the number of parameters associated with FeedForward neural network class in the EncoderLayer. \n",
    "# The first linear layer in the feed forward expands the input to a higher dimension (d_model to d_feed_forward). \n",
    "# The input to this linear layer is a 8-dimensional vector (d_model) and the output is a 16-dimensional vector \n",
    "# (d_feed_forward). So, the number of parameters in this linear layer associated with the weight matrix is \n",
    "# 8 * 16 = 128. We also have bias terms (d_feed_forward=16) associated with this linear layer. So, the total number \n",
    "# of parameters associated with the first linear layer in the feed forward neural network is 128 + 16 = 144. The \n",
    "# second linear layer in the feed forward neural network compresses the input back to its original dimension \n",
    "# (d_feed_forward to d_model). The input to this linear layer is a 16-dimensional vector (d_feed_forward) and the \n",
    "# output is an 8-dimensional vector (d_model). So, the number of parameters in this linear layer associated with \n",
    "# the weight matrix is 16 * 8 = 128. We also have bias terms (d_model=8) associated with this linear layer. So, \n",
    "# the total number of parameters associated with the second linear layer in the feed forward neural network is \n",
    "# 128 + 8 = 136.\n",
    "num_encoder_feed_forward_linear_layer_1_params = d_model * d_feed_forward + d_feed_forward\n",
    "num_encoder_feed_forward_linear_layer_2_params = d_feed_forward * d_model + d_model\n",
    "# The output of MultiHeadedAttention and FeedForward neural network is normalized using Layer Normalization. Layer\n",
    "# Normalization is applied along the last dimension of the input tensor (input to Layer Normalization). Each of the\n",
    "# features is scaled independently with the learned paramaters. So, the number of parameters is the number of \n",
    "# features in the last dimension multiplied by 2 (1 parameter for gamma and 1 parameter for beta per feature). Both \n",
    "# the output of MultiHeadedAttention and FeedForward neural network have the same size in the last dimension which \n",
    "# is 8 (d_model). So, the number of parameters associated with Layer Normalization layer that is applied after \n",
    "# MultiHeadedAttention is 8 (gamma) + 8 (beta) = 16. Similarly, the number of parameters associated with Layer\n",
    "# Normalization layer that is applied after FeedForward neural network is 8 (gamma) + 8 (beta) = 16.\n",
    "num_encoder_attention_layer_norm_params = d_model + d_model\n",
    "num_encoder_feed_forward_layer_norm_params = d_model + d_model\n",
    "# The total number of parameters associated with a single EncoderLayer is sum of the above 8 variables = 600.\n",
    "num_encoder_layer_params = num_encoder_query_params + num_encoder_key_params + num_encoder_value_params + num_encoder_attention_output_params + num_encoder_feed_forward_linear_layer_1_params + num_encoder_feed_forward_linear_layer_2_params + num_encoder_attention_layer_norm_params + num_encoder_feed_forward_layer_norm_params\n",
    "# We also apply Layer Normalization to the output of the last EncoderLayer and pass this as the output of the Encoder.\n",
    "# This layer again has same number of parameters associated with it as other Layer Normalization layers i.e.,\n",
    "# 8 (gamma) + 8 (beta) = 16\n",
    "num_encoder_layer_layer_norm_params = d_model + d_model\n",
    "# The transformer model has 6 EncoderLayers stacked on top of each other. So, the total number of parameters \n",
    "# associated with Encoder is (6 * 600) + 16 = 3616\n",
    "num_total_encoder_params = (num_layers * num_encoder_layer_params) + num_encoder_layer_layer_norm_params\n",
    "# Now, lets calculate the number of parameters associated with the DecoderLayer and Decoder.\n",
    "# The method to calculate number of parameters in the DecoderLayer is very similar to how it was done for the \n",
    "# EncoderLayer. DecoderLayer just contains 1 additional MultiHeadedAttention Layer (for source attention) and 1\n",
    "# additional Layer Normalization layer associated with this source attention layer.\n",
    "# Same as in EncoderLayer ==> Linear Layer parameters ==> 64 (weights) + 8 (bias) = 72\n",
    "num_decoder_self_attention_query_params = d_model * d_model + d_model\n",
    "num_decoder_self_attention_key_params = d_model * d_model + d_model\n",
    "num_decoder_self_attention_value_params = d_model * d_model + d_model\n",
    "num_decoder_self_attention_output_params = d_model * d_model + d_model\n",
    "# The next 4 variables correspond to the 1 additional MultiHeadedAttention layer (src attention) present in the \n",
    "# DecoderLayer.\n",
    "# Same as in EncoderLayer ==> Linear Layer parameters ==> 64 (weights) + 8 (bias) = 72\n",
    "num_decoder_src_attention_query_params = d_model * d_model + d_model\n",
    "num_decoder_src_attention_key_params = d_model * d_model + d_model\n",
    "num_decoder_src_attention_value_params = d_model * d_model + d_model\n",
    "num_decoder_src_attention_output_params = d_model * d_model + d_model\n",
    "# The FeedForward neural network is exactly the same as in EncoderLayer.\n",
    "# Same as in EncoderLayer ==> 8 * 16 + 16 = 144\n",
    "num_decoder_feed_forward_linear_layer_1_params = d_model * d_feed_forward + d_feed_forward\n",
    "# Same as in EncoderLayer ==> 8 * 16 + 8 = 136\n",
    "num_decoder_feed_forward_linear_layer_2_params = d_model * d_feed_forward + d_model\n",
    "# We have 1 additional LayerNormalization layer associated with the source attention. However, its architecture and\n",
    "# the parameters are the same as in EncoderLayer.\n",
    "# Same as in EncoderLayer ==> 8 (gamma) + 8 (beta) = 16\n",
    "num_decoder_self_attention_layer_norm_params = d_model + d_model\n",
    "num_decoder_src_attention_layer_norm_params = d_model + d_model\n",
    "num_decoder_feed_forward_layer_norm_params = d_model + d_model\n",
    "# The total number of parameters associated with a single DecoderLayer is the sum of the above 13 variables = \n",
    "num_decoder_layer_params = num_decoder_self_attention_query_params + num_decoder_self_attention_key_params + num_decoder_self_attention_value_params + num_decoder_self_attention_output_params + num_decoder_src_attention_query_params + num_decoder_src_attention_key_params + num_decoder_src_attention_value_params + num_decoder_src_attention_output_params + num_decoder_feed_forward_linear_layer_1_params + num_decoder_feed_forward_linear_layer_2_params + num_decoder_self_attention_layer_norm_params + num_decoder_src_attention_layer_norm_params + num_decoder_feed_forward_layer_norm_params\n",
    "# We also apply Layer Normalization to the output of the last DecoderLayer and pass this as the output of the Decoder. \n",
    "# This layer again has same number of parameters associated with it as other Layer Normalization layers i.e.,\n",
    "# 8 (gamma) + 8 (beta) = 16\n",
    "num_decoder_layer_layer_norm_params = d_model + d_model\n",
    "# The transformer model has 6 EncoderLayers stacked on top of each other. So, the total number of parameters \n",
    "# associated with Encoder is (6 * 904) + 16 = 5440\n",
    "num_total_decoder_params = (num_layers * num_decoder_layer_params) + num_decoder_layer_layer_norm_params\n",
    "# The output of the decoder is passed to a linear layer that projects the output to the target vocabulary space.\n",
    "# These parameters are associated with the TokenPredictor layer in the transformer above. The input to the linear\n",
    "# layer are 8-dimensional vectors (d_model) and output of the linear layers are 6-dimensional vectors (tgt_vocab_size).\n",
    "# So, the number of parameters associated with the TokenPredictor layer is 8 * 6 (weights) + 6 (bias) = 54\n",
    "num_vocab_projection_params = d_model * tgt_vocab_size + tgt_vocab_size\n",
    "# Finally, the total number of parameters in the model is the number of parameters associated with the Embeddings plus\n",
    "# the number of parameters in the Encoder plus the number of parameters in the Decoder plus the number of parameters\n",
    "# in the TokenPredictor.\n",
    "num_total_model_params = num_src_embedding_params + num_tgt_embedding_params + num_total_encoder_params + num_total_decoder_params + num_vocab_projection_params\n",
    "print(\"Total Number of parameters associated with the model: \", num_total_model_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, the number of parameters in the model we built is the same as the number of parameters expected within the model.\n",
    "# Usually, counting the number of parameters could make several bugs in the code visible and a good check to implement\n",
    "# for any machine learning model.\n",
    "assert total_params_with_grad == num_total_model_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
