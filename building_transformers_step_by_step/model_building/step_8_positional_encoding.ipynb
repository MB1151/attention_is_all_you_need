{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook you learn:\n",
    "#\n",
    "# 1) How to create positional encodings as described in 'Attention Is All You Need' paper?\n",
    "#\n",
    "# NOTE: Intuitively, we don't need to do all this messy stuff (as added below) to calculate positional encodings. We \n",
    "# can just run two for loops to calculate the positional encodings according to the formula given in the paper. But,\n",
    "# we are doing this to calculate the positional encodings in a vectorized way and also avoid numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources to understand positional encoding:\n",
    "#\n",
    "# 1) https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\n",
    "# 2) https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model\n",
    "# 3) https://www.youtube.com/watch?v=dichIcUZfOw\n",
    "# 4) https://kazemnejad.com/blog/transformer_architecture_positional_encoding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encoding is a way to inject information about the position of the token in the sequence.\n",
    "#\n",
    "# In the 'Attention Is All You Need' paper, the authors used the following formula to calculate the \n",
    "# positional encoding:\n",
    "#   -- PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "#   -- PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "#\n",
    "# where: \n",
    "# 'pos' is the position of the token in the sequence (0, 1, 2, 3, ..., sentence_length - 1).\n",
    "# 'i' is the index in the positional encoding vector (0, 1, 2, 3, ..., 254, 255).\n",
    "# 'd_model' is the size of the positional encoding vector (512)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the maximum length of the sentence that we expect as input to the model.\n",
    "max_len = 20\n",
    "# This is the size of the positional encoding vector.\n",
    "d_model = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (10000^(2i/d_model)) involves the calculation of the power of 10000 to (2i/d_model) which can be a very\n",
    "# small number. Calculations involving the large numbers and small numbers can lead to numerical instability.\n",
    "# To avoid these numbers, we will calculate positional encodings in the log space.\n",
    "#\n",
    "# Resources to understand more about representing floating point numbers in computers and rounding errors:\n",
    "#\n",
    "# https://softwareengineering.stackexchange.com/questions/215065/can-anyone-explain-representation-of-float-in-memory\n",
    "#       -- Quick overview of how floats are represented in computers\n",
    "# https://www.youtube.com/watch?v=yvdtwKF87Ts\n",
    "#       -- Quick overview of how floats are represented in computers\n",
    "# https://www.youtube.com/watch?v=PZRI1IfStY0\n",
    "#       -- Explains why floating point rounding errors occur\n",
    "# https://docs.python.org/3/tutorial/floatingpoint.html\n",
    "#       -- How floats are represented in python\n",
    "# https://www.youtube.com/watch?v=m_G3z-C1C2g&t=1s\n",
    "#       -- Two's complement representation of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([20, 16])\n"
     ]
    }
   ],
   "source": [
    "# A tensor (container) to hold the positional encodings until position 19 where the size of each positional encoding \n",
    "# vector is fixed to be 16.\n",
    "positional_encoding = torch.zeros(max_len, d_model)\n",
    "print(positional_encoding)\n",
    "print(positional_encoding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple steps involved in creating positional encoding vectors. We go step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0],\n",
      "        [ 1],\n",
      "        [ 2],\n",
      "        [ 3],\n",
      "        [ 4],\n",
      "        [ 5],\n",
      "        [ 6],\n",
      "        [ 7],\n",
      "        [ 8],\n",
      "        [ 9],\n",
      "        [10],\n",
      "        [11],\n",
      "        [12],\n",
      "        [13],\n",
      "        [14],\n",
      "        [15],\n",
      "        [16],\n",
      "        [17],\n",
      "        [18],\n",
      "        [19]])\n",
      "torch.Size([20, 1])\n"
     ]
    }
   ],
   "source": [
    "# The numerator in the above forumale for Positional Encoding is the position. Here we are considering\n",
    "# positional encoding vectors until 19th position. So, we create a tensor that holds the numerators\n",
    "# until position 19.\n",
    "positional_encoding_numerators = torch.arange(0, max_len).unsqueeze(1)\n",
    "print(positional_encoding_numerators)\n",
    "print(positional_encoding_numerators.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets calculate the denominators from the positional encoding formula above. Just calculating the denominators <br>\n",
    "in log space itself involves multiple steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  2,  4,  6,  8, 10, 12, 14])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# We will pre-calculate the values of 10000^(2i/d_model) for all the values of 'i' in the positional encoding vector. \n",
    "# We will do this calculation in the log space to avoid numerical stability.\n",
    "# \n",
    "# 1 / (10000^(2i/d_model)) = 10000^(-2i/d_model)\n",
    "#                          = exp(log(10000^(-2i/d_model)))\n",
    "#                          = exp(-2i/d_model * log(10000))\n",
    "#                          = exp(2i * (-log(10000) / d_model))\n",
    "# \n",
    "#\n",
    "# Saves the values of 2i for each i from 0 to (d_model - 2) i.e., [0, 14] in this context.\n",
    "numerators_in_exponent = torch.arange(0, d_model, 2)\n",
    "print(numerators_in_exponent)\n",
    "print(numerators_in_exponent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000e+00, 3.1623e-01, 1.0000e-01, 3.1623e-02, 1.0000e-02, 3.1623e-03,\n",
      "        1.0000e-03, 3.1623e-04])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Calculate exp(2i * (-log(10000) / d_model)) which is same as \n",
    "# (10000^(2i/d_model)) \n",
    "# for 2i in {0, 2, 4, 6, 8, 10, 12, 14}\n",
    "positional_encoding_denominators = torch.exp(numerators_in_exponent * (-math.log(10000.0) / d_model))\n",
    "print(positional_encoding_denominators)\n",
    "print(positional_encoding_denominators.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 8.4147e-01,  3.1098e-01,  9.9833e-02,  3.1618e-02,  9.9998e-03,\n",
      "          3.1623e-03,  1.0000e-03,  3.1623e-04],\n",
      "        [ 9.0930e-01,  5.9113e-01,  1.9867e-01,  6.3203e-02,  1.9999e-02,\n",
      "          6.3245e-03,  2.0000e-03,  6.3246e-04],\n",
      "        [ 1.4112e-01,  8.1265e-01,  2.9552e-01,  9.4726e-02,  2.9995e-02,\n",
      "          9.4867e-03,  3.0000e-03,  9.4868e-04],\n",
      "        [-7.5680e-01,  9.5358e-01,  3.8942e-01,  1.2615e-01,  3.9989e-02,\n",
      "          1.2649e-02,  4.0000e-03,  1.2649e-03],\n",
      "        [-9.5892e-01,  9.9995e-01,  4.7943e-01,  1.5746e-01,  4.9979e-02,\n",
      "          1.5811e-02,  5.0000e-03,  1.5811e-03],\n",
      "        [-2.7942e-01,  9.4715e-01,  5.6464e-01,  1.8860e-01,  5.9964e-02,\n",
      "          1.8973e-02,  6.0000e-03,  1.8974e-03],\n",
      "        [ 6.5699e-01,  8.0042e-01,  6.4422e-01,  2.1956e-01,  6.9943e-02,\n",
      "          2.2134e-02,  6.9999e-03,  2.2136e-03],\n",
      "        [ 9.8936e-01,  5.7432e-01,  7.1736e-01,  2.5029e-01,  7.9915e-02,\n",
      "          2.5296e-02,  7.9999e-03,  2.5298e-03],\n",
      "        [ 4.1212e-01,  2.9126e-01,  7.8333e-01,  2.8078e-01,  8.9879e-02,\n",
      "          2.8457e-02,  8.9999e-03,  2.8460e-03],\n",
      "        [-5.4402e-01, -2.0684e-02,  8.4147e-01,  3.1098e-01,  9.9833e-02,\n",
      "          3.1617e-02,  9.9998e-03,  3.1623e-03],\n",
      "        [-9.9999e-01, -3.3057e-01,  8.9121e-01,  3.4088e-01,  1.0978e-01,\n",
      "          3.4778e-02,  1.1000e-02,  3.4785e-03],\n",
      "        [-5.3657e-01, -6.0768e-01,  9.3204e-01,  3.7043e-01,  1.1971e-01,\n",
      "          3.7938e-02,  1.2000e-02,  3.7947e-03],\n",
      "        [ 4.2017e-01, -8.2453e-01,  9.6356e-01,  3.9961e-01,  1.2963e-01,\n",
      "          4.1098e-02,  1.3000e-02,  4.1110e-03],\n",
      "        [ 9.9061e-01, -9.5961e-01,  9.8545e-01,  4.2840e-01,  1.3954e-01,\n",
      "          4.4257e-02,  1.4000e-02,  4.4272e-03],\n",
      "        [ 6.5029e-01, -9.9952e-01,  9.9749e-01,  4.5675e-01,  1.4944e-01,\n",
      "          4.7416e-02,  1.4999e-02,  4.7434e-03],\n",
      "        [-2.8790e-01, -9.4031e-01,  9.9957e-01,  4.8465e-01,  1.5932e-01,\n",
      "          5.0575e-02,  1.5999e-02,  5.0596e-03],\n",
      "        [-9.6140e-01, -7.8785e-01,  9.9166e-01,  5.1206e-01,  1.6918e-01,\n",
      "          5.3733e-02,  1.6999e-02,  5.3758e-03],\n",
      "        [-7.5099e-01, -5.5726e-01,  9.7385e-01,  5.3897e-01,  1.7903e-01,\n",
      "          5.6890e-02,  1.7999e-02,  5.6921e-03],\n",
      "        [ 1.4988e-01, -2.7141e-01,  9.4630e-01,  5.6533e-01,  1.8886e-01,\n",
      "          6.0047e-02,  1.8999e-02,  6.0083e-03]])\n",
      "torch.Size([20, 8])\n"
     ]
    }
   ],
   "source": [
    "# We already have the denominators for the positional encoding formula.\n",
    "# Now, we need to calculate the positional encodings for each position in the sequence:\n",
    "#   -- PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "#   -- PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "#\n",
    "# Refer to 'understanding_pytorch/tensor_manipulations/understanding_tensor_manipulations_part_4.ipynb' (https://github.com/MB1151/understanding_pytorch/blob/main/tensor_manipulations/understanding_tensor_manipulations_part_4.ipynb) \n",
    "# to understand more about broadcasting in python.\n",
    "#\n",
    "# The below element wise multiplication is possible because of the python broadcasting feature.\n",
    "# (20, 1) --> positional_encoding_numerators\n",
    "# (  , 8) --> positional_encoding_denominators\n",
    "# (20, 8) --> Resultant tensor shape\n",
    "# \n",
    "# Each element in the 1D tensor (1,) in the positional_encoding_numerators is repeated to create a \n",
    "# 2D tensor of shape (20, 8):\n",
    "# [[0, 0, ..., 0]\n",
    "# [1, 1, ..., 1]\n",
    "# ...\n",
    "# [19, 19, ..., 19]]\n",
    "#\n",
    "# The 1D tensor (8,) positional_encoding_denominators itself is repeated 20 times to create a 2D\n",
    "# tesnor of shape (20, 8):\n",
    "# [positional_encoding_denominators, positional_encoding_denominators, ..., positional_encoding_denominators]\n",
    "#\n",
    "# The broadcasted (expanded) tensors are multiplied element wise to get the positional encodings.\n",
    "positional_encoding_sin_terms = torch.sin(positional_encoding_numerators * positional_encoding_denominators)\n",
    "print(positional_encoding_sin_terms)\n",
    "print(positional_encoding_sin_terms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.5403,  0.9504,  0.9950,  0.9995,  0.9999,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.4161,  0.8066,  0.9801,  0.9980,  0.9998,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.9900,  0.5828,  0.9553,  0.9955,  0.9996,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.6536,  0.3011,  0.9211,  0.9920,  0.9992,  0.9999,  1.0000,  1.0000],\n",
      "        [ 0.2837, -0.0103,  0.8776,  0.9875,  0.9988,  0.9999,  1.0000,  1.0000],\n",
      "        [ 0.9602, -0.3208,  0.8253,  0.9821,  0.9982,  0.9998,  1.0000,  1.0000],\n",
      "        [ 0.7539, -0.5994,  0.7648,  0.9756,  0.9976,  0.9998,  1.0000,  1.0000],\n",
      "        [-0.1455, -0.8186,  0.6967,  0.9682,  0.9968,  0.9997,  1.0000,  1.0000],\n",
      "        [-0.9111, -0.9566,  0.6216,  0.9598,  0.9960,  0.9996,  1.0000,  1.0000],\n",
      "        [-0.8391, -0.9998,  0.5403,  0.9504,  0.9950,  0.9995,  0.9999,  1.0000],\n",
      "        [ 0.0044, -0.9438,  0.4536,  0.9401,  0.9940,  0.9994,  0.9999,  1.0000],\n",
      "        [ 0.8439, -0.7942,  0.3624,  0.9289,  0.9928,  0.9993,  0.9999,  1.0000],\n",
      "        [ 0.9074, -0.5658,  0.2675,  0.9167,  0.9916,  0.9992,  0.9999,  1.0000],\n",
      "        [ 0.1367, -0.2813,  0.1700,  0.9036,  0.9902,  0.9990,  0.9999,  1.0000],\n",
      "        [-0.7597,  0.0310,  0.0707,  0.8896,  0.9888,  0.9989,  0.9999,  1.0000],\n",
      "        [-0.9577,  0.3403, -0.0292,  0.8747,  0.9872,  0.9987,  0.9999,  1.0000],\n",
      "        [-0.2752,  0.6159, -0.1288,  0.8589,  0.9856,  0.9986,  0.9999,  1.0000],\n",
      "        [ 0.6603,  0.8303, -0.2272,  0.8423,  0.9838,  0.9984,  0.9998,  1.0000],\n",
      "        [ 0.9887,  0.9625, -0.3233,  0.8249,  0.9820,  0.9982,  0.9998,  1.0000]])\n",
      "torch.Size([20, 8])\n"
     ]
    }
   ],
   "source": [
    "# Same logic as the above cell but just with the cos function.\n",
    "positional_encoding_cos_terms = torch.cos(positional_encoding_numerators * positional_encoding_denominators)\n",
    "print(positional_encoding_cos_terms)\n",
    "print(positional_encoding_cos_terms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [ 8.4147e-01,  0.0000e+00,  3.1098e-01,  0.0000e+00,  9.9833e-02,\n",
      "          0.0000e+00,  3.1618e-02,  0.0000e+00,  9.9998e-03,  0.0000e+00,\n",
      "          3.1623e-03,  0.0000e+00,  1.0000e-03,  0.0000e+00,  3.1623e-04,\n",
      "          0.0000e+00],\n",
      "        [ 9.0930e-01,  0.0000e+00,  5.9113e-01,  0.0000e+00,  1.9867e-01,\n",
      "          0.0000e+00,  6.3203e-02,  0.0000e+00,  1.9999e-02,  0.0000e+00,\n",
      "          6.3245e-03,  0.0000e+00,  2.0000e-03,  0.0000e+00,  6.3246e-04,\n",
      "          0.0000e+00],\n",
      "        [ 1.4112e-01,  0.0000e+00,  8.1265e-01,  0.0000e+00,  2.9552e-01,\n",
      "          0.0000e+00,  9.4726e-02,  0.0000e+00,  2.9995e-02,  0.0000e+00,\n",
      "          9.4867e-03,  0.0000e+00,  3.0000e-03,  0.0000e+00,  9.4868e-04,\n",
      "          0.0000e+00],\n",
      "        [-7.5680e-01,  0.0000e+00,  9.5358e-01,  0.0000e+00,  3.8942e-01,\n",
      "          0.0000e+00,  1.2615e-01,  0.0000e+00,  3.9989e-02,  0.0000e+00,\n",
      "          1.2649e-02,  0.0000e+00,  4.0000e-03,  0.0000e+00,  1.2649e-03,\n",
      "          0.0000e+00],\n",
      "        [-9.5892e-01,  0.0000e+00,  9.9995e-01,  0.0000e+00,  4.7943e-01,\n",
      "          0.0000e+00,  1.5746e-01,  0.0000e+00,  4.9979e-02,  0.0000e+00,\n",
      "          1.5811e-02,  0.0000e+00,  5.0000e-03,  0.0000e+00,  1.5811e-03,\n",
      "          0.0000e+00],\n",
      "        [-2.7942e-01,  0.0000e+00,  9.4715e-01,  0.0000e+00,  5.6464e-01,\n",
      "          0.0000e+00,  1.8860e-01,  0.0000e+00,  5.9964e-02,  0.0000e+00,\n",
      "          1.8973e-02,  0.0000e+00,  6.0000e-03,  0.0000e+00,  1.8974e-03,\n",
      "          0.0000e+00],\n",
      "        [ 6.5699e-01,  0.0000e+00,  8.0042e-01,  0.0000e+00,  6.4422e-01,\n",
      "          0.0000e+00,  2.1956e-01,  0.0000e+00,  6.9943e-02,  0.0000e+00,\n",
      "          2.2134e-02,  0.0000e+00,  6.9999e-03,  0.0000e+00,  2.2136e-03,\n",
      "          0.0000e+00],\n",
      "        [ 9.8936e-01,  0.0000e+00,  5.7432e-01,  0.0000e+00,  7.1736e-01,\n",
      "          0.0000e+00,  2.5029e-01,  0.0000e+00,  7.9915e-02,  0.0000e+00,\n",
      "          2.5296e-02,  0.0000e+00,  7.9999e-03,  0.0000e+00,  2.5298e-03,\n",
      "          0.0000e+00],\n",
      "        [ 4.1212e-01,  0.0000e+00,  2.9126e-01,  0.0000e+00,  7.8333e-01,\n",
      "          0.0000e+00,  2.8078e-01,  0.0000e+00,  8.9879e-02,  0.0000e+00,\n",
      "          2.8457e-02,  0.0000e+00,  8.9999e-03,  0.0000e+00,  2.8460e-03,\n",
      "          0.0000e+00],\n",
      "        [-5.4402e-01,  0.0000e+00, -2.0684e-02,  0.0000e+00,  8.4147e-01,\n",
      "          0.0000e+00,  3.1098e-01,  0.0000e+00,  9.9833e-02,  0.0000e+00,\n",
      "          3.1617e-02,  0.0000e+00,  9.9998e-03,  0.0000e+00,  3.1623e-03,\n",
      "          0.0000e+00],\n",
      "        [-9.9999e-01,  0.0000e+00, -3.3057e-01,  0.0000e+00,  8.9121e-01,\n",
      "          0.0000e+00,  3.4088e-01,  0.0000e+00,  1.0978e-01,  0.0000e+00,\n",
      "          3.4778e-02,  0.0000e+00,  1.1000e-02,  0.0000e+00,  3.4785e-03,\n",
      "          0.0000e+00],\n",
      "        [-5.3657e-01,  0.0000e+00, -6.0768e-01,  0.0000e+00,  9.3204e-01,\n",
      "          0.0000e+00,  3.7043e-01,  0.0000e+00,  1.1971e-01,  0.0000e+00,\n",
      "          3.7938e-02,  0.0000e+00,  1.2000e-02,  0.0000e+00,  3.7947e-03,\n",
      "          0.0000e+00],\n",
      "        [ 4.2017e-01,  0.0000e+00, -8.2453e-01,  0.0000e+00,  9.6356e-01,\n",
      "          0.0000e+00,  3.9961e-01,  0.0000e+00,  1.2963e-01,  0.0000e+00,\n",
      "          4.1098e-02,  0.0000e+00,  1.3000e-02,  0.0000e+00,  4.1110e-03,\n",
      "          0.0000e+00],\n",
      "        [ 9.9061e-01,  0.0000e+00, -9.5961e-01,  0.0000e+00,  9.8545e-01,\n",
      "          0.0000e+00,  4.2840e-01,  0.0000e+00,  1.3954e-01,  0.0000e+00,\n",
      "          4.4257e-02,  0.0000e+00,  1.4000e-02,  0.0000e+00,  4.4272e-03,\n",
      "          0.0000e+00],\n",
      "        [ 6.5029e-01,  0.0000e+00, -9.9952e-01,  0.0000e+00,  9.9749e-01,\n",
      "          0.0000e+00,  4.5675e-01,  0.0000e+00,  1.4944e-01,  0.0000e+00,\n",
      "          4.7416e-02,  0.0000e+00,  1.4999e-02,  0.0000e+00,  4.7434e-03,\n",
      "          0.0000e+00],\n",
      "        [-2.8790e-01,  0.0000e+00, -9.4031e-01,  0.0000e+00,  9.9957e-01,\n",
      "          0.0000e+00,  4.8465e-01,  0.0000e+00,  1.5932e-01,  0.0000e+00,\n",
      "          5.0575e-02,  0.0000e+00,  1.5999e-02,  0.0000e+00,  5.0596e-03,\n",
      "          0.0000e+00],\n",
      "        [-9.6140e-01,  0.0000e+00, -7.8785e-01,  0.0000e+00,  9.9166e-01,\n",
      "          0.0000e+00,  5.1206e-01,  0.0000e+00,  1.6918e-01,  0.0000e+00,\n",
      "          5.3733e-02,  0.0000e+00,  1.6999e-02,  0.0000e+00,  5.3758e-03,\n",
      "          0.0000e+00],\n",
      "        [-7.5099e-01,  0.0000e+00, -5.5726e-01,  0.0000e+00,  9.7385e-01,\n",
      "          0.0000e+00,  5.3897e-01,  0.0000e+00,  1.7903e-01,  0.0000e+00,\n",
      "          5.6890e-02,  0.0000e+00,  1.7999e-02,  0.0000e+00,  5.6921e-03,\n",
      "          0.0000e+00],\n",
      "        [ 1.4988e-01,  0.0000e+00, -2.7141e-01,  0.0000e+00,  9.4630e-01,\n",
      "          0.0000e+00,  5.6533e-01,  0.0000e+00,  1.8886e-01,  0.0000e+00,\n",
      "          6.0047e-02,  0.0000e+00,  1.8999e-02,  0.0000e+00,  6.0083e-03,\n",
      "          0.0000e+00]])\n",
      "torch.Size([20, 16])\n"
     ]
    }
   ],
   "source": [
    "# We are filling the even terms in the positional encoding tensor with the sin terms.\n",
    "# The even terms are at the 0th, 2nd, 4th, 6th, ..., 14th positions in the positional encoding vector.\n",
    "# Notice that the odd terms (cos terms) are still zeros since we haven't filled them yet.\n",
    "positional_encoding[:, 0::2] = positional_encoding_sin_terms\n",
    "print(positional_encoding)\n",
    "print(positional_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "          0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  3.1098e-01,  9.5042e-01,  9.9833e-02,\n",
      "          9.9500e-01,  3.1618e-02,  9.9950e-01,  9.9998e-03,  9.9995e-01,\n",
      "          3.1623e-03,  9.9999e-01,  1.0000e-03,  1.0000e+00,  3.1623e-04,\n",
      "          1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  5.9113e-01,  8.0658e-01,  1.9867e-01,\n",
      "          9.8007e-01,  6.3203e-02,  9.9800e-01,  1.9999e-02,  9.9980e-01,\n",
      "          6.3245e-03,  9.9998e-01,  2.0000e-03,  1.0000e+00,  6.3246e-04,\n",
      "          1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  8.1265e-01,  5.8275e-01,  2.9552e-01,\n",
      "          9.5534e-01,  9.4726e-02,  9.9550e-01,  2.9995e-02,  9.9955e-01,\n",
      "          9.4867e-03,  9.9995e-01,  3.0000e-03,  1.0000e+00,  9.4868e-04,\n",
      "          1.0000e+00],\n",
      "        [-7.5680e-01, -6.5364e-01,  9.5358e-01,  3.0114e-01,  3.8942e-01,\n",
      "          9.2106e-01,  1.2615e-01,  9.9201e-01,  3.9989e-02,  9.9920e-01,\n",
      "          1.2649e-02,  9.9992e-01,  4.0000e-03,  9.9999e-01,  1.2649e-03,\n",
      "          1.0000e+00],\n",
      "        [-9.5892e-01,  2.8366e-01,  9.9995e-01, -1.0342e-02,  4.7943e-01,\n",
      "          8.7758e-01,  1.5746e-01,  9.8753e-01,  4.9979e-02,  9.9875e-01,\n",
      "          1.5811e-02,  9.9988e-01,  5.0000e-03,  9.9999e-01,  1.5811e-03,\n",
      "          1.0000e+00],\n",
      "        [-2.7942e-01,  9.6017e-01,  9.4715e-01, -3.2080e-01,  5.6464e-01,\n",
      "          8.2534e-01,  1.8860e-01,  9.8205e-01,  5.9964e-02,  9.9820e-01,\n",
      "          1.8973e-02,  9.9982e-01,  6.0000e-03,  9.9998e-01,  1.8974e-03,\n",
      "          1.0000e+00],\n",
      "        [ 6.5699e-01,  7.5390e-01,  8.0042e-01, -5.9944e-01,  6.4422e-01,\n",
      "          7.6484e-01,  2.1956e-01,  9.7560e-01,  6.9943e-02,  9.9755e-01,\n",
      "          2.2134e-02,  9.9976e-01,  6.9999e-03,  9.9998e-01,  2.2136e-03,\n",
      "          1.0000e+00],\n",
      "        [ 9.8936e-01, -1.4550e-01,  5.7432e-01, -8.1863e-01,  7.1736e-01,\n",
      "          6.9671e-01,  2.5029e-01,  9.6817e-01,  7.9915e-02,  9.9680e-01,\n",
      "          2.5296e-02,  9.9968e-01,  7.9999e-03,  9.9997e-01,  2.5298e-03,\n",
      "          1.0000e+00],\n",
      "        [ 4.1212e-01, -9.1113e-01,  2.9126e-01, -9.5664e-01,  7.8333e-01,\n",
      "          6.2161e-01,  2.8078e-01,  9.5977e-01,  8.9879e-02,  9.9595e-01,\n",
      "          2.8457e-02,  9.9960e-01,  8.9999e-03,  9.9996e-01,  2.8460e-03,\n",
      "          1.0000e+00],\n",
      "        [-5.4402e-01, -8.3907e-01, -2.0684e-02, -9.9979e-01,  8.4147e-01,\n",
      "          5.4030e-01,  3.1098e-01,  9.5042e-01,  9.9833e-02,  9.9500e-01,\n",
      "          3.1617e-02,  9.9950e-01,  9.9998e-03,  9.9995e-01,  3.1623e-03,\n",
      "          9.9999e-01],\n",
      "        [-9.9999e-01,  4.4257e-03, -3.3057e-01, -9.4378e-01,  8.9121e-01,\n",
      "          4.5360e-01,  3.4088e-01,  9.4011e-01,  1.0978e-01,  9.9396e-01,\n",
      "          3.4778e-02,  9.9940e-01,  1.1000e-02,  9.9994e-01,  3.4785e-03,\n",
      "          9.9999e-01],\n",
      "        [-5.3657e-01,  8.4385e-01, -6.0768e-01, -7.9418e-01,  9.3204e-01,\n",
      "          3.6236e-01,  3.7043e-01,  9.2886e-01,  1.1971e-01,  9.9281e-01,\n",
      "          3.7938e-02,  9.9928e-01,  1.2000e-02,  9.9993e-01,  3.7947e-03,\n",
      "          9.9999e-01],\n",
      "        [ 4.2017e-01,  9.0745e-01, -8.2453e-01, -5.6582e-01,  9.6356e-01,\n",
      "          2.6750e-01,  3.9961e-01,  9.1668e-01,  1.2963e-01,  9.9156e-01,\n",
      "          4.1098e-02,  9.9916e-01,  1.3000e-02,  9.9992e-01,  4.1110e-03,\n",
      "          9.9999e-01],\n",
      "        [ 9.9061e-01,  1.3674e-01, -9.5961e-01, -2.8135e-01,  9.8545e-01,\n",
      "          1.6997e-01,  4.2840e-01,  9.0359e-01,  1.3954e-01,  9.9022e-01,\n",
      "          4.4257e-02,  9.9902e-01,  1.4000e-02,  9.9990e-01,  4.4272e-03,\n",
      "          9.9999e-01],\n",
      "        [ 6.5029e-01, -7.5969e-01, -9.9952e-01,  3.1022e-02,  9.9749e-01,\n",
      "          7.0737e-02,  4.5675e-01,  8.8959e-01,  1.4944e-01,  9.8877e-01,\n",
      "          4.7416e-02,  9.9888e-01,  1.4999e-02,  9.9989e-01,  4.7434e-03,\n",
      "          9.9999e-01],\n",
      "        [-2.8790e-01, -9.5766e-01, -9.4031e-01,  3.4032e-01,  9.9957e-01,\n",
      "         -2.9199e-02,  4.8465e-01,  8.7471e-01,  1.5932e-01,  9.8723e-01,\n",
      "          5.0575e-02,  9.9872e-01,  1.5999e-02,  9.9987e-01,  5.0596e-03,\n",
      "          9.9999e-01],\n",
      "        [-9.6140e-01, -2.7516e-01, -7.8785e-01,  6.1586e-01,  9.9166e-01,\n",
      "         -1.2884e-01,  5.1206e-01,  8.5895e-01,  1.6918e-01,  9.8558e-01,\n",
      "          5.3733e-02,  9.9856e-01,  1.6999e-02,  9.9986e-01,  5.3758e-03,\n",
      "          9.9999e-01],\n",
      "        [-7.5099e-01,  6.6032e-01, -5.5726e-01,  8.3034e-01,  9.7385e-01,\n",
      "         -2.2720e-01,  5.3897e-01,  8.4233e-01,  1.7903e-01,  9.8384e-01,\n",
      "          5.6890e-02,  9.9838e-01,  1.7999e-02,  9.9984e-01,  5.6921e-03,\n",
      "          9.9998e-01],\n",
      "        [ 1.4988e-01,  9.8870e-01, -2.7141e-01,  9.6246e-01,  9.4630e-01,\n",
      "         -3.2329e-01,  5.6533e-01,  8.2487e-01,  1.8886e-01,  9.8200e-01,\n",
      "          6.0047e-02,  9.9820e-01,  1.8999e-02,  9.9982e-01,  6.0083e-03,\n",
      "          9.9998e-01]])\n",
      "torch.Size([20, 16])\n"
     ]
    }
   ],
   "source": [
    "# We are filling the odd terms in the positional encoding tensor with the cos terms.\n",
    "# The even terms are at the 1th, 3nd, 5th, 7th, ..., 15th positions in the positional encoding vector.\n",
    "# Now, the positional_encoding tensor is completely filled with the positional encodings.\n",
    "positional_encoding[:, 1::2] = positional_encoding_cos_terms\n",
    "print(positional_encoding)\n",
    "print(positional_encoding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding in Transformers\n",
    "\n",
    "The __init__ code below just combines all the individual steps discussed above into a single initialization function.\n",
    "\n",
    "In addition to the actual implementation of Positional Encoding class, we also add Dropout layer and apply it in the <br>\n",
    "forward function. This is because dropout is applied to the output of Positional Encoding layer in the actual <br>\n",
    "translation model from 'Attention Is All You Need' paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will combine the above steps and put the logic into a module to use the Positional Encoding in the Transformer model.\n",
    "# Refer 'understanding_pytorch/modules/using_modules.ipynb notebook' (https://github.com/MB1151/understanding_pytorch) to \n",
    "# understand more about modules in pytorch.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # d_model above is the same as encoding_size here.\n",
    "    def __init__(self, encoding_size: int, dropout_prob: float, max_len: int = 5000):\n",
    "        \"\"\"Creates the positional encodings.\n",
    "\n",
    "        Args:\n",
    "            encoding_size (int): Size of the positional encoding vector that represents the position of the token.\n",
    "            dropout_prob (float): Probability of an element to be zeroed or dropped.\n",
    "            max_len (int, optional): Largest position for which the positional encoding vector is generated. Defaults to 5000.\n",
    "                                     By default, it generates positional encodings for the first 5000 positions.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Refer to step_7_drop_out.ipynb to understand more about dropout.\n",
    "        self.dropout = nn.Dropout(p=dropout_prob, inplace=False)\n",
    "        # Compute the positional encodings in log space.\n",
    "        positional_encoding = torch.zeros(size=(max_len, encoding_size), dtype=torch.float)\n",
    "        positional_encoding_numerators = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        numerators_in_exponent = torch.arange(0, encoding_size, 2, dtype=torch.float)\n",
    "        positional_encoding_denominators = torch.exp(numerators_in_exponent * (-math.log(10000.0) / encoding_size))\n",
    "        positional_encoding[:, 0::2] = torch.sin(positional_encoding_numerators * positional_encoding_denominators)\n",
    "        positional_encoding[:, 1::2] = torch.cos(positional_encoding_numerators * positional_encoding_denominators)\n",
    "        # Refer to 'understanding_pytorch/tensor_manipulations/understanding_tensor_manipulations_part_1.ipynb' (https://github.com/MB1151/understanding_pytorch/blob/main/tensor_manipulations/understanding_tensor_manipulations_part_1.ipynb) \n",
    "        # to understand more about unsqueeze operation in pytorch.\n",
    "        # In transformer model, we receive 3D tensors as input to this module. Each 1D tensor in the last dimension \n",
    "        # is an embedding for the token. Each 2D tensor is a sentence. The entire 3D tensor is a batch of sentences. \n",
    "        # To work with 3D tensors in the forward method, we convert the positional encoding to a 3D tensor.\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        # Refer to 'understanding_pytorch/tensor_manipulations/using_modules.ipynb' (https://github.com/MB1151/understanding_pytorch/blob/main/modules/using_modules.ipynb) \n",
    "        # to understand more about buffers in pytorch. This tells the module to not update the positional encoding \n",
    "        # tensor during the training. It is not a trainable parameter but it is still part of the state of the model.\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "    \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"Adds the positional encodings to the input tensor.\n",
    "        Args:\n",
    "            input (Tensor): The input tensor containing the embeddings of the tokens.\n",
    "                            shape: [batch_size, sentence_length, d_model]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Input with the positional encodings added to it.\n",
    "                    shape: [batch_size, sentence_length, d_model]\n",
    "        \"\"\"\n",
    "        # Refer to 'understanding_pytorch/tensor_manipulations/understanding_tensor_manipulations_part_5.ipynb' (https://github.com/MB1151/understanding_pytorch/blob/main/tensor_manipulations/understanding_tensor_manipulations_part_5.ipynb) \n",
    "        # to understand more about broadcasting in python.\n",
    "        #\n",
    "        # There are two important bits of information condensed in this operation:\n",
    "        # 1) We pre-calculate positional encodings for a fixed number (max_len=100 here) of positions. However the input \n",
    "        #    tensor might be of length 56 in which case, we only need the positional encoding vectors for the first 56\n",
    "        #    positions. This is the first part.\n",
    "        # 2) The input tensor is a 3D tensor of shape (batch_size, sentence_length, encoding_size) in the translation \n",
    "        #    model. However, the shape of pre-calculated positional encoding is (1, max_len, encoding_size). So, after\n",
    "        #    step 1 is done, the positional encoding tensor is broadcasted for the addition operation with input tensor.\n",
    "        #                                                  (broadcasted)\n",
    "        # positional_encoding: (1, max_len, encoding_size) -------------> (1, sentence_length, encoding_size) \n",
    "        #       -- Extracts the positional encodings for the sentence_length from the positional_encoding tensor.\n",
    "        #\n",
    "        # (batch_size, sentence_length, encoding_size) --> input\n",
    "        # (batch_size, sentence_length, encoding_size) --> Resultant tensor shape after broadcasting.\n",
    "        # requires_grad_(False) is not needed since the positional encoding is already registered\n",
    "        # as a Buffer and not a trainable parameter. It is just included for clarity.\n",
    "        input = input + self.positional_encoding[:, :input.size(1)].requires_grad_(False)\n",
    "        return self.dropout(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PositionalEncoding(\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "positional_encoding_layer = PositionalEncoding(encoding_size=16, dropout_prob=0.2)\n",
    "print(positional_encoding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_parameters of PositionalEncoding(\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# Notice that positional_encoding is a not part of the parameters in the module. This is because\n",
    "# we registered the positional_encoding tensor as a buffer and not as a parameter.\n",
    "print(positional_encoding_layer.named_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
      "           11.,  12.,  13.,  14.,  15.],\n",
      "         [ 16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,\n",
      "           27.,  28.,  29.,  30.,  31.],\n",
      "         [ 32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,\n",
      "           43.,  44.,  45.,  46.,  47.],\n",
      "         [ 48.,  49.,  50.,  51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,\n",
      "           59.,  60.,  61.,  62.,  63.]],\n",
      "\n",
      "        [[ 64.,  65.,  66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,\n",
      "           75.,  76.,  77.,  78.,  79.],\n",
      "         [ 80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,  90.,\n",
      "           91.,  92.,  93.,  94.,  95.],\n",
      "         [ 96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105., 106.,\n",
      "          107., 108., 109., 110., 111.],\n",
      "         [112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,\n",
      "          123., 124., 125., 126., 127.]]])\n",
      "torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "input_1 = torch.arange(start=0, end=128, dtype=torch.float).reshape(2, 4, 16)\n",
    "print(input_1)\n",
    "print(input_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0.0000,   2.5000,   0.0000,   5.0000,   0.0000,   0.0000,   7.5000,\n",
      "           10.0000,  10.0000,   0.0000,  12.5000,  15.0000,  15.0000,   0.0000,\n",
      "            0.0000,   0.0000],\n",
      "         [ 21.0518,  21.9254,  22.8887,  24.9380,  25.1248,  27.4938,  27.5395,\n",
      "           29.9994,  30.0125,  32.4999,   0.0000,  35.0000,  35.0013,  37.5000,\n",
      "           37.5004,  40.0000],\n",
      "         [ 41.1366,  40.7298,  43.2389,   0.0000,   0.0000,  47.4751,  47.5790,\n",
      "           49.9975,  50.0250,  52.4998,   0.0000,  55.0000,  55.0025,  57.5000,\n",
      "            0.0000,  60.0000],\n",
      "         [ 60.1764,  60.0125,  63.5158,  64.4784,  65.3694,   0.0000,   0.0000,\n",
      "           69.9944,  70.0375,  72.4994,   0.0000,  74.9999,  75.0037,  77.5000,\n",
      "           77.5012,   0.0000]],\n",
      "\n",
      "        [[ 80.0000,  82.5000,   0.0000,  85.0000,  85.0000,  87.5000,  87.5000,\n",
      "           90.0000,  90.0000,  92.5000,  92.5000,  95.0000,  95.0000,  97.5000,\n",
      "            0.0000, 100.0000],\n",
      "         [101.0518, 101.9254,   0.0000, 104.9380, 105.1248,   0.0000, 107.5395,\n",
      "          109.9994,   0.0000,   0.0000, 112.5040, 115.0000, 115.0013, 117.5000,\n",
      "          117.5004, 120.0000],\n",
      "         [121.1366, 120.7298,   0.0000, 124.7582, 125.2483,   0.0000, 127.5790,\n",
      "          129.9975, 130.0250, 132.4998, 132.5079, 135.0000, 135.0025,   0.0000,\n",
      "          137.5008, 140.0000],\n",
      "         [140.1764, 140.0125, 143.5158, 144.4784, 145.3694, 147.4442, 147.6184,\n",
      "          149.9944, 150.0375,   0.0000, 152.5119, 154.9999,   0.0000, 157.5000,\n",
      "          157.5012, 160.0000]]])\n",
      "torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "# Lets analyze a single output element:\n",
    "# output[0][0][1] = 2.5\n",
    "# input[0][0][1] = 1\n",
    "# positional_encoding[0][0][1] = 1\n",
    "# input[0][0][1] + positional_encoding[0][0][1] = 2\n",
    "#\n",
    "# Now, the dropout is applied to the output element. So, the element\n",
    "# is scaled up by 1 / (1 - dropout_prob) = 1 / (1 - 0.2) = 1 / 0.8 = 1.25\n",
    "# \n",
    "# output[0][0][1] = 2 * 1.25 = 2.5\n",
    "# Of course, this analysis might not be valid (values might change but the logic holds good) if this cell \n",
    "# is run again on the input since Dropout's behavior is probablistic.\n",
    "#\n",
    "# So, the module behaves as expected.\n",
    "output = positional_encoding_layer(input_1)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
