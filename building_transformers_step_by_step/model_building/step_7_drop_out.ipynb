{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "# \n",
    "# 1) What is Dropout Regularization?\n",
    "# 2) How to use Dropout with Pytorch?\n",
    "# 3) How does Dropout module work within Neural Networks?\n",
    "#\n",
    "# Dropout is used along with Positional Encoding in the Transformer model to prevent \n",
    "# overfitting. This will be explained in the step_9_positional_encoding.ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources to go through to understand about Regularization and Dropout before continuing \n",
    "# further in this notebook:\n",
    "#\n",
    "# https://www.youtube.com/watch?v=6g0t3Phly2M&t=1s\n",
    "#       -- Explains what Regularization is and L2 Regularization in particular.\n",
    "# https://www.youtube.com/watch?v=NyG-7nRpsW8\n",
    "#       -- Gives intuition on why Regularization works i.e., why it prevents the model from overfitting?\n",
    "# https://www.youtube.com/watch?v=D8PJAL-MZv8\n",
    "#       -- What is Dropout Regularization and how to implement it (Inverted Dropout)?\n",
    "# https://www.youtube.com/watch?v=ARq74QuavAo\n",
    "#       -- Gives intuition on why Dropout works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [torch.nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.5, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "# p=0.5 implies that any input is zeroed out with a probability of 0.5.\n",
    "dropout_module = nn.Dropout(p=0.5, inplace=False)\n",
    "print(dropout_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.,  9.],\n",
      "         [10., 11., 12., 13., 14.],\n",
      "         [15., 16., 17., 18., 19.]],\n",
      "\n",
      "        [[20., 21., 22., 23., 24.],\n",
      "         [25., 26., 27., 28., 29.],\n",
      "         [30., 31., 32., 33., 34.],\n",
      "         [35., 36., 37., 38., 39.]],\n",
      "\n",
      "        [[40., 41., 42., 43., 44.],\n",
      "         [45., 46., 47., 48., 49.],\n",
      "         [50., 51., 52., 53., 54.],\n",
      "         [55., 56., 57., 58., 59.]]])\n",
      "torch.Size([3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "input_1 = torch.arange(start=0, end=60, dtype=torch.float).reshape(3, 4, 5)\n",
    "print(input_1)\n",
    "print(input_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0.,   2.,   4.,   0.,   8.],\n",
      "         [ 10.,   0.,   0.,  16.,   0.],\n",
      "         [ 20.,  22.,   0.,  26.,   0.],\n",
      "         [  0.,  32.,  34.,   0.,  38.]],\n",
      "\n",
      "        [[ 40.,   0.,   0.,  46.,   0.],\n",
      "         [ 50.,  52.,   0.,  56.,   0.],\n",
      "         [ 60.,   0.,  64.,   0.,   0.],\n",
      "         [ 70.,  72.,   0.,  76.,   0.]],\n",
      "\n",
      "        [[ 80.,  82.,   0.,   0.,   0.],\n",
      "         [  0.,  92.,  94.,  96.,   0.],\n",
      "         [100., 102.,   0., 106.,   0.],\n",
      "         [  0., 112.,   0.,   0., 118.]]])\n"
     ]
    }
   ],
   "source": [
    "# Notice that roughly half of the values in the input tensor have been zeroed out.\n",
    "# Also, notice that the remaining values are scaled up by dividing each element with \n",
    "# '0.5'. As explained in one of the above videos, this scaling is done so that the \n",
    "# magnitude (maybe sum -- need to verify this) of the input expected by the layers \n",
    "# after dropout is not impacted because of the Dropout.  \n",
    "output_1 = dropout_module(input_1)\n",
    "print(output_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout in Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of how Dropout works in neural Networks: \n",
    "#\n",
    "# Dropout is a regularization technique used to prevent overfitting in Neural Networks. \n",
    "# The neurons are randomly dropped with some probability which has an effect of \n",
    "# training the model with a smaller network (since neurons have been deleted) that \n",
    "# prevents overfitting.\n",
    "#\n",
    "# In practice, Dropout is implemented using the Inverted Dropout technique. Inverted \n",
    "# Dropout creates a binary mask (same size as input) that holds information whether \n",
    "# the output of any neuron is to be propagated or dropped. This mask is then \n",
    "# multiplied (element wise multiplication) to the neuron outputs (activations) and \n",
    "# the resultant output is passed to the next layer.\n",
    "#\n",
    "# Back Propagation with Dropout works the same way as it works in neural networks \n",
    "# without Dropout i.e., there is no additional step required to handle gradients with \n",
    "# dropout. It gets taken care of by the mask variable in the gradient computation. In \n",
    "# the end, the effect of dropout in gradient computation is that the gradients wrt to \n",
    "# the weights attached (coming in and going out) to the dropped neuron are all zeros. \n",
    "#\n",
    "# In mini-batch gradient descent, the neurons are dropped independently for each input \n",
    "# from the batch i.e., the binary mask is generated independently for every input in \n",
    "# the batch. However, the gradients in this case are calculated based on the average \n",
    "# loss (Loss averaged over all the inputs in the batch). So, the gradients of the \n",
    "# weights associated with dropped neurons (different for different inputs) are not \n",
    "# always zero. Elaborating the above statement, If neuron 'n' is dropped for input 5 but \n",
    "# is not dropped for input 7 in the batch, the gradient calculation wrt the weights \n",
    "# associated (weights coming in or going out of 'n') with neuron 'n' will include \n",
    "# contribution from both input 5 and input 7. Since neuron 'n' is not dropped for \n",
    "# input 7, the gradient for the corresponding weight might be a non-zero value \n",
    "# (gradient contribution from input 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer this notebook (link to using_modules.ipynb) to understand more about pytorch modules.\n",
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        # Refer this notebook (link to understanding_nn_linear.ipynb) to understand more about pytorch Linear module.\n",
    "        self.layer_1 = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        # We define the Dropout module here.\n",
    "        self.dropout = nn.Dropout(p=dropout_prob, inplace=False)\n",
    "        self.layer_2 = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        output = self.layer_1(input)\n",
    "        output = self.relu(output)\n",
    "        print(\"output before applying dropout: \\n\\n\", output)\n",
    "        # This randomly drops the output values with probability of 'self.dropout_prob'\n",
    "        output = self.dropout(output)\n",
    "        print(\"\\n\\n output after applying dropout: \\n\\n\", output)\n",
    "        output = self.layer_2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size: int = 10\n",
    "hidden_size: int = 40\n",
    "# Setting it to a higher value so that the difference is visible in the example.\n",
    "dropout_prob: float = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNeuralNetwork(\n",
      "  (layer_1): Linear(in_features=10, out_features=40, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (layer_2): Linear(in_features=40, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "my_neural_net = SimpleNeuralNetwork(input_size=input_size, hidden_size=hidden_size, dropout_prob=dropout_prob)\n",
    "print(my_neural_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSELoss()\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.03\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Refer this notebook (link to building_simple_neural_network_using_modules.ipynb) to understand more\n",
    "# about building neural network and training it.\n",
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 0.03\n",
    "sgd_optimizer = torch.optim.SGD(params=my_neural_net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "print(loss_function)\n",
    "print(sgd_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7289, -2.7069, -0.9212,  0.1576, -1.8981,  0.8384,  0.8872,  1.0395,\n",
      "          1.9443, -0.2546],\n",
      "        [-0.6895, -0.6642, -0.2122, -0.5916, -1.7462,  0.5320,  0.3308,  0.6395,\n",
      "         -0.3654,  2.6568],\n",
      "        [-0.3839,  2.0729, -0.4598, -1.9365,  0.8274, -0.3351,  0.0230, -0.3876,\n",
      "         -0.5108, -0.2736],\n",
      "        [-0.0928,  2.0255, -1.1254, -0.3370, -1.5516,  0.8941,  0.7879, -1.0145,\n",
      "          0.9762, -1.1817],\n",
      "        [ 0.0563,  0.1312,  0.0361,  1.9600,  2.5043,  0.0831, -1.2609,  0.3114,\n",
      "         -0.2559, -1.2298]]) torch.Size([5, 10])\n",
      "tensor([[0.6992],\n",
      "        [0.9715],\n",
      "        [0.8541],\n",
      "        [0.1547],\n",
      "        [0.5483]]) torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "# Create dummy inputs and output for to see the network running.\n",
    "# This means we have 5 inputs and each input containing 10 features.\n",
    "model_inputs = torch.randn(size=(5, 10))\n",
    "# We have 5 targets, each one to the corresponding input.\n",
    "input_targets = torch.rand(size=(5, 1))\n",
    "print(model_inputs, model_inputs.shape)\n",
    "print(input_targets, input_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output before applying dropout: \n",
      "\n",
      " tensor([[0.0000, 1.3293, 0.5765, 0.1958, 0.0000, 0.8037, 0.0000, 0.0000, 0.3233,\n",
      "         0.7857, 0.1811, 0.0974, 0.5224, 0.5422, 0.6177, 0.0000, 0.4496, 0.0153,\n",
      "         0.0000, 2.0704, 0.4691, 0.6185, 0.0000, 0.6780, 1.1849, 0.0000, 0.9923,\n",
      "         0.0000, 0.0000, 0.0000, 0.0452, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0290, 0.9173, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5209, 0.0122, 1.0171, 0.5432, 0.2443, 0.0000, 0.5771, 0.0227,\n",
      "         0.1358, 0.7629, 0.0967, 0.1740, 0.0000, 0.0000, 0.0000, 1.0735, 0.8084,\n",
      "         0.0000, 0.8091, 1.2554, 0.0461, 0.0000, 0.3882, 0.0000, 1.1974, 0.0029,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6189, 0.3849,\n",
      "         1.1420, 0.0000, 0.0134, 0.0000],\n",
      "        [0.1213, 0.2353, 0.0000, 0.2969, 0.0000, 0.0000, 1.0119, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1121, 0.0204, 1.2546, 0.4286, 0.1234,\n",
      "         0.7397, 0.0000, 0.2635, 0.0000, 0.7562, 0.0000, 0.0000, 0.8936, 0.4235,\n",
      "         0.1016, 0.0000, 0.5364, 0.7196, 0.8786, 0.0000, 0.9609, 0.2325, 0.3047,\n",
      "         0.5586, 0.0000, 0.0744, 0.0000],\n",
      "        [0.0765, 0.4524, 0.0000, 0.0000, 0.0000, 0.9288, 1.0964, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.2368, 1.0135, 0.4003, 0.7268, 0.3375, 0.0000,\n",
      "         1.0483, 0.8848, 0.6289, 0.0621, 0.0000, 0.0000, 0.0000, 0.3986, 0.7817,\n",
      "         0.6634, 0.0000, 0.0000, 0.0000, 0.9394, 0.0000, 1.1781, 0.0000, 0.0000,\n",
      "         0.3727, 0.0000, 0.0000, 0.3763],\n",
      "        [0.1872, 0.0725, 0.0000, 0.0000, 0.0000, 0.4399, 0.4243, 0.0000, 0.0000,\n",
      "         0.1440, 0.5946, 0.0000, 0.0000, 0.0000, 0.9804, 0.3331, 0.0000, 0.0000,\n",
      "         0.2149, 0.0000, 0.0000, 0.8538, 0.2069, 0.1165, 0.4609, 0.0000, 0.1727,\n",
      "         0.7507, 1.4488, 1.3288, 0.0782, 0.0000, 0.5177, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.1319, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "\n",
      " output after applying dropout: \n",
      "\n",
      " tensor([[0.0000, 0.0000, 1.1530, 0.3916, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.5714, 0.0000, 0.1948, 1.0449, 1.0844, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 4.1408, 0.0000, 1.2369, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0903, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0579, 1.8346, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0244, 0.0000, 1.0865, 0.0000, 0.0000, 1.1542, 0.0455,\n",
      "         0.0000, 1.5258, 0.0000, 0.3480, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.6182, 0.0000, 0.0922, 0.0000, 0.0000, 0.0000, 2.3948, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2377, 0.7698,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2425, 0.4705, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.2242, 0.0000, 0.0000, 0.8571, 0.2467,\n",
      "         0.0000, 0.0000, 0.5271, 0.0000, 1.5123, 0.0000, 0.0000, 1.7872, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.4392, 1.7572, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1487, 0.0000],\n",
      "        [0.1530, 0.0000, 0.0000, 0.0000, 0.0000, 1.8576, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.4736, 2.0269, 0.8007, 1.4537, 0.6750, 0.0000,\n",
      "         0.0000, 1.7696, 0.0000, 0.1242, 0.0000, 0.0000, 0.0000, 0.7971, 1.5635,\n",
      "         1.3267, 0.0000, 0.0000, 0.0000, 1.8789, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.7525],\n",
      "        [0.0000, 0.1450, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.9608, 0.6662, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.4138, 0.0000, 0.0000, 0.0000, 0.3455,\n",
      "         1.5013, 0.0000, 2.6577, 0.1563, 0.0000, 1.0354, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sgd_optimizer.zero_grad()\n",
    "# Observe the following things from the printed outputs below (output before applying \n",
    "# dropout and output after applying dropout):\n",
    "# \n",
    "# 1) A lot of the output values (output after applying dropout) are set to zero and the \n",
    "#    non-zero values are scaled up (by 2) after Dropout is applied.\n",
    "# 2) Each 1D tensor below corresponds to the hidden layer outputs produced by a single \n",
    "#    input.\n",
    "# 3) For each input, the neurons (hidden layer) are dropped independently using the \n",
    "#    probability from the other inputs.\n",
    "#       -- 2nd Neuron is dropped for input 0.\n",
    "#           -- output[0][1] = 1.3293 (output before applying dropout)\n",
    "#           -- output[0][1] = 0.0000 (output after applying dropout)\n",
    "#           -- Ofcourse, this data won't be (probably) valid if you run this cell again.\n",
    "#       -- 2nd Neuron is active for input 2 where as 4th neuron is dropped for input 2.\n",
    "#           -- output[2][1] = 0.4705 (output after applying dropout)\n",
    "#           -- output[2][3] = 0.2969 (output before applying dropout)\n",
    "#           -- output[2][3] = 0.0000 (output after applying dropout)\n",
    "#           -- ofcourse, this data won't be (probably) valid if you run this cell again. \n",
    "#\n",
    "model_predictions = my_neural_net(model_inputs)\n",
    "loss = loss_function(model_predictions, input_targets)\n",
    "loss.backward()\n",
    "sgd_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintModelParameters(model):\n",
    "  for name, param in model.named_parameters():\n",
    "    print(\"\\nPrinting Model Parameters:\\n\\n\", f\"{name}: {param.data}\")\n",
    "    if param.requires_grad:\n",
    "      print(\"\\nPrinting Parameter Gradients:\\n\\n\", f\"{name}: {param.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Printing Model Parameters:\n",
      "\n",
      " layer_1.weight: tensor([[ 0.1481,  0.0321,  0.1532,  0.2778, -0.1383, -0.1684, -0.2570, -0.3082,\n",
      "         -0.0702,  0.1710],\n",
      "        [-0.0975, -0.0337, -0.1405, -0.1601, -0.2358,  0.1764,  0.3005, -0.1157,\n",
      "         -0.0870, -0.0054],\n",
      "        [ 0.0391, -0.0073,  0.2941, -0.3008,  0.1426,  0.2562, -0.1783,  0.2085,\n",
      "         -0.1614, -0.1403],\n",
      "        [ 0.1527, -0.0482, -0.1914, -0.0590,  0.0636,  0.1869, -0.0845, -0.2904,\n",
      "          0.2017,  0.0700],\n",
      "        [ 0.1266,  0.1643,  0.1210,  0.1053, -0.0793, -0.0570, -0.1535,  0.1699,\n",
      "          0.2627,  0.3099],\n",
      "        [-0.0246, -0.0234,  0.0109,  0.3076, -0.0882,  0.1236, -0.1018, -0.2685,\n",
      "          0.1116,  0.2856],\n",
      "        [ 0.0079,  0.3016, -0.1408, -0.1778,  0.0037, -0.0810,  0.1429, -0.0711,\n",
      "          0.1075, -0.1725],\n",
      "        [-0.0417, -0.2125,  0.3024, -0.2208, -0.2260, -0.0640,  0.1855, -0.1335,\n",
      "          0.0368,  0.1503],\n",
      "        [-0.1350, -0.1790, -0.1752,  0.1196,  0.1866,  0.2226,  0.2406, -0.2032,\n",
      "         -0.3146, -0.1162],\n",
      "        [-0.0372, -0.2286, -0.0384,  0.1523, -0.1460,  0.1067,  0.2832, -0.2118,\n",
      "         -0.1426, -0.2609],\n",
      "        [ 0.1555, -0.0889,  0.2432, -0.2332, -0.1329,  0.0188,  0.1615, -0.2897,\n",
      "         -0.1508, -0.2468],\n",
      "        [ 0.0554,  0.1897, -0.2807, -0.1519, -0.0588,  0.0367, -0.1873,  0.2676,\n",
      "          0.1251, -0.1361],\n",
      "        [-0.2943,  0.1583,  0.0442, -0.2019,  0.0681,  0.2182, -0.2503,  0.0582,\n",
      "          0.0375,  0.3114],\n",
      "        [-0.2275,  0.0044,  0.1489, -0.2179,  0.1837, -0.0475,  0.1800,  0.0993,\n",
      "         -0.0746,  0.0791],\n",
      "        [ 0.1945,  0.1134, -0.0370,  0.2363,  0.1385, -0.1591, -0.1345, -0.0106,\n",
      "         -0.0912, -0.0098],\n",
      "        [ 0.0811,  0.0800, -0.0392,  0.2848,  0.1012, -0.2091, -0.2192,  0.2805,\n",
      "         -0.1932,  0.3092],\n",
      "        [ 0.3069, -0.1674, -0.2452, -0.2366,  0.0301, -0.1444,  0.0596,  0.0880,\n",
      "          0.2285,  0.0264],\n",
      "        [ 0.2717, -0.1129,  0.0083,  0.1145, -0.1064,  0.2737,  0.0184, -0.1619,\n",
      "         -0.1741,  0.1170],\n",
      "        [-0.1096, -0.1395, -0.3165, -0.2148, -0.2967, -0.2502,  0.1957,  0.2107,\n",
      "          0.0273, -0.1804],\n",
      "        [ 0.0930, -0.2178,  0.2415,  0.1220,  0.1418,  0.0059, -0.0617,  0.0863,\n",
      "         -0.3142,  0.2617],\n",
      "        [-0.2448, -0.0425,  0.1390, -0.1285, -0.1737, -0.0862, -0.2010, -0.0790,\n",
      "         -0.0809,  0.2490],\n",
      "        [ 0.0904, -0.2694,  0.2156, -0.2584, -0.1822, -0.1275,  0.2402,  0.0678,\n",
      "         -0.0845, -0.0213],\n",
      "        [-0.1421, -0.0248,  0.0059, -0.1997,  0.1938, -0.2480,  0.1927,  0.0533,\n",
      "          0.0433, -0.2654],\n",
      "        [-0.1170,  0.1770,  0.1816,  0.2008,  0.0095, -0.2021,  0.2370, -0.1217,\n",
      "         -0.1464, -0.2618],\n",
      "        [-0.0230,  0.2463, -0.0005, -0.2393, -0.2418,  0.2049, -0.1324, -0.1207,\n",
      "         -0.0441,  0.2252],\n",
      "        [ 0.0952,  0.1197,  0.1866,  0.0230, -0.1973, -0.1981, -0.0993,  0.2463,\n",
      "          0.2711, -0.0466],\n",
      "        [ 0.0747,  0.1962,  0.1331, -0.1306,  0.2168, -0.0905, -0.0439,  0.3151,\n",
      "          0.1990,  0.1231],\n",
      "        [ 0.0066, -0.1778, -0.0226,  0.1701, -0.0380, -0.2816,  0.0761,  0.0089,\n",
      "         -0.0489,  0.1474],\n",
      "        [ 0.0157, -0.1871,  0.0461,  0.0103,  0.1483,  0.2735,  0.1166,  0.2795,\n",
      "          0.2525, -0.2702],\n",
      "        [-0.2668, -0.3031, -0.0907,  0.0928,  0.2466, -0.2436, -0.1670,  0.1056,\n",
      "         -0.0489,  0.2769],\n",
      "        [ 0.0999, -0.1800,  0.3054, -0.1226,  0.2087,  0.0419,  0.3064,  0.0635,\n",
      "         -0.1323, -0.0919],\n",
      "        [ 0.2455, -0.2343,  0.0223, -0.2863, -0.0192, -0.3162, -0.1729,  0.2453,\n",
      "         -0.1191,  0.1263],\n",
      "        [-0.3092,  0.2082,  0.0225, -0.1622, -0.0718,  0.2451, -0.2207, -0.0543,\n",
      "          0.0191, -0.2189],\n",
      "        [ 0.0616,  0.2489, -0.2330,  0.2884, -0.1220, -0.2531,  0.2380,  0.2290,\n",
      "          0.1994, -0.2128],\n",
      "        [ 0.2654, -0.1948,  0.0813,  0.2925, -0.0479,  0.0151, -0.2870,  0.1160,\n",
      "          0.2858, -0.2056],\n",
      "        [ 0.0426, -0.2878, -0.2652, -0.2963, -0.0835,  0.0178,  0.1301, -0.1238,\n",
      "          0.2306, -0.2698],\n",
      "        [ 0.0051, -0.0393, -0.3017, -0.1760,  0.0623, -0.1875,  0.0632, -0.2050,\n",
      "         -0.2561,  0.2475],\n",
      "        [ 0.1347,  0.3089,  0.1182,  0.0290,  0.1411,  0.2664,  0.0123, -0.1974,\n",
      "          0.2615, -0.1715],\n",
      "        [ 0.0733, -0.3002,  0.2641,  0.0112, -0.2218,  0.0851,  0.1349,  0.3103,\n",
      "         -0.1752,  0.0181],\n",
      "        [ 0.2173,  0.1854, -0.1819, -0.1145, -0.2597, -0.2709, -0.2498,  0.2368,\n",
      "         -0.1753, -0.2503]])\n",
      "\n",
      "Printing Parameter Gradients:\n",
      "\n",
      " layer_1.weight: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 5.6520e-02, -6.4966e-02,  4.1491e-02,  6.4462e-02, -2.5384e-02,\n",
      "          8.4442e-02, -8.9439e-02, -2.6918e-03,  1.9408e-02, -1.1545e-02],\n",
      "        [-3.1858e-03, -1.6465e-02, -2.7134e-02, -8.8710e-03,  5.2090e-03,\n",
      "         -3.2538e-02,  4.2473e-02, -6.3654e-03, -4.1148e-02,  2.4318e-02],\n",
      "        [ 5.8194e-02,  7.1606e-03,  2.2212e-03,  3.2324e-02, -6.1260e-03,\n",
      "          2.6308e-02, -4.4081e-02,  1.6486e-02, -3.4915e-02, -9.9061e-03],\n",
      "        [-5.7130e-03,  8.0546e-03, -1.2250e-03, -4.7030e-03,  3.8255e-03,\n",
      "          1.2802e-02,  1.2889e-02, -5.0053e-03,  2.0791e-02,  9.2170e-03],\n",
      "        [-8.1332e-04,  2.1953e-03, -2.5784e-03, -3.9349e-03, -1.4339e-03,\n",
      "         -5.2550e-03,  4.3548e-03, -2.3210e-04,  2.3166e-04,  4.9640e-04],\n",
      "        [-1.2541e-02,  1.7681e-02, -2.6892e-03, -1.0324e-02,  8.3976e-03,\n",
      "          2.8101e-02,  2.8292e-02, -1.0987e-02,  4.5639e-02,  2.0233e-02],\n",
      "        [-8.8000e-02, -5.2904e-02,  2.5312e-02, -2.5798e-02, -8.2301e-04,\n",
      "          2.8414e-02,  4.3695e-02, -4.4346e-02,  1.1968e-01,  2.5838e-02],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 1.0868e-01,  2.5834e-02,  2.7153e-03,  5.8190e-02, -6.8530e-03,\n",
      "          7.1022e-02, -7.0143e-02,  2.5969e-02, -4.0501e-02, -6.7692e-03],\n",
      "        [ 4.9323e-02, -5.3540e-02,  4.2500e-02,  6.0095e-02, -1.9482e-02,\n",
      "          1.1891e-01, -6.9894e-02, -1.3534e-02,  6.5159e-02,  6.9871e-03],\n",
      "        [ 1.8835e-02, -4.6236e-02, -1.3441e-02,  1.8876e-02,  2.5006e-04,\n",
      "          4.5500e-02,  3.3517e-02, -2.4178e-02,  1.0186e-02,  5.1132e-02],\n",
      "        [-9.5012e-03, -2.4654e-02,  2.6483e-03,  1.8164e-03, -2.2909e-03,\n",
      "          7.8905e-03,  8.1851e-03, -1.0809e-02,  1.3172e-02,  1.0658e-02],\n",
      "        [-1.1995e-01, -7.2112e-02,  3.4502e-02, -3.5165e-02, -1.1218e-03,\n",
      "          3.8731e-02,  5.9560e-02, -6.0447e-02,  1.6313e-01,  3.5220e-02],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-4.3519e-02,  9.3758e-02,  1.9446e-02, -4.1374e-02,  7.9651e-03,\n",
      "         -4.6878e-02, -2.6960e-02,  2.8845e-02,  2.8756e-02, -6.3999e-02],\n",
      "        [-3.2300e-02,  3.8072e-03,  2.8666e-02,  4.2662e-02,  6.2069e-02,\n",
      "          5.8541e-02, -2.9986e-02,  8.6982e-03, -2.4598e-02, -2.1334e-03],\n",
      "        [-1.3426e-02,  1.8798e-02,  4.2545e-02,  3.1104e-02,  2.6734e-02,\n",
      "          6.3500e-02, -5.8908e-02,  1.1000e-02,  2.9360e-02, -2.5962e-02],\n",
      "        [-1.6425e-01, -3.9043e-02, -4.1037e-03, -8.7943e-02,  1.0357e-02,\n",
      "         -1.0734e-01,  1.0601e-01, -3.9248e-02,  6.1209e-02,  1.0230e-02],\n",
      "        [-7.0833e-04, -5.7908e-04,  2.4200e-04, -1.6727e-04, -5.8360e-05,\n",
      "          8.2575e-05,  2.1765e-04, -3.2859e-04,  7.8967e-04,  1.0742e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 2.0363e-02, -4.9987e-02, -1.4532e-02,  2.0407e-02,  2.7034e-04,\n",
      "          4.9192e-02,  3.6237e-02, -2.6140e-02,  1.1012e-02,  5.5281e-02],\n",
      "        [-2.8795e-03, -3.5432e-04, -1.0991e-04, -1.5994e-03,  3.0312e-04,\n",
      "         -1.3017e-03,  2.1812e-03, -8.1575e-04,  1.7277e-03,  4.9017e-04],\n",
      "        [-2.5214e-02,  3.5302e-02,  7.9896e-02,  5.8411e-02,  5.0204e-02,\n",
      "          1.1925e-01, -1.1062e-01,  2.0657e-02,  5.5136e-02, -4.8755e-02],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-7.7165e-02, -4.6390e-02,  2.2195e-02, -2.2622e-02, -7.2168e-04,\n",
      "          2.4916e-02,  3.8315e-02, -3.8886e-02,  1.0495e-01,  2.2657e-02],\n",
      "        [-1.1743e-02,  1.3841e-03,  1.0422e-02,  1.5510e-02,  2.2566e-02,\n",
      "          2.1283e-02, -1.0901e-02,  3.1622e-03, -8.9428e-03, -7.7559e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-2.6247e-02, -1.5779e-02,  7.5494e-03, -7.6946e-03, -2.4547e-04,\n",
      "          8.4747e-03,  1.3032e-02, -1.3227e-02,  3.5696e-02,  7.7065e-03],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.0260e-01, -1.2585e-01, -6.2861e-02, -1.1375e-01, -6.5097e-02,\n",
      "         -1.0815e-01,  2.0753e-01, -9.4129e-02,  1.1304e-01,  1.0154e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 4.7284e-02, -1.0187e-01, -2.1128e-02,  4.4953e-02, -8.6542e-03,\n",
      "          5.0934e-02,  2.9292e-02, -3.1341e-02, -3.1244e-02,  6.9536e-02]])\n",
      "\n",
      "Printing Model Parameters:\n",
      "\n",
      " layer_1.bias: tensor([ 0.0398,  0.3033, -0.2104,  0.0267, -0.1892, -0.0222, -0.1361, -0.1485,\n",
      "        -0.0459,  0.3033,  0.2831,  0.2683,  0.0991,  0.1161, -0.2417, -0.2344,\n",
      "        -0.1462,  0.0813, -0.2775, -0.0241,  0.2309,  0.2905, -0.2386,  0.0582,\n",
      "         0.2421,  0.0748,  0.1914,  0.1489, -0.2884,  0.0405, -0.2952, -0.0554,\n",
      "         0.1828, -0.2832, -0.1013, -0.1249,  0.1564, -0.0510, -0.2757, -0.2244])\n",
      "\n",
      "Printing Parameter Gradients:\n",
      "\n",
      " layer_1.bias: tensor([ 0.0000,  0.0612, -0.0586,  0.0057,  0.0093, -0.0035,  0.0204,  0.0488,\n",
      "         0.0000,  0.0246,  0.0862, -0.0167,  0.0004,  0.0666,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0479,  0.0345,  0.0778, -0.0372,  0.0003,  0.0000,\n",
      "        -0.0181, -0.0003,  0.1460,  0.0000,  0.0428,  0.0125,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0146,  0.0000, -0.1110,  0.0000, -0.0520])\n",
      "\n",
      "Printing Model Parameters:\n",
      "\n",
      " layer_2.weight: tensor([[ 0.0248, -0.1301,  0.0647, -0.0452,  0.0646,  0.0171,  0.1427,  0.0855,\n",
      "          0.1251, -0.0944, -0.1352,  0.0971,  0.0166,  0.1248,  0.0110, -0.0358,\n",
      "         -0.1033,  0.0203, -0.1433, -0.1157, -0.0560,  0.1336, -0.0016,  0.0401,\n",
      "          0.0951,  0.0045, -0.1048, -0.1036,  0.0804, -0.0400,  0.0495,  0.0388,\n",
      "         -0.1355, -0.0800,  0.0472,  0.0175, -0.1557,  0.1623, -0.1426,  0.1635]])\n",
      "\n",
      "Printing Parameter Gradients:\n",
      "\n",
      " layer_2.weight: tensor([[ 0.0000,  0.0564, -0.0964, -0.0505,  0.0408, -0.4165,  0.0611,  0.4336,\n",
      "          0.0000,  0.0224, -0.1455, -0.3173,  0.0756,  0.3178,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, -0.0301, -0.2192, -0.2368,  0.2675,  0.0841,  0.0000,\n",
      "         -0.0143, -0.0728, -0.4593,  0.0000,  0.2011, -0.1469,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.3957,  0.0000, -0.3852,  0.0000, -0.2273]])\n",
      "\n",
      "Printing Model Parameters:\n",
      "\n",
      " layer_2.bias: tensor([0.0191])\n",
      "\n",
      "Printing Parameter Gradients:\n",
      "\n",
      " layer_2.bias: tensor([-0.3681])\n"
     ]
    }
   ],
   "source": [
    "PrintModelParameters(model=my_neural_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
