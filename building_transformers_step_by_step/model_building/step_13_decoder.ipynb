{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) How to build Decoder for the Transformer model?\n",
    "#       -- The idea is to use all the building blocks you learned in the previous notebooks to \n",
    "#          build the Decoder.\n",
    "#\n",
    "# IF YOU UNDERSTOOD 'step_12_encoder.ipynb' notebook, IT SHOULD BE VERY EASY TO UNDERSTAND THIS\n",
    "# CURRENT NOTEBOOK. IT IS ALMOST THE SAME AS THE ENCODER, EXCEPT FOR A FEW CHANGES (SOURCE_ATTENTION). \n",
    "# HOWEVER, BE CAREFUL WITH SHAPES IN DECODER ESPECIALLY DURING THE SOURCE ATTENTION CALCULATION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources I referred to build this notebook:\n",
    "#\n",
    "# 1) https://nlp.seas.harvard.edu/annotated-transformer/\n",
    "#       -- Refer to the EncoderLayer part of the blog post.\n",
    "# 2) https://jalammar.github.io/illustrated-transformer/\n",
    "#       -- It is a great blog post to understand the Transformer model. I highly recommend reading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS TO BE USED IN THIS NOTEBOOK.\n",
    "# Number of sentences in a batch.\n",
    "batch_size = 3\n",
    "# Number of tokens in a sentence.\n",
    "seq_len = 4\n",
    "# Dimension of the word embeddings.\n",
    "d_model = 8\n",
    "# Number of heads in the MultiHeadedAttention layer.\n",
    "num_heads = 2\n",
    "# Number of neurons in the hidden layer (that expands the input) in the feed forward neural network.\n",
    "d_feed_forward = 16\n",
    "# Probability with which the nodes are dropped in the dropout layer.\n",
    "dropout_prob = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder is a stack of 6 identical DecoderLayers. Lets first deep dive into the DecoderLayer.\n",
    "#\n",
    "# Every DecoderLayer has three main transformations:\n",
    "# 1) Multi-headed Attention mechanism on target sentences.\n",
    "#       -- This is self attention on the tgt sentences i.e., queries, keys and values, all \n",
    "#          come from the tgt sentences. \n",
    "# 2) Multi-Headed Attention mechanism on the target sentence with the keys, values taken \n",
    "#    from the Encoder output (encoded src sentences).\n",
    "#       -- This is shown as Encoder-Decoder attention in the below image.\n",
    "#       -- This is similar to the self attention mechanism, but the keys and values come\n",
    "#          from the output of the Encoder (encoded src sentences) and queries come from the \n",
    "#          tgt sentences.\n",
    "# 2) Position-wise feedforward neural network\n",
    "#\n",
    "# There is a Layer Normalization layer after each of these three transformations.\n",
    "# So, the overall structure of the encoder layer is:\n",
    "#\n",
    "# Input\n",
    "#   -> Self Attention (Multi-Headed Attention)\n",
    "#       -> Layer Normalization\n",
    "#   -> Add + Dropout\n",
    "#   -> Source Attention (Multi-Headed Attention)\n",
    "#       -> Layer Normalization\n",
    "#   -> Add Input To Output + Dropout\n",
    "#   -> Position-wise feedforward neural network\n",
    "#       -> Layer Normalization\n",
    "#   -> Add Input To Output + Dropout\n",
    "# Output\n",
    "#   -> This is the output of one Decoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../Data/Images/DecoderLayer.png\" alt=\"Decoder Layer\" width=\"450\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credits: The above image is taken from this blog post: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP THIS CELL IF YOU ALREADY LOOKED INTO THE STEP_9 AND STEP_10 NOTEBOOKS. ALL THIS CODE IN THIS CELL \n",
    "# IS COPIED FROM THE PREVIOUS NOTEBOOKS. JUST RUN IT BLINDLY.\n",
    "#\n",
    "def clone_module(module: nn.Module, num_clones: int) -> nn.ModuleList:\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(num_clones)])\n",
    "\n",
    "# Refer to 'step_11_feed_forward_neural_network.ipynb' to understand more about the FeedForwardNN class.\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, d_model: int, d_feed_forward: int, dropout_prob: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear_layer_1 = nn.Linear(in_features=d_model, out_features=d_feed_forward)\n",
    "        self.linear_layer_2 = nn.Linear(in_features=d_feed_forward, out_features=d_model)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # This input is forwaded from the Attention layer.\n",
    "        # input: [batch_size, seq_len, d_model]\n",
    "        # We first expand the input to higher dimension. We apply the ReLU activation function \n",
    "        # in this layer.\n",
    "        intermediate_output = self.linear_layer_1(input).relu()\n",
    "        # Dropout layer to prevent overfitting\n",
    "        intermediate_output = self.dropout_layer(intermediate_output)\n",
    "        # We then compress the input back to its original dimension.\n",
    "        # There is no specific intuitive explanation as to why this is done. It is just shown\n",
    "        # to be working practically.\n",
    "        return self.linear_layer_2(intermediate_output)\n",
    "    \n",
    "\n",
    "# Refer to 'step_11_multi_headed_attention.ipynb' to understand more about the MultiHeadedAttention class.\n",
    "# This function is just copied from that notebook to use it here.\n",
    "def construct_attention_heads(queries: Tensor, keys: Tensor, values: Tensor, mask: Optional[Tensor]=None, dropout_layer: Optional[nn.Module]=None) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Calculates the attention scores for each token in the sequence with every other token in the sequence.\n",
    "       Applies the mask if provided and then normalizes the scores using softmax. It then calculates the \n",
    "       attention heads for each token in the sequence.\n",
    "\n",
    "    Args:\n",
    "        queries (Tensor): [batch_size, num_heads, seq_len, d_k]\n",
    "        keys (Tensor): [batch_size, num_heads, seq_len, d_k]\n",
    "        values (Tensor): [batch_size, num_heads, seq_len, d_k]\n",
    "        mask (Optional[Tensor], optional): [batch_size, 1, seq_len, seq_len]. Defaults to None.\n",
    "        dropout_layer (Optional[nn.Module], optional): probability with which the values are dropped on dropout layer. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: Returns the attention heads and the attention scores.\n",
    "                               attention_heads: [batch_size, num_heads, seq_len, d_k]\n",
    "                               attention_scores: [batch_size, num_heads, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    # Size of the vectors for each token for each head in the sequence.\n",
    "    d_k = queries.shape[-1]\n",
    "    # Calculate the attention scores for each token in the sequence with every other token in the sequence.\n",
    "    attention_scores = torch.matmul(queries, keys.transpose(dim0=2, dim1=3)) / math.sqrt(d_k)\n",
    "    # Mask the attention scores if a mask is provided. Mask is used in two different ways:\n",
    "    # 1) To prevent the model from attending to the padding tokens --> This applies for both src and tgt sentences.\n",
    "    # 2) To prevent the model from attending to the future tokens in the sequence --> This applies only for tgt sentences.\n",
    "    if mask is not None:\n",
    "        # Please do not set the masked values to float('-inf') as it sometimes (not in everycase) causes softmax to return nan.\n",
    "        attention_scores = attention_scores.masked_fill(mask == False, float('-1e9'))\n",
    "    # Normalize the attention scores using softmax.\n",
    "    attention_scores = attention_scores.softmax(dim=-1)\n",
    "    # Apply dropout regularization to prevent overfitting problems.\n",
    "    if dropout_layer is not None:\n",
    "        dropout_layer(attention_scores)\n",
    "    # Calculate the attention heads for each token in the sequence. The head for each token is calculated by\n",
    "    # taking the weighted average (averaged by attention scores) of the values for all the tokens in the \n",
    "    # sequence for the token of interest.\n",
    "    attention_heads = torch.matmul(attention_scores, values)\n",
    "    return attention_heads, attention_scores\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_model: int, dropout_prob: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads.\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        # We use dropout to prevent overfitting.\n",
    "        self.dropout_layer = nn.Dropout(p=dropout_prob)\n",
    "        # Creating the linear layers that generate queries, keys and values for each token in the sequence.\n",
    "        # Also, creating an additional linear layer to generate the output of the Multi-Headed Attention from concatenated attention heads.\n",
    "        self.linear_layers = clone_module(module=nn.Linear(in_features=d_model, out_features=d_model), num_clones=4)\n",
    "\n",
    "\n",
    "    def forward(self, query_input: Tensor, key_input: Tensor, value_input: Tensor, mask: Optional[Tensor]=None) -> Tensor:\n",
    "        \"\"\"Forward pass of the Multi-Headed Attention layer. \n",
    "\n",
    "        Args:\n",
    "            query (Tensor): Input to be used for query creation.\n",
    "                            query_input: [batch_size, seq_len, d_model]\n",
    "            key (Tensor): Input to be used for key creation.\n",
    "                          key_input  : [batch_size, seq_len, d_model]\n",
    "            value (Tensor): Input to be used for value creation.\n",
    "                            value_input: [batch_size, seq_len, d_model]\n",
    "            mask (Tensor): Mask to be applied to the attention scores. Default is None. Same mask will \n",
    "                           be applied to all the heads in the Multi-Headed Attention layer.\n",
    "                           mask: [batch_size, 1, seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Mutli-Headed Attention Output: Output of the Multi-Headed Attention layer. Generates one output vector \n",
    "                                           for each token in the sequence. Does this for each sequence in the batch.\n",
    "                                           output: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Generates the queries, keys and values for each token in the sequence.\n",
    "        # shape of queries, keys, values: [batch_size, seq_len, d_model]\n",
    "        queries, keys, values = [linear_layer(input) for linear_layer, input in zip(self.linear_layers, (query_input, key_input, value_input))]\n",
    "        batch_size = query_input.shape[0]\n",
    "        seq_len = query_input.shape[1]\n",
    "        # Separating the queries, keys and values for each head into a separate vector. The vectors for each token in all the heads\n",
    "        # are concatenated when they are created using the linear_layers above.\n",
    "        # Shape for queries, keys, values after view: [batch_size, seq_len, num_heads, d_k]\n",
    "        # Shape for queries, key, values after transpose: [batch_size, num_heads, seq_len, d_k]\n",
    "        queries, keys, values = [data.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(dim0=1, dim1=2) for data in (queries, keys, values)]\n",
    "        # Calculate the attention heads for each token in the sequence.\n",
    "        # attention_heads: [batch_size, num_heads, seq_len, d_k]\n",
    "        attention_heads, attention_scores = construct_attention_heads(queries=queries, keys=keys, values=values, mask=mask, dropout_layer=self.dropout_layer)\n",
    "        # Concatenate the attention heads for each token from all the heads.\n",
    "        # attention_heads: [batch_size, seq_len, d_model]\n",
    "        attention_heads = attention_heads.transpose(dim0=1, dim1=2).reshape(batch_size, seq_len, self.d_model)\n",
    "        # Generate the output of the Multi-Headed Attention layer.\n",
    "        return self.linear_layers[-1](attention_heads)\n",
    "    \n",
    "\n",
    "# This class is the same as the SubLayerWrapper in the 'step_14_encoder.ipynb' notbeook.\n",
    "class SubLayerWrapper(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout_prob: float):\n",
    "        \"\"\"This class is a wrapper around the MultiHeadedAttention and PositionwiseFeedForward classes.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimension of the vectors used in the Attention model.\n",
    "            dropout_prob (float): probability with which nodes can be dropped.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, input: Tensor, sublayer: nn.Module) -> Tensor:\n",
    "        \"\"\"It applies the provided operation on the input, applies dropout, adds the input back to the \n",
    "           transformed input, does normalization and returns the output.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Input to be transformer by the sublayer.\n",
    "                            input: [batch_size, seq_len, d_model]\n",
    "            sublayer (nn.Module): sublayer could be either MultiHeadedAttention or PositionwiseFeedForward.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: Output of the sublayer transformation.\n",
    "                    output: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        return self.layer_norm(input + self.dropout(sublayer(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE MAIN PART (THAT IS DIFFERENT FROM EARLIER NOTEBOOKS) OF THIS NOTEBOOK STARTS HERE. HOWEVER,\n",
    "# IT IS STILL VERY SIMILAR TO THE ENCODER PART. SO, IT SHOULD BE STRAIGHT FORWARD TO UNDERSTAND."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (seq_len - 1) in the decoder input and mask comes from the fact that we remove the last token from the \n",
    "# decoder input when we create batches and masks. Refer to 'step_5_data_batching_and_masking.ipynb' notebook\n",
    "# to understand this better.\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, self_attention: MultiHeadedAttention, src_attention: MultiHeadedAttention, feed_forward: FeedForwardNN, d_model: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout_prob = dropout_prob\n",
    "        # These modules are now the child modules of the DecoderLayer and will be registered as parameters of the DecoderLayer.\n",
    "        self.self_attention = self_attention\n",
    "        self.src_attention = src_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer_wrappers = clone_module(module=SubLayerWrapper(d_model=d_model, dropout_prob=dropout_prob), num_clones=3)\n",
    "\n",
    "    def forward(self, input: Tensor, encoded_src: Tensor, tgt_mask: Tensor, src_mask: Optional[Tensor]=None) -> Tensor:\n",
    "        \"\"\"This method is the forward pass of the DecoderLayer class.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Target sentence provided as input to the DecoderLayer. These are the embeddings of the target \n",
    "                            sentence for the first DecoderLayer.\n",
    "                            SHAPE: [batch_size, seq_len - 1, d_model]\n",
    "            encoded_src (Tensor): Encoded source sentence. This is the output of the Encoder. This is used to calculate the\n",
    "                                  source attention scores for the target sentence. \n",
    "                                  SHAPE: [batch_size, seq_len, d_model] \n",
    "            tgt_mask (Tensor): Mask to prevent the future tokens in the target sentence to attend to the previous tokens and\n",
    "                               also to prevent padding tokens from attending to any other token except other padding tokens.\n",
    "                               SHAPE: [batch_size, 1, seq_len - 1, seq_len - 1]\n",
    "            src_mask (Optional[Tensor], optional): Mask to prevent the the padding tokens to attend to the tokens in the tgt sentence. \n",
    "                                                   Defaults to None.\n",
    "                                                   SHAPE: [batch_size, 1, seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Returns the output of the DecoderLayer. This is the output of the Positionwise FeedForward Neural Network.\n",
    "                    SHAPE: [batch_size, seq_len - 1, d_model]\n",
    "        \"\"\"\n",
    "        # First sublayer: Self-Attention on the target sentence. Hence, it uses the tgt_mask.\n",
    "        self_attention_output = self.sublayer_wrappers[0](input=input, sublayer=lambda input: self.self_attention(query_input=input, key_input=input, value_input=input, mask=tgt_mask)) \n",
    "        # To give intuition about src_attention, I have a query for a token in the target sentence. I want to know whether \n",
    "        # some token in the source sentence is important for me to predict the output for the token in the target sentence. \n",
    "        # So, I go to the source sentence and get the values for all the tokens in the source sentence. I then calculate \n",
    "        # the attention scores between the query (in tgt) and the keys (in src). I then calculate the attention heads for \n",
    "        # the token in the target sentence using the attention scores. This is what is done in the below line. Note that \n",
    "        # referring to statement 'the keys and values are from the source' doesn't mean that you get keys and values \n",
    "        # explicitly. It means we use the encoded data from the source sentence to calculate the queries and keys for \n",
    "        # this transformation.\n",
    "        # Second sublayer: Attention on the source sentence. Hence, it uses the src_mask.\n",
    "        src_attention_output = self.sublayer_wrappers[1](input=self_attention_output, sublayer=lambda self_attention_output: self.src_attention(query_input=self_attention_output, key_input=encoded_src, value_input=encoded_src, mask=src_mask))\n",
    "        # Third sublayer: Positionwise FeedForward Neural Network\n",
    "        return self.sublayer_wrappers[2](input=src_attention_output, sublayer=self.feed_forward)\n",
    "    \n",
    "\n",
    "# Here, lets try to understand how the shapes change when we caluclate source_attention above.\n",
    "# queries from tgt:    [batch_size, num_heads, seq_len - 1, d_k]\n",
    "# keys from encoder:   [batch_size, num_heads, seq_len, d_k]\n",
    "# values from encoder: [batch_size, num_heads, seq_len, d_k]\n",
    "#\n",
    "# attention_scores = queries * keys^{transpose}  --> * here represents matrix multiplication.\n",
    "# attention_scores: [batch_size, num_heads, seq_len - 1, seq_len]\n",
    "#\n",
    "# attention_heads = attentions_scores * values  --> * here represents matrix multiplication.\n",
    "# attention_heads: [batch_size, num_heads, seq_len - 1, d_k]\n",
    "# output of source attention calculation: [batch_size, num_heads, seq_len - 1, d_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets look at the Decoder itself. The Decoder is a stack of 6 identical DecoderLayers.\n",
    "# The output of one DecoderLayer is passed as input to the next DecoderLayer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../Data/Images/Decoder.png\" alt=\"Decoder\" width=\"450\" height=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_layer: DecoderLayer, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.decoder_layers = clone_module(module=decoder_layer, num_clones=num_layers)\n",
    "        self.layer_norm = nn.LayerNorm(decoder_layer.d_model)\n",
    "\n",
    "    def forward(self, input: Tensor, encoded_src: Tensor, tgt_mask: Tensor, src_mask: Optional[Tensor]=None) -> Tensor:\n",
    "        \"\"\"This method is the forward pass of the Decoder class. The output of the current DecoderLayer is\n",
    "           passed as input to the next DecoderLayer. We have 6 identical DecoderLayers stacked on top of \n",
    "           each other. The output of the Encoder (last EncoderLayer) is also passed as input to the \n",
    "           first DecoderLayer. The output of the last DecoderLayer is passed through a Layer Normalization \n",
    "           layer and returned as the final output of the Decoder.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Input to the Decoder i.e., embeddings of the tokenized tgt sequences.\n",
    "                            SHAPE: [batch_size, seq_len - 1, d_model]\n",
    "            encoded_src (Tensor): output of the encoder i.e., encoded src sequences.\n",
    "                                  SHAPE: [batch_size, seq_len, d_model]\n",
    "            tgt_mask (Tensor): Boolean mask to be applied during self attention scores calculation.\n",
    "                               SHAPE: [batch_size, 1, seq_len - 1, seq_len - 1].\n",
    "            src_mask (Optional[Tensor], optional): Boolean mask to be applied during src attention scores calculation.\n",
    "                                                   SHAPE: [batch_size, 1, seq_len, seq_len]. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output of the Decoder.\n",
    "                    SHAPE: [batch_size, seq_len - 1, d_model]\n",
    "        \"\"\"\n",
    "        output = input\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            # Pass the output of the previous DecoderLayer to the current DecoderLayer.\n",
    "            output = decoder_layer(input=output, encoded_src=encoded_src, tgt_mask=tgt_mask, src_mask=src_mask)\n",
    "        return self.layer_norm(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates input to experiment with the pipeline.\n",
    "def generate_batch_of_input_data(batch_size: int, seq_len: int, d_model: int) -> Tensor:\n",
    "    return torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# We are just creating a random mask for testing here. This is not how masks are created to be used \n",
    "# in the transformers. Refer to 'step_7_data_batching_and_masking.ipynb' to understand how src and \n",
    "# tgt masks are created in the transformers.\n",
    "def construct_random_mask(batch_size: int, seq_len: int) -> Tensor:\n",
    "    # If some index is set to False, then it will be masked out.\n",
    "    mask = torch.randn(size=(batch_size, 1, seq_len, seq_len)) > 0.5\n",
    "    return mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([3, 4, 8])\n",
      "src_data:  tensor([[[-1.1403e+00, -9.2024e-01,  8.0869e-01,  5.7408e-01, -1.1810e+00,\n",
      "          -2.8915e-01,  2.4586e-01, -1.3560e+00],\n",
      "         [-6.8936e-01,  2.0820e-01, -2.3100e-01,  2.7594e-01, -7.0491e-01,\n",
      "          -3.3236e+00, -1.6732e-02, -4.9242e-01],\n",
      "         [-8.3267e-01, -1.2729e-01,  1.3480e-01, -7.5081e-01, -1.6244e-01,\n",
      "           1.7945e+00,  6.8148e-01,  4.4234e-01],\n",
      "         [ 1.6017e+00, -6.5314e-01,  1.2028e+00,  1.2371e+00, -1.5162e-01,\n",
      "           1.8575e-03, -1.2741e+00, -1.6410e-01]],\n",
      "\n",
      "        [[-4.8748e-02,  4.0851e-01, -6.6606e-01,  1.1173e-01, -5.0911e-01,\n",
      "           1.0118e+00,  5.2170e-01, -1.0424e-01],\n",
      "         [ 8.1525e-02,  3.4766e-01,  7.4616e-01,  1.2160e+00,  1.5857e+00,\n",
      "          -2.8427e-02, -9.0159e-01, -1.2319e+00],\n",
      "         [-1.6478e+00, -6.8876e-01,  4.7857e-01,  8.3703e-01, -7.3230e-01,\n",
      "          -1.2180e+00, -5.0042e-02, -2.2114e+00],\n",
      "         [-1.2035e-01,  1.2297e+00, -5.9811e-01,  6.7815e-03,  1.1323e-01,\n",
      "           3.5423e-01, -1.0239e+00, -1.0908e-01]],\n",
      "\n",
      "        [[-1.1746e+00, -8.1928e-01,  3.0074e-01,  3.1343e-02, -2.7751e-02,\n",
      "          -1.9950e-01,  1.6322e+00,  3.4641e-01],\n",
      "         [ 4.5599e-01, -3.1280e-01,  2.2100e-01, -2.3739e+00,  1.4567e+00,\n",
      "           1.2790e-01,  1.5121e+00,  2.4173e-01],\n",
      "         [ 3.5852e-01,  6.5475e-01,  3.0303e-02,  7.6177e-01,  2.0913e-01,\n",
      "           1.3315e+00,  1.7534e+00,  2.8772e-01],\n",
      "         [-1.4546e+00, -9.0749e-01, -9.1125e-01,  1.8383e+00, -6.4617e-01,\n",
      "           1.7890e-01,  9.0480e-02,  9.4216e-01]]])\n",
      "-----------------------------------------------------\n",
      "shape:  torch.Size([3, 1, 4, 4])\n",
      "src_mask:  tensor([[[[False,  True, False, False],\n",
      "          [False, False, False, False],\n",
      "          [False,  True, False, False],\n",
      "          [ True, False, False, False]]],\n",
      "\n",
      "\n",
      "        [[[False,  True, False,  True],\n",
      "          [False, False,  True, False],\n",
      "          [False, False,  True, False],\n",
      "          [False, False, False, False]]],\n",
      "\n",
      "\n",
      "        [[[False, False, False,  True],\n",
      "          [ True, False, False, False],\n",
      "          [ True, False, False, False],\n",
      "          [ True, False, False, False]]]])\n",
      "-----------------------------------------------------\n",
      "shape:  torch.Size([3, 4, 8])\n",
      "tgt_data:  tensor([[[-0.3887, -1.3336, -0.5937, -0.0901, -0.7015,  0.1743,  1.4982,\n",
      "          -0.4464],\n",
      "         [ 0.6737, -0.7910,  0.3584, -0.9228, -0.5938,  1.5756, -1.0500,\n",
      "          -0.8625],\n",
      "         [ 0.2714, -0.3356,  1.3819,  0.0347,  0.9271, -0.4157,  0.9369,\n",
      "          -0.7596],\n",
      "         [-0.6286, -0.7452,  2.2698, -2.5023,  0.5209,  0.7546,  0.1668,\n",
      "          -0.3157]],\n",
      "\n",
      "        [[ 0.9422,  1.2610, -0.0751,  0.9807, -0.2199,  0.1399, -0.0638,\n",
      "           0.1212],\n",
      "         [ 0.7010,  0.3308,  0.7658,  0.0984, -0.2710, -0.3089,  0.0920,\n",
      "           0.5641],\n",
      "         [-0.4263, -2.5622, -0.2587,  0.7908, -1.3192,  0.2072,  1.1641,\n",
      "          -0.4828],\n",
      "         [-0.7927, -1.6824,  1.3816, -0.5412, -1.1750,  0.7757, -1.3510,\n",
      "          -0.6824]],\n",
      "\n",
      "        [[ 0.5499,  1.4757,  0.2701,  0.3635, -1.7578, -0.3562,  0.1717,\n",
      "           0.6288],\n",
      "         [-0.2704, -1.6204, -0.0709,  2.0378,  0.5758,  0.3648,  0.8068,\n",
      "           0.7927],\n",
      "         [-1.3766, -0.4217,  1.4925,  0.8383, -0.9295, -1.3225, -1.2211,\n",
      "           1.6629],\n",
      "         [ 1.7513,  0.2186, -0.2829, -0.7534, -0.3695,  1.0439,  0.2603,\n",
      "           1.6678]]])\n",
      "-----------------------------------------------------\n",
      "shape:  torch.Size([3, 1, 4, 4])\n",
      "tgt_mask:  tensor([[[[ True,  True,  True, False],\n",
      "          [False,  True, False, False],\n",
      "          [ True, False, False, False],\n",
      "          [False,  True,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[False,  True, False, False],\n",
      "          [ True,  True, False, False],\n",
      "          [False, False, False, False],\n",
      "          [ True,  True, False, False]]],\n",
      "\n",
      "\n",
      "        [[[False,  True, False, False],\n",
      "          [ True, False, False, False],\n",
      "          [False, False, False,  True],\n",
      "          [False,  True,  True, False]]]])\n",
      "-----------------------------------------------------\n",
      "shape:  torch.Size([3, 4, 8])\n",
      "encoder_output:  tensor([[[-0.8519, -0.6209,  0.5439,  0.2359, -1.0315, -0.6122, -0.7959,\n",
      "          -0.1030],\n",
      "         [ 1.1865, -1.2771,  0.1111, -1.0305,  2.4154,  1.4751,  0.5994,\n",
      "          -1.5001],\n",
      "         [-0.0156, -1.7125, -2.1154,  0.4430,  0.5170,  0.8806, -0.9433,\n",
      "          -0.7899],\n",
      "         [-0.2742,  0.6230, -0.4602, -1.2269,  0.9419, -0.4038, -0.6490,\n",
      "           1.3844]],\n",
      "\n",
      "        [[-1.6082, -0.2725, -1.3135,  0.2850,  1.2524,  0.8203,  0.7784,\n",
      "           2.1533],\n",
      "         [-1.0531,  1.3362,  0.2957, -0.1544,  2.2455,  0.2964, -1.0268,\n",
      "          -0.7878],\n",
      "         [ 1.2022,  0.2779, -0.6773, -0.5685, -1.0658, -0.8110, -0.0246,\n",
      "          -0.2080],\n",
      "         [-0.7266,  0.8150,  1.3440,  1.4117,  0.7648,  0.2176,  0.3754,\n",
      "          -0.2862]],\n",
      "\n",
      "        [[ 0.5652,  0.8430,  0.0368, -0.4397, -0.6245,  1.2451, -0.0695,\n",
      "           0.8300],\n",
      "         [-1.5076, -1.0948,  1.3504, -0.0557, -1.0911,  0.7851,  3.3987,\n",
      "           0.0916],\n",
      "         [-0.1937, -0.1064, -1.8716, -0.4556, -1.5295, -0.7068, -0.8740,\n",
      "          -1.1613],\n",
      "         [-0.7130,  0.6432, -1.7166,  0.5502, -0.1031, -0.2124,  0.7997,\n",
      "           0.7394]]])\n"
     ]
    }
   ],
   "source": [
    "# Please note that transformer expects src_data and tgt_data in some specific format. However, \n",
    "# for the sake of simplicity, I am just generating random data here. The actual format of the data\n",
    "# is shown in the 'step_7_data_batching_and_masking.ipynb' notebook. We will also explore it in\n",
    "# the future notebooks.\n",
    "src_data = generate_batch_of_input_data(batch_size=batch_size, seq_len=seq_len, d_model=d_model)\n",
    "print(\"shape: \", src_data.shape)\n",
    "print(\"src_data: \", src_data)\n",
    "print(\"-----------------------------------------------------\")\n",
    "src_mask = construct_random_mask(batch_size=batch_size, seq_len=seq_len)\n",
    "print(\"shape: \", src_mask.shape)\n",
    "print(\"src_mask: \", src_mask)\n",
    "print(\"-----------------------------------------------------\")\n",
    "tgt_data = generate_batch_of_input_data(batch_size=batch_size, seq_len=seq_len, d_model=d_model)\n",
    "print(\"shape: \", tgt_data.shape)\n",
    "print(\"tgt_data: \", tgt_data)\n",
    "print(\"-----------------------------------------------------\")\n",
    "tgt_mask = construct_random_mask(batch_size=batch_size, seq_len=seq_len)\n",
    "print(\"shape: \", tgt_mask.shape)\n",
    "print(\"tgt_mask: \", tgt_mask)\n",
    "print(\"-----------------------------------------------------\")\n",
    "# In the actual model, we first the encoder on the src data and then pass the output of the encoder\n",
    "# to the decoder. However, for the sake of simplicity, I am just passing the src_data to the decoder.\n",
    "# We will explore this connection in the future notebooks.\n",
    "encoder_output = generate_batch_of_input_data(batch_size=batch_size, seq_len=seq_len, d_model=d_model)\n",
    "print(\"shape: \", encoder_output.shape)\n",
    "print(\"encoder_output: \", encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiheaded_attention:  MultiHeadedAttention(\n",
      "  (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "  (linear_layers): ModuleList(\n",
      "    (0-3): 4 x Linear(in_features=8, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------\n",
      "feed_forward_nn:  FeedForwardNN(\n",
      "  (linear_layer_1): Linear(in_features=8, out_features=16, bias=True)\n",
      "  (linear_layer_2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "multiheaded_attention = MultiHeadedAttention(num_heads=num_heads, d_model=d_model, dropout_prob=dropout_prob)\n",
    "print(\"multiheaded_attention: \", multiheaded_attention)\n",
    "print(\"----------------------------------------------\")\n",
    "feed_forward_nn = FeedForwardNN(d_model=d_model, d_feed_forward=d_feed_forward, dropout_prob=dropout_prob)\n",
    "print(\"feed_forward_nn: \", feed_forward_nn)\n",
    "print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_layer:  DecoderLayer(\n",
      "  (self_attention): MultiHeadedAttention(\n",
      "    (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "    (linear_layers): ModuleList(\n",
      "      (0-3): 4 x Linear(in_features=8, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (src_attention): MultiHeadedAttention(\n",
      "    (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "    (linear_layers): ModuleList(\n",
      "      (0-3): 4 x Linear(in_features=8, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (feed_forward): FeedForwardNN(\n",
      "    (linear_layer_1): Linear(in_features=8, out_features=16, bias=True)\n",
      "    (linear_layer_2): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (sublayer_wrappers): ModuleList(\n",
      "    (0-2): 3 x SubLayerWrapper(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "decoder_layer = DecoderLayer(self_attention=copy.deepcopy(multiheaded_attention), \n",
    "                             src_attention=copy.deepcopy(multiheaded_attention), \n",
    "                             feed_forward=copy.deepcopy(feed_forward_nn), \n",
    "                             d_model=d_model, \n",
    "                             dropout_prob=dropout_prob)\n",
    "print(\"decoder_layer: \", decoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder:  Decoder(\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-5): 6 x DecoderLayer(\n",
      "      (self_attention): MultiHeadedAttention(\n",
      "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "        (linear_layers): ModuleList(\n",
      "          (0-3): 4 x Linear(in_features=8, out_features=8, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (src_attention): MultiHeadedAttention(\n",
      "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "        (linear_layers): ModuleList(\n",
      "          (0-3): 4 x Linear(in_features=8, out_features=8, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (feed_forward): FeedForwardNN(\n",
      "        (linear_layer_1): Linear(in_features=8, out_features=16, bias=True)\n",
      "        (linear_layer_2): Linear(in_features=16, out_features=8, bias=True)\n",
      "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (sublayer_wrappers): ModuleList(\n",
      "        (0-2): 3 x SubLayerWrapper(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(decoder_layer=decoder_layer, num_layers=6)\n",
    "print(\"decoder: \", decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([3, 4, 8])\n",
      "decoder_output:  tensor([[[-1.1061, -0.5500,  0.0733, -0.7903,  1.2729,  1.9033, -0.0283,\n",
      "          -0.7748],\n",
      "         [-0.1326, -0.5112, -0.0748, -0.2682,  1.7345,  1.4551, -0.8880,\n",
      "          -1.3149],\n",
      "         [-0.5470, -1.0540, -0.3246, -0.5026,  0.9811,  2.0944,  0.2564,\n",
      "          -0.9038],\n",
      "         [-0.1037, -0.6275, -0.6672,  0.0567,  0.9113,  2.1837, -0.8015,\n",
      "          -0.9518]],\n",
      "\n",
      "        [[-0.1121,  0.2154, -0.3915,  0.7804,  1.0024,  1.4002, -1.5558,\n",
      "          -1.3390],\n",
      "         [ 0.3801,  0.4446, -0.3432,  0.5393,  0.6576,  1.3945, -1.8112,\n",
      "          -1.2616],\n",
      "         [ 0.1352, -0.0923, -0.4717,  0.6608,  0.6897,  1.7826, -1.3983,\n",
      "          -1.3059],\n",
      "         [ 0.1704,  0.0198, -0.4958,  0.4451,  0.7853,  1.7894, -1.4684,\n",
      "          -1.2458]],\n",
      "\n",
      "        [[ 0.5008,  0.4563, -0.4886,  1.0461,  0.4811,  1.0075, -1.9767,\n",
      "          -1.0265],\n",
      "         [ 0.7084,  0.4482, -0.3441,  0.9553,  0.2433,  1.0916, -1.8691,\n",
      "          -1.2336],\n",
      "         [-0.2425,  0.0074, -0.1271,  1.2768,  0.0692,  1.5519, -1.8451,\n",
      "          -0.6907],\n",
      "         [ 0.4493, -0.2858, -0.3820,  1.0114,  0.2951,  1.6318, -1.5827,\n",
      "          -1.1371]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "decoder_output = decoder(input=tgt_data, encoded_src=encoder_output, tgt_mask=tgt_mask, src_mask=src_mask)\n",
    "print(\"shape: \", decoder_output.shape)\n",
    "print(\"decoder_output: \", decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
