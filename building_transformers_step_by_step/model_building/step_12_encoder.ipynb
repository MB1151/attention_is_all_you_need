{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) How to build Encoder for the Transformer model?\n",
    "#       -- The idea is to use all the building blocks you learned in the previous notebooks to build the Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources I referred to build this notebook:\n",
    "#\n",
    "# 1) https://nlp.seas.harvard.edu/annotated-transformer/\n",
    "#       -- Refer to the EncoderLayer code part of the blog post.\n",
    "# 2) https://jalammar.github.io/illustrated-transformer/\n",
    "#       -- It is a great blog post to understand the Transformer model. I highly recommend reading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS TO BE USED IN THIS NOTEBOOK.\n",
    "# Number of sentence in a batch.\n",
    "batch_size = 3\n",
    "# Number of tokens in a sentence.\n",
    "seq_len = 4\n",
    "# Dimension of the word embeddings.\n",
    "d_model = 8\n",
    "# Number of heads in the MultiHeadedAttention layer.\n",
    "num_heads = 2\n",
    "# Number of neurons in the hidden layer (that expands the input) in the feed forward neural network.\n",
    "d_feed_forward = 16\n",
    "# Probability with which the nodes are dropped in the dropout layer.\n",
    "dropout_prob = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder is a stack of 6 identical EncoderLayers. Lets first deep dive into the EncoderLayer.\n",
    "#\n",
    "# Every EncoderLayer has two main transformations:\n",
    "# 1) Multi-head self-attention mechanism\n",
    "# 2) Position-wise feedforward neural network\n",
    "#\n",
    "# There is a Layer Normalization layer after each of these two transformations.\n",
    "# So, the overall structure of the encoder layer is:\n",
    "#\n",
    "# Input\n",
    "#   -> Multi-head self-attention\n",
    "#       -> Layer Normalization\n",
    "#   -> Add Input To Output + Dropout\n",
    "#   -> Position-wise feedforward neural network\n",
    "#       -> Layer Normalization\n",
    "#   -> Add Input To Output + Dropout\n",
    "# Output\n",
    "#   -> This is the output of one encoder layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../Data/Images/EncoderLayer.png\" alt=\"Encoder Layer\" width=\"450\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credits: The above image is taken from this blog post: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The EncoderLayer class can be viewed as a combination of two sublayers:\n",
    "# 1) MultiHeadedAttention + LayerNormalization\n",
    "# 2) PositionwiseFeedforward + LayerNormalization\n",
    "#\n",
    "# Both the MultiHeadedAttention and PositionwiseFeedForward are two operations that take an input and \n",
    "# produce an output. We don't need to differentiate between these two operations while using them in\n",
    "# the EncoderLayer class. We can treat them as black boxes that take an input and produce an output.\n",
    "# So, we will create a single class called 'SubLayerWrapper' that takes in an operation \n",
    "# (MultiHeadedAttention or PositionwiseFeedForward), applies to the input, then applies a dropout, \n",
    "# adds the input back to the transformed input, does normalization and returns the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP THIS CELL IF YOU ALREADY LOOKED INTO THE STEP_9 AND STEP_10 NOTEBOOKS. JUST RUN IT BLINDLY.\n",
    "#\n",
    "def clone_module(module: nn.Module, num_clones: int) -> nn.ModuleList:\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(num_clones)])\n",
    "\n",
    "# Refer to 'step_11_feed_forward_neural_network.ipynb' to understand more about the FeedForwardNN class.\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, d_model: int, d_feed_forward: int, dropout_prob: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear_layer_1 = nn.Linear(in_features=d_model, out_features=d_feed_forward)\n",
    "        self.linear_layer_2 = nn.Linear(in_features=d_feed_forward, out_features=d_model)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # This input is forwaded from the Attention layer.\n",
    "        # input: [batch_size, seq_len, d_model]\n",
    "        # We first expand the input to higher dimension. We apply the ReLU activation function \n",
    "        # in this layer.\n",
    "        intermediate_output = self.linear_layer_1(input).relu()\n",
    "        # Dropout layer to prevent overfitting\n",
    "        intermediate_output = self.dropout_layer(intermediate_output)\n",
    "        # We then compress the input back to its original dimension.\n",
    "        # There is no specific intuitive explanation as to why this is done. It is just shown\n",
    "        # to be working practically.\n",
    "        return self.linear_layer_2(intermediate_output)\n",
    "    \n",
    "\n",
    "# Refer to 'step_11_multi_headed_attention.ipynb' to understand more about the MultiHeadedAttention class.\n",
    "# This function is just copied from that notebook to use it here.\n",
    "def construct_attention_heads(queries: Tensor, keys: Tensor, values: Tensor, mask: Optional[Tensor]=None, dropout_layer: Optional[nn.Module]=None) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Calculates the attention scores for each token in the sequence with every other token in the sequence.\n",
    "       Applues the mask if provided and then normalizes the scores using softmax. It then calculates the \n",
    "       attention heads for each token in the sequence.\n",
    "\n",
    "    Args:\n",
    "        queries (Tensor): [batch_size, num_heads, seq_len, d_k]\n",
    "        keys (Tensor): [batch_size, num_heads, seq_len, d_k]\n",
    "        values (Tensor): [batch_size, num_heads, seq_len, d_k]\n",
    "        mask (Optional[Tensor], optional): [batch_size, 1, seq_len, seq_len]. Defaults to None.\n",
    "        dropout_layer (Optional[nn.Module], optional): probability with which the values are dropped on dropout layer. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: Returns the attention heads and the attention scores.\n",
    "                               attention_heads: [batch_size, num_heads, seq_len, d_k]\n",
    "                               attention_scores: [batch_size, num_heads, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    # Size of the vectors for each token for each head in the sequence.\n",
    "    d_k = queries.shape[-1]\n",
    "    # Calculate the attention scores for each token in the sequence with every other token in the sequence.\n",
    "    attention_scores = torch.matmul(queries, keys.transpose(dim0=2, dim1=3)) / math.sqrt(d_k)\n",
    "    # Mask the attention scores if a mask is provided. Mask is used in two different ways:\n",
    "    # 1) To prevent the model from attending to the padding tokens --> This applies for both src and tgt sentences.\n",
    "    # 2) To prevent the model from attending to the future tokens in the sequence --> This applies only for tgt sentences.\n",
    "    if mask is not None:\n",
    "        # Please do not set the masked values to float('-inf') as it sometimes (not in everycase) causes softmax to return nan.\n",
    "        attention_scores = attention_scores.masked_fill(mask == False, float('-1e9'))\n",
    "    # Normalize the attention scores using softmax.\n",
    "    attention_scores = attention_scores.softmax(dim=-1)\n",
    "    # Apply dropout regularization to prevent overfitting problems.\n",
    "    if dropout_layer is not None:\n",
    "        dropout_layer(attention_scores)\n",
    "    # Calculate the attention heads for each token in the sequence. The head for each token is calculated by\n",
    "    # taking the weighted average (averaged by attention scores) of the values for all the tokens in the \n",
    "    # sequence for the token of interest.\n",
    "    attention_heads = torch.matmul(attention_scores, values)\n",
    "    return attention_heads, attention_scores\n",
    "\n",
    "\n",
    "# Refer to 'using_modules.ipynb' (Add link to the notebook) to understand more about Pytorch modules.\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_model: int, dropout_prob: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads.\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        # We use dropout to prevent overfitting.\n",
    "        self.dropout_layer = nn.Dropout(p=dropout_prob)\n",
    "        # Creating the linear layers that generate queries, keys and values for each token in the sequence.\n",
    "        # Also, creating an additional linear layer to generate the output of the Multi-Headed Attention from concatenated attention heads.\n",
    "        self.linear_layers = clone_module(module=nn.Linear(in_features=d_model, out_features=d_model), num_clones=4)\n",
    "\n",
    "\n",
    "    def forward(self, query_input: Tensor, key_input: Tensor, value_input: Tensor, mask: Optional[Tensor]=None) -> Tensor:\n",
    "        \"\"\"Forward pass of the Multi-Headed Attention layer. \n",
    "\n",
    "        Args:\n",
    "            query (Tensor): Input to be used for query creation.\n",
    "                            query_input: [batch_size, seq_len, d_model]\n",
    "            key (Tensor): Input to be used for key creation.\n",
    "                          key_input  : [batch_size, seq_len, d_model]\n",
    "            value (Tensor): Input to be used for value creation.\n",
    "                            value_input: [batch_size, seq_len, d_model]\n",
    "            mask (Tensor): Mask to be applied to the attention scores. Default is None. Same mask will \n",
    "                           be applied to all the heads in the Multi-Headed Attention layer.\n",
    "                           mask: [batch_size, 1, seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Mutli-Headed Attention Output: Output of the Multi-Headed Attention layer. Generates one output vector \n",
    "                                           for each token in the sequence. Does this for each sequence in the batch.\n",
    "                                           output: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Generates the queries, keys and values for each token in the sequence.\n",
    "        # shape of queries, keys, values: [batch_size, seq_len, d_model]\n",
    "        queries, keys, values = [linear_layer(input) for linear_layer, input in zip(self.linear_layers, (query_input, key_input, value_input))]\n",
    "        batch_size = query_input.shape[0]\n",
    "        seq_len = query_input.shape[1]\n",
    "        # Separating the queries, keys and values for each head into a separate vector. The vectors for each token in all the heads\n",
    "        # are concatenated when they are created using the linear_layers above.\n",
    "        # Shape for queries, keys, values after view: [batch_size, seq_len, num_heads, d_k]\n",
    "        # Shape for queries, key, values after transpose: [batch_size, num_heads, seq_len, d_k]\n",
    "        queries, keys, values = [data.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(dim0=1, dim1=2) for data in (queries, keys, values)]\n",
    "        # Calculate the attention heads for each token in the sequence.\n",
    "        # attention_heads: [batch_size, num_heads, seq_len, d_k]\n",
    "        attention_heads, attention_scores = construct_attention_heads(queries=queries, keys=keys, values=values, mask=mask, dropout_layer=self.dropout_layer)\n",
    "        # Concatenate the attention heads for each token from all the heads.\n",
    "        # attention_heads: [batch_size, seq_len, d_model]\n",
    "        attention_heads = attention_heads.transpose(dim0=1, dim1=2).reshape(batch_size, seq_len, self.d_model)\n",
    "        # Generate the output of the Multi-Headed Attention layer.\n",
    "        return self.linear_layers[-1](attention_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that dropout and layer_norm are the child modules (part of BackPropagation) of the SubLayerWrapper \n",
    "# class. However, 'sublayer' (argument to the forward function) is not a child module of the SubLayerWrapper \n",
    "# class. It is passed as an argument to the forward method of the SubLayerWrapper class.\n",
    "class SubLayerWrapper(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout_prob: float):\n",
    "        \"\"\"This class is a wrapper around the MultiHeadedAttention and PositionwiseFeedForward classes.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimension of the vectors used in the Attention model.\n",
    "            dropout_prob (float): probability with which nodes can be dropped.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, input: Tensor, sublayer: nn.Module) -> Tensor:\n",
    "        \"\"\"It applies the operation on the input, applies dropout, adds the input back to the transformed \n",
    "           input, does normalization and returns the output.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Input to be transformer by the sublayer.\n",
    "                            shape: [batch_size, seq_len, d_model]\n",
    "            sublayer (nn.Module): sublayer could be either MultiHeadedAttention or PositionwiseFeedForward.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: Output of the sublayer transformation.\n",
    "                    shape: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        return self.layer_norm(input + self.dropout(sublayer(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MultiHeadedAttention (self_attention here) and FeedForward modules are also common (common meaning they \n",
    "# have the same implementation and instantiation mechanism and not that they share weights) to the DecoderLayer \n",
    "# Hence, we create them in a common way at the top level and pass them as arguments to the EncoderLayer and \n",
    "# DecoderLayer classes. Passing them as arguments is more of a design choice than a necessity. Since \n",
    "# EncodeLayer is a common abstraction that can act on any kind of layers, it is reasonable to create encoder \n",
    "# as a container and pass the layers as arguments to the container. \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, self_attention: MultiHeadedAttention, feed_forward: FeedForwardNN, d_model: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout_prob = dropout_prob\n",
    "        # These modules are now the child modules of the EncoderLayer and will be registered as parameters of the EncoderLayer.\n",
    "        self.self_attention = self_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        # We need two instances of the SubLayerWrapper class. One for the self_attention and the other for the feed_forward.\n",
    "        self.sublayer_wrappers = clone_module(module=SubLayerWrapper(d_model=self.d_model, dropout_prob=self.dropout_prob), num_clones=2)\n",
    "\n",
    "    def forward(self, input: Tensor, mask: Tensor) -> Tensor:\n",
    "        \"\"\"This method is the forward pass of the EncoderLayer class.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Source sentence provided as input to the EncoderLayer. These are the embeddings of the source \n",
    "                            sentence for the first EncoderLayer.\n",
    "                            SHAPE: [batch_size, seq_len, d_model]\n",
    "            mask (Tensor): Boolean mask to be applied to the input during attention scores calculation.\n",
    "                           SHAPE: [batch_size, 1, seq_len, seq_len]\n",
    "        Returns:\n",
    "            Tensor: Output of the EncoderLayer.\n",
    "                    SHAPE: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # We are just saving the function call to the self_attention method in a variable and passing the\n",
    "        # lambda function (contained within the variable) to the sublayer_wrappers[0] to execute it when \n",
    "        # needed.\n",
    "        output = self.sublayer_wrappers[0](input, lambda input: self.self_attention(query_input=input, key_input=input, value_input=input, mask=mask))\n",
    "        return self.sublayer_wrappers[1](output, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating input to experiment with the FeedForward Neural Network\n",
    "def generate_batch_of_input_data(batch_size: int, seq_len: int, d_model: int) -> Tensor:\n",
    "    return torch.randn(batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([3, 4, 8])\n",
      "input_data:  tensor([[[ 0.2582, -1.3831, -1.9438,  0.5316, -0.1784,  0.0083,  0.2884,\n",
      "          -0.8376],\n",
      "         [ 0.2480,  0.8048, -2.7526,  1.5818, -0.2004,  0.5739,  0.5272,\n",
      "           0.0656],\n",
      "         [ 1.1624,  1.2678,  0.1227,  0.2279, -0.5776, -0.5855, -0.9168,\n",
      "           0.0203],\n",
      "         [ 1.4978,  1.0155,  0.4399, -0.8802, -0.1166,  0.3307,  0.1024,\n",
      "          -1.5923]],\n",
      "\n",
      "        [[ 1.3564,  1.2711, -0.0802, -0.3287, -1.6290, -0.1725, -0.8015,\n",
      "          -1.1178],\n",
      "         [-0.8002, -0.6339,  0.3804,  2.0195,  0.7491, -0.3428,  0.5965,\n",
      "          -0.6687],\n",
      "         [-0.8388, -1.0904,  0.9625, -0.5597,  1.2343,  0.6819,  1.2447,\n",
      "          -0.3270],\n",
      "         [ 0.4437, -1.3110,  0.3671, -2.0343, -1.1671, -1.5498, -0.4165,\n",
      "          -0.5960]],\n",
      "\n",
      "        [[-0.5625, -0.5839,  2.0064, -0.8538, -0.2274,  0.7378,  0.3497,\n",
      "           1.1398],\n",
      "         [-0.3905, -0.1371, -1.7092, -0.3213, -0.9406, -0.9600,  0.3203,\n",
      "           1.0399],\n",
      "         [-0.4192,  0.2997, -0.2379,  1.8094, -1.3516,  0.5444,  1.5267,\n",
      "           0.2985],\n",
      "         [-0.4186, -0.1363, -0.8262,  0.4918, -2.9946,  1.7301,  0.5504,\n",
      "           0.0576]]])\n"
     ]
    }
   ],
   "source": [
    "input_data = generate_batch_of_input_data(batch_size, seq_len, d_model)\n",
    "print(\"shape: \", input_data.shape)\n",
    "print(\"input_data: \", input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiheaded_attention:  MultiHeadedAttention(\n",
      "  (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "  (linear_layers): ModuleList(\n",
      "    (0-3): 4 x Linear(in_features=8, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------\n",
      "feed_forward_nn:  FeedForwardNN(\n",
      "  (linear_layer_1): Linear(in_features=8, out_features=16, bias=True)\n",
      "  (linear_layer_2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "----------------------------------------------\n",
      "encoder_layer:  EncoderLayer(\n",
      "  (self_attention): MultiHeadedAttention(\n",
      "    (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "    (linear_layers): ModuleList(\n",
      "      (0-3): 4 x Linear(in_features=8, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (feed_forward): FeedForwardNN(\n",
      "    (linear_layer_1): Linear(in_features=8, out_features=16, bias=True)\n",
      "    (linear_layer_2): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (sublayer_wrappers): ModuleList(\n",
      "    (0-1): 2 x SubLayerWrapper(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "multiheaded_attention = MultiHeadedAttention(num_heads=num_heads, d_model=d_model, dropout_prob=dropout_prob)\n",
    "print(\"multiheaded_attention: \", multiheaded_attention)\n",
    "print(\"----------------------------------------------\")\n",
    "feed_forward_nn = FeedForwardNN(d_model=d_model, d_feed_forward=d_feed_forward, dropout_prob=dropout_prob)\n",
    "print(\"feed_forward_nn: \", feed_forward_nn)\n",
    "print(\"----------------------------------------------\")\n",
    "# We are using the deepcopy function to create a new instance of the multiheaded_attention and feed_forward_nn.\n",
    "encoder_layer = EncoderLayer(self_attention=copy.deepcopy(multiheaded_attention), \n",
    "                             feed_forward=copy.deepcopy(feed_forward_nn), \n",
    "                             d_model=d_model, \n",
    "                             dropout_prob=dropout_prob)\n",
    "print(\"encoder_layer: \", encoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_layer_output:  tensor([[[ 0.6012, -1.4467, -1.3647,  1.5490,  0.0944,  0.6453,  0.6146,\n",
      "          -0.6933],\n",
      "         [ 0.0136, -0.0800, -2.1195,  1.6702,  0.2798,  0.5962,  0.1458,\n",
      "          -0.5061],\n",
      "         [ 1.7685,  1.0970, -0.1746, -0.1778, -0.4999, -0.9963, -1.4695,\n",
      "           0.4526],\n",
      "         [ 2.1072,  0.2719,  0.2543, -1.1158, -0.1703,  0.3268, -0.2718,\n",
      "          -1.4023]],\n",
      "\n",
      "        [[ 1.9583,  0.6615, -0.1043,  0.3946, -1.5033, -0.1120, -1.1217,\n",
      "          -0.1731],\n",
      "         [-0.6857, -1.2708,  0.6155,  1.9099,  0.9417, -0.6115, -0.1152,\n",
      "          -0.7838],\n",
      "         [-0.8260, -1.8154,  0.6749, -0.2955,  1.4778,  0.6743,  0.7010,\n",
      "          -0.5910],\n",
      "         [ 1.0786, -1.0121,  1.6652, -1.1747,  0.0932, -1.0857, -0.2165,\n",
      "           0.6519]],\n",
      "\n",
      "        [[-1.2896, -1.1925,  1.5293, -0.1240, -0.5423,  1.0088, -0.4238,\n",
      "           1.0341],\n",
      "         [-1.0181, -0.2743, -1.5114,  1.0017, -0.1723, -0.2879,  0.4495,\n",
      "           1.8127],\n",
      "         [-1.2134, -0.2620, -0.6588,  1.6348, -1.2410,  0.7391,  1.1189,\n",
      "          -0.1176],\n",
      "         [-0.5394, -0.4340, -0.6427,  1.0632, -1.6808,  1.7031,  0.5005,\n",
      "           0.0301]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder_layer_output = encoder_layer(input=input_data, mask=None)\n",
    "print(\"encoder_layer_output: \", encoder_layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets look at the Encoder itself. The Encoder is a stack of 6 identical EncoderLayers.\n",
    "# The output of one EncoderLayer is passed as input to the next EncoderLayer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../Data/Images/Encoder.png\" alt=\"Encoder\" width=\"450\" height=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_layer: EncoderLayer, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = clone_module(module=encoder_layer, num_clones=num_layers)\n",
    "        self.layer_norm = nn.LayerNorm(encoder_layer.d_model)\n",
    "\n",
    "    def forward(self, input: Tensor, mask: Optional[Tensor]=None) -> Tensor:\n",
    "        \"\"\"This method is the forward pass of the Encoder class. The output of the current EncoderLayer is\n",
    "           passed as input to the next EncoderLayer. We have 6 identical EncoderLayers stacked on top of \n",
    "           each other. The output of the last EncoderLayer is passed through a Layer Normalization layer\n",
    "           and returned as the final output of the Encoder\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Input to the Encoder i.e., embeddings of the tokenized src sequences.\n",
    "                            input: [batch_size, seq_len, d_model]\n",
    "            mask (Optional[Tensor], optional): Boolean mask to be applied during attention scores calculation.\n",
    "                                               mask: [batch_size, 1, seq_len, seq_len]. Defaults to None.\n",
    "                            \n",
    "        Returns:\n",
    "            Tensor: Output of the Encoder i.e., encoded src sentences.\n",
    "                    output: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        output = input\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            # Pass the output of the previous EncoderLayer to the current EncoderLayer.\n",
    "            output = encoder_layer(input=output, mask=mask)\n",
    "        return self.layer_norm(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder:  Encoder(\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0-5): 6 x EncoderLayer(\n",
      "      (self_attention): MultiHeadedAttention(\n",
      "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "        (linear_layers): ModuleList(\n",
      "          (0-3): 4 x Linear(in_features=8, out_features=8, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (feed_forward): FeedForwardNN(\n",
      "        (linear_layer_1): Linear(in_features=8, out_features=16, bias=True)\n",
      "        (linear_layer_2): Linear(in_features=16, out_features=8, bias=True)\n",
      "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (sublayer_wrappers): ModuleList(\n",
      "        (0-1): 2 x SubLayerWrapper(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(encoder_layer=encoder_layer, num_layers=6)\n",
    "print(\"encoder: \", encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are just creating a random mask for testing here. This is not how masks are created to be used \n",
    "# in the transformers. Refer to 'step_7_data_batching_and_masking.ipynb' to understand how masks \n",
    "# are created in the transformers.\n",
    "def construct_random_mask(batch_size: int, seq_len: int) -> Tensor:\n",
    "    # If some index is set to False, then it will be masked out.\n",
    "    mask = torch.randn(size=(batch_size, 1, seq_len, seq_len)) > 0.5\n",
    "    return mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([3, 1, 4, 4])\n",
      "mask: \n",
      " tensor([[[[ True, False, False, False],\n",
      "          [False,  True,  True, False],\n",
      "          [ True, False, False, False],\n",
      "          [False, False,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[False, False, False, False],\n",
      "          [False, False, False,  True],\n",
      "          [ True,  True, False, False],\n",
      "          [False,  True,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[ True, False, False,  True],\n",
      "          [False,  True,  True,  True],\n",
      "          [False, False, False, False],\n",
      "          [False, False, False, False]]]])\n"
     ]
    }
   ],
   "source": [
    "mask = construct_random_mask(batch_size=batch_size, seq_len=seq_len)\n",
    "print(\"shape: \", mask.shape)\n",
    "print(\"mask: \\n\", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_output:  tensor([[[-0.3792, -1.9030,  0.3590,  1.7789,  0.0416,  0.8241, -0.3721,\n",
      "          -0.3493],\n",
      "         [ 0.2684, -0.6450,  0.9577,  0.3035,  1.5067, -1.0332, -1.7380,\n",
      "           0.3798],\n",
      "         [-1.2583, -0.2144,  0.2131,  0.3853,  1.7022, -0.4989, -1.3844,\n",
      "           1.0554],\n",
      "         [ 0.3593, -0.3572,  1.4368, -0.2228,  0.5058, -1.0850, -1.7314,\n",
      "           1.0946]],\n",
      "\n",
      "        [[-0.0035, -0.9030,  1.2484, -0.0305,  1.2161, -1.0708, -1.4315,\n",
      "           0.9749],\n",
      "         [-0.3114, -0.7708,  1.2795,  0.1511,  1.1976, -1.1423, -1.3911,\n",
      "           0.9874],\n",
      "         [-0.0956, -1.5132,  1.5087,  0.6951,  0.8065, -0.5209, -1.3437,\n",
      "           0.4631],\n",
      "         [-0.4436, -0.9031,  1.2466, -0.0507,  1.5930, -1.0334, -1.1376,\n",
      "           0.7290]],\n",
      "\n",
      "        [[-1.7342, -0.9609,  0.1137,  1.1219, -0.4349,  1.4785, -0.2019,\n",
      "           0.6177],\n",
      "         [-1.8125, -0.2758, -0.6063,  1.5421,  0.0272,  1.3618, -0.1888,\n",
      "          -0.0477],\n",
      "         [-0.8641, -0.9186, -0.5245,  1.7321, -0.8503,  1.4819,  0.2983,\n",
      "          -0.3549],\n",
      "         [-1.1104, -0.3283, -0.8702,  1.9813, -0.6071,  1.0814,  0.3891,\n",
      "          -0.5357]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder_output = encoder(input=input_data, mask=mask)\n",
    "print(\"encoder_output: \", encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
