{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) How to load a dataset from the Hugging Face Datasets library.\n",
    "# 2) How to explore the dataset and the data it contains.\n",
    "# \n",
    "# I am building a transformer model for English to Telugu translation. So, we need a dataset of English and Telugu \n",
    "# sentences. I will be using the dataset from the Hugging Face Datasets library. \n",
    "# The dataset is called ai4bharat/samanantar (https://huggingface.co/datasets/ai4bharat/samanantar). It contains \n",
    "# sentences for several Indian languages. However, I will be using only the English to Telugu translation pairs.\n",
    "# \n",
    "# This notebook explores the ai4bharat dataset and the data it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path at which the smaller datasets created from the original dataset are stored.\n",
    "AI4_BHARAT_DATA_PATH = \"../../Data/AI4Bharat\"\n",
    "# Just a separator to print in the console.\n",
    "SEPARATOR_LENGTH = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This piece of code downloads the entire AI4BHARAT dataset that is about 7.5GB.\n",
    "# So, it might take very long when you run this line for the first time. For the subsequent runs, it will load \n",
    "# the dataset from the machine itself and so it should be significantly faster.\n",
    "# \n",
    "# Here we are only loading the English to Telugu translation dataset for now. However, the entire dataset is\n",
    "# downloaded to the machine.\n",
    "en_te_translation_dataset = load_dataset(\"ai4bharat/samanantar\", 'te', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'src', 'tgt'],\n",
      "        num_rows: 4946035\n",
      "    })\n",
      "})\n",
      "<class 'datasets.dataset_dict.DatasetDict'>\n"
     ]
    }
   ],
   "source": [
    "print(en_te_translation_dataset)\n",
    "# We can essentaially use this as a dictionary to access the data.\n",
    "print(type(en_te_translation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['idx', 'src', 'tgt'],\n",
      "    num_rows: 4946035\n",
      "})\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(en_te_translation_dataset[\"train\"])\n",
    "# Notice the type of the dataset. This is the hugging face dataset object.\n",
    "print(type(en_te_translation_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 0, 'src': 'Have you heard about Foie gras?', 'tgt': 'ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?'}\n",
      "{'idx': 10000, 'src': 'You eat ants?', 'tgt': 'మీరు చీమలు తినడానికి?'}\n",
      "{'idx': 1249849, 'src': 'Ban on international flights extended till Oct 31', 'tgt': 'అంతర్జాతీయ విమాన సర్వీసులపై నిషేధాన్ని కేంద్ర ప్రభుత్వం అక్టోబర్ 31 వరకు పొడగించింది'}\n"
     ]
    }
   ],
   "source": [
    "# Note that the translation dataset itself is not perfect. Some of the English sentences are not translated very \n",
    "# well to Telugu. For example, a good translation for second sentence in the dataset is \"మీరు చీమలను తింటారా?\".\n",
    "print(en_te_translation_dataset[\"train\"][0])\n",
    "print(en_te_translation_dataset[\"train\"][10000])\n",
    "print(en_te_translation_dataset[\"train\"][1249849])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset to be used to train the translation model.\n",
    "train_dataset = en_te_translation_dataset[\"train\"].select(range(0, 250000))\n",
    "# Dataset to be used for validation purposes.\n",
    "validation_dataset = en_te_translation_dataset[\"train\"].select(range(4946034, 4941035, -1))\n",
    "# Complete dataset. This will be used for training the Tokenizers.\n",
    "full_en_te_dataset = en_te_translation_dataset[\"train\"]\n",
    "# Example datasets with 200 examples to test run and resolve any issues in the model.\n",
    "# A very small dataset to perform sample runs and debug issues.\n",
    "debug_dataset = en_te_translation_dataset[\"train\"].select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset:  Dataset({\n",
      "    features: ['idx', 'src', 'tgt'],\n",
      "    num_rows: 250000\n",
      "})\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "validation_dataset:  Dataset({\n",
      "    features: ['idx', 'src', 'tgt'],\n",
      "    num_rows: 4999\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "debug_dataset:  Dataset({\n",
      "    features: ['idx', 'src', 'tgt'],\n",
      "    num_rows: 200\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "full_en_te_dataset:  Dataset({\n",
      "    features: ['idx', 'src', 'tgt'],\n",
      "    num_rows: 4946035\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"train_dataset: \", train_dataset)\n",
    "print(type(train_dataset))\n",
    "print(\"-\" * SEPARATOR_LENGTH)\n",
    "print(\"validation_dataset: \", validation_dataset)\n",
    "print(\"-\" * SEPARATOR_LENGTH)\n",
    "print(\"debug_dataset: \", debug_dataset)\n",
    "print(\"-\" * SEPARATOR_LENGTH)\n",
    "print(\"full_en_te_dataset: \", full_en_te_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 250000/250000 [00:00<00:00, 1419601.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4999/4999 [00:00<00:00, 137025.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 200/200 [00:00<00:00, 30148.82 examples/s]\n",
      "Saving the dataset (3/3 shards): 100%|██████████| 4946035/4946035 [00:02<00:00, 1740910.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Saving the filtered datasets to the disk. These datasets will be used for training the transformer model.\n",
    "# Running thie cell will replace the existing datasets in this repository although the dataset itself should\n",
    "# remain the same.\n",
    "train_dataset.save_to_disk(dataset_path=f\"{AI4_BHARAT_DATA_PATH}/train_dataset\")\n",
    "validation_dataset.save_to_disk(dataset_path=f\"{AI4_BHARAT_DATA_PATH}/validation_dataset\")\n",
    "debug_dataset.save_to_disk(dataset_path=f\"{AI4_BHARAT_DATA_PATH}/debug_dataset\")\n",
    "full_en_te_dataset.save_to_disk(dataset_path=f\"{AI4_BHARAT_DATA_PATH}/full_en_te_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['idx', 'src', 'tgt'],\n",
      "    num_rows: 250000\n",
      "})\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "{'idx': 0, 'src': 'Have you heard about Foie gras?', 'tgt': 'ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?'}\n",
      "{'idx': 10000, 'src': 'You eat ants?', 'tgt': 'మీరు చీమలు తినడానికి?'}\n"
     ]
    }
   ],
   "source": [
    "# Load the saved datasets and print the data points which we printed from the original dataset.\n",
    "# Notice that the datapoints are exactly the same as before as they should be.\n",
    "train_dataset_loaded = load_from_disk(dataset_path=f\"{AI4_BHARAT_DATA_PATH}/train_dataset\")\n",
    "print(train_dataset_loaded)\n",
    "print(type(train_dataset_loaded))\n",
    "print(\"-\" * SEPARATOR_LENGTH)\n",
    "print(train_dataset_loaded[0])\n",
    "print(train_dataset_loaded[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': [0, 1, 2, 3, 4],\n",
       " 'src': ['Have you heard about Foie gras?',\n",
       "  'I never thought of acting in films.',\n",
       "  'Installed Software',\n",
       "  'A case has been registered under Sections 302 and 376, IPC.',\n",
       "  'Of this, 10 people succumbed to the injuries.'],\n",
       " 'tgt': ['ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?',\n",
       "  'సూర్య సినిమాల్లో నటించాలని ఎప్పుడూ అనుకోలేదు.',\n",
       "  'స్థాపించబడిన సాఫ్ట్\\u200dవేర్',\n",
       "  'నిందితులపై సెక్షన్ 376 మరియు 302ల కింద కేసు నమోదు చేశాం.',\n",
       "  'అందులో 10 మంది తీవ్రంగా గాయపడ్డారు.']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that accessing the hugging face dataset this way is returning a dictionary. This is important.\n",
    "# I ran into extremely annoying bugs in the later notebooks not noticing this before hand.\n",
    "debug_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(debug_dataset[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# Use this format to find the size of the dataset i.e., number of examples in the dataset.\n",
    "print(debug_dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 4,\n",
       " 'src': 'Of this, 10 people succumbed to the injuries.',\n",
       " 'tgt': 'అందులో 10 మంది తీవ్రంగా గాయపడ్డారు.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the data_point at index 4 which is a dictionary in this case.\n",
    "debug_dataset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
