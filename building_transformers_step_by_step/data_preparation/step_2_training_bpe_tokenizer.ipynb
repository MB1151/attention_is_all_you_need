{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "#\n",
    "# 1) What is tokenization?\n",
    "# 2) What is Byte Pair Encoding?\n",
    "# 3) How to use HuggingFace's Tokenizer class to train a byte pair encoding tokenizer? \n",
    "#\n",
    "# NOTE: Use the pre-trained spacy tokenizerS (step_2_tokenization_with_spacy) for tokenization if you are\n",
    "# looking for a simple get it done tokenization solution and not looking to train Byte Pair Encoding tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources to learn about Tokenization:\n",
    "#\n",
    "# 1) https://www.youtube.com/watch?v=zduSFxRajkE\n",
    "#       -- Explains tokenization and creates a byte pair encoding tokenizer from scratch.\n",
    "#       -- Best video by far to understand byte level BPE tokenization.\n",
    "#       -- Very long video.\n",
    "#       -- MUST WATCH - MUST WATCH - MUST WATCH - MUST WATCH - MUST WATCH - MUST WATCH.\n",
    "# 2) https://realpython.com/introduction-to-python-generators/\n",
    "#       -- Excellent resource to learn about Generators and yield statement in python.\n",
    "# 3) https://github.com/huggingface/tokenizers/blob/14a07b06e4a8bd8f80d884419ae4630f5a3d8098/bindings/python/py_src/tokenizers/implementations/byte_level_bpe.py#L10\n",
    "#       -- ByteLevelBPETokenizer class in github. \n",
    "#       -- Could not find any official HuggingFace documentation for this class. So, had to refer to the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace imports to train my own byte level BPE tokenizer.\n",
    "from tokenizers import ByteLevelBPETokenizer # type: ignore\n",
    "from datasets import load_from_disk\n",
    "import datasets\n",
    "from typing import Generator, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI4_BHARAT_DATA_PATH = \"../../Data/AI4Bharat\"\n",
    "ENGLISH_TOKENIZER_SAVE_PATH = \"../../Data/trained_models/tokenizers/bpe/bpe_english_tokenizer\"\n",
    "TELUGU_TOKENIZER_SAVE_PATH = \"../../Data/trained_models/tokenizers/bpe/bpe_telugu_tokenizer\"\n",
    "# Number of tokens in the vocabulary of the tokenizer.\n",
    "ENGLISH_VOCAB_SIZE = 30000\n",
    "TELUGU_VOCAB_SIZE = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = f\"{AI4_BHARAT_DATA_PATH}/full_en_te_dataset\"\n",
    "tokenizer_train_dataset = load_from_disk(dataset_path=train_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need an iterator to iterate on the datapoints in the tokenizer dataset. In general, the datasets could\n",
    "# be arbitrarily large and we do not want to load the entire dataset into memory at once. Using a lazy iterator \n",
    "# (generator) ensures that not all the data is loaded into memory at once.\n",
    "# The general syntax for a generator type hint is Generator[YieldType, SendType, ReturnType]. In our case, \n",
    "# we are only yielding strings and not sending or returning anything, so we use Generator[str, None, None].\n",
    "def get_data_iterator(input_dataset: datasets.arrow_dataset.Dataset, language: str) -> Generator[str, None, None]:\n",
    "    for en_te_datapoint in input_dataset:\n",
    "        if language == \"en\":\n",
    "            yield en_te_datapoint[\"src\"]\n",
    "        else:\n",
    "            yield en_te_datapoint[\"tgt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(train_dataset: datasets.arrow_dataset.Dataset, language: str, vocab_size: Optional[int] = ENGLISH_VOCAB_SIZE) -> ByteLevelBPETokenizer:\n",
    "    # Use BPE to train a ByteLevel BPE tokenizer.\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    # Train the tokenizer on the ai4bharat mini train dataset. train_from_iterator is used so that the entire \n",
    "    # dataset is not loaded into memory at once.\n",
    "    tokenizer.train_from_iterator(iterator=get_data_iterator(input_dataset=train_dataset, language=language), \n",
    "                                  vocab_size= vocab_size, \n",
    "                                  special_tokens=[\"<sos>\", \"<eos>\", \"<pad>\", \"<unk>\"])\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note that we don't need to train two separate tokenizers for English and Telugu. We can train a single tokenizer\n",
    "# on the entire dataset. The tokenizer will learn the vocabulary of both languages. However, we made a design choice\n",
    "# here to train a separate tokenizers for each language. Each tokenizer performs merges and learns tokens based \n",
    "# on the language it is trained on. \n",
    "# Training English tokenizer.\n",
    "en_tokenizer = train_tokenizer(train_dataset=tokenizer_train_dataset, language=\"en\", vocab_size=ENGLISH_VOCAB_SIZE)\n",
    "# Training Telugu tokenizer\n",
    "te_tokenizer = train_tokenizer(train_dataset=tokenizer_train_dataset, language=\"te\", vocab_size=TELUGU_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(vocabulary_size=32000, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False) 32000 <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>\n",
      "Tokenizer(vocabulary_size=32000, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False) 32000 <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>\n"
     ]
    }
   ],
   "source": [
    "print(en_tokenizer, en_tokenizer.get_vocab_size(), type(en_tokenizer))\n",
    "print(te_tokenizer, te_tokenizer.get_vocab_size(), type(te_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> [('ĠSadananda', 25660), ('Ġdoll', 6059), ('Ġdefended', 24073), ('Ġhung', 10345), ('ĠJaitley', 5551), ('Ġclusters', 20086), ('Ġfranchise', 10877), ('Ġembroiled', 27552), ('eep', 983), ('Ġbr', 1019)]\n",
      "<class 'dict'> [('randomization', 22668), ('à°³à°¹à°°', 29099), ('à°¦à°Ł', 550), ('à°¦à°Ĺà°²', 3050), ('Ġà°°à°¡', 9637), ('munity', 13471), ('à°°à°¯à°ªà°Ł', 16107), ('ette', 25747), ('Ġà°Ķà°°à°¯', 27160), ('1951', 6895)]\n"
     ]
    }
   ],
   "source": [
    "# We don't need to specifically build vocabulary again if we use the HuggingFace Tokenizer. This is different from \n",
    "# using spacy tokenizer where we build our own vocabulary from the dataset. Though spacy provides inbuilt \n",
    "# vocabulary, it is not based on our training dataset but based on pre-existing training corpus used to train the \n",
    "# word based spacy tokenizer.\n",
    "en_vocab = en_tokenizer.get_vocab()\n",
    "te_vocab = te_tokenizer.get_vocab()\n",
    "print(type(en_vocab), list(en_vocab.items())[:10])\n",
    "print(type(te_vocab), list(te_vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 3 2\n",
      "0 1 3 2\n",
      "5463 5463 None\n",
      "[5463, 779, 779] ['pet', 'ss', 'ss']\n"
     ]
    }
   ],
   "source": [
    "# Just to show that the special tokens are present in the vocabulary. \n",
    "print(en_tokenizer.token_to_id(\"<sos>\"), en_tokenizer.token_to_id(\"<eos>\"), en_tokenizer.token_to_id(\"<unk>\"), en_tokenizer.token_to_id(\"<pad>\"))\n",
    "print(te_tokenizer.token_to_id(\"<sos>\"), te_tokenizer.token_to_id(\"<eos>\"), te_tokenizer.token_to_id(\"<unk>\"), te_tokenizer.token_to_id(\"<pad>\"))\n",
    "# Note that the token_to_id method returns None if the token is not directly present in the vocabulary. Hence, converting \n",
    "# 'petssss' to id returns None. \n",
    "print(en_tokenizer.token_to_id(\"pet\"), en_tokenizer.encode(\"pet\").ids[0], en_tokenizer.token_to_id(\"petssss\"))\n",
    "# Here, you can see that 'petssss' is a combination of 3 tokens 'pet', 'ss' and 'ss'. Hence, the ids for these tokens are \n",
    "# returned.\n",
    "print(en_tokenizer.encode(\"petssss\").ids, en_tokenizer.encode(\"petssss\").tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_encoded_to_token_ids:  [44, 3089, 577, 393, 2188, 394, 1267, 28779, 8204, 17]\n",
      "sentence_encoded_to_tokens:  ['I', 'Ġhope', 'Ġpeople', 'Ġwill', 'Ġfind', 'Ġthis', 'Ġrep', 'ository', 'Ġuseful', '.']\n",
      "token_ids_decoded_to_sentence:  I hope people will find this repository useful.\n"
     ]
    }
   ],
   "source": [
    "encoded_english_sentence = en_tokenizer.encode(\"I hope people will find this repository useful.\")\n",
    "print(\"sentence_encoded_to_token_ids: \", encoded_english_sentence.ids)\n",
    "print(\"sentence_encoded_to_tokens: \", encoded_english_sentence.tokens)\n",
    "decoded_english_sentence = en_tokenizer.decode(encoded_english_sentence.ids)\n",
    "print(\"token_ids_decoded_to_sentence: \", decoded_english_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_encoded_to_token_ids:  [325, 269, 370, 263, 303, 265, 272, 283, 274, 291, 264, 281, 265, 268, 457, 263, 280, 283, 278, 266, 496, 305, 299, 264, 302, 276, 308, 265, 268, 263, 268, 264, 289, 364, 273, 263, 267, 285, 263, 388, 266, 272, 263, 280, 269, 512, 263, 277, 275, 560, 263, 1379, 537, 266, 285, 310, 610, 294, 286, 294, 273, 264]\n",
      "sentence_encoded_to_tokens:  ['à°ħ', 'à°Ĥ', 'à°¤à°°', 'à±į', 'à°ľ', 'à°¾', 'à°¤', 'à±Ģ', 'à°¯', 'Ġà°µ', 'à°¿', 'à°®', 'à°¾', 'à°¨', 'Ġà°¸à°°', 'à±į', 'à°µ', 'à±Ģ', 'à°¸', 'à±ģ', 'à°²à°ª', 'à±Ī', 'Ġà°¨', 'à°¿', 'à°·', 'à±ĩ', 'à°§', 'à°¾', 'à°¨', 'à±į', 'à°¨', 'à°¿', 'Ġà°ķ', 'à±ĩà°Ĥ', 'à°¦', 'à±į', 'à°°', 'Ġà°ª', 'à±į', 'à°°à°Ń', 'à±ģ', 'à°¤', 'à±į', 'à°µ', 'à°Ĥ', 'Ġà°ħà°ķ', 'à±į', 'à°Ł', 'à±ĭ', 'à°¬à°°', 'à±į', 'Ġ31', 'Ġà°µà°°à°ķ', 'à±ģ', 'Ġà°ª', 'à±Ĭ', 'à°¡à°Ĺ', 'à°¿à°Ĥ', 'à°ļ', 'à°¿à°Ĥ', 'à°¦', 'à°¿']\n",
      "token_ids_decoded_to_sentence:  అంతర్జాతీయ విమాన సర్వీసులపై నిషేధాన్ని కేంద్ర ప్రభుత్వం అక్టోబర్ 31 వరకు పొడగించింది\n"
     ]
    }
   ],
   "source": [
    "encoded_telugu_sentence = te_tokenizer.encode(\"అంతర్జాతీయ విమాన సర్వీసులపై నిషేధాన్ని కేంద్ర ప్రభుత్వం అక్టోబర్ 31 వరకు పొడగించింది\")\n",
    "print(\"sentence_encoded_to_token_ids: \", encoded_telugu_sentence.ids)\n",
    "# This is my assumption in understanding this behavior. Since we are using ByteLevelBPETokenizer, the tokens are not valid Telugu characters \n",
    "# but some random combination of bytes. Hence, the tokens themselves are not human readable. However, when decoded using the tokenizer, we\n",
    "# get the original Telugu sentence back correctly.\n",
    "print(\"sentence_encoded_to_tokens: \", encoded_telugu_sentence.tokens)\n",
    "decoded_telugu_sentence = te_tokenizer.decode(encoded_telugu_sentence.ids)\n",
    "print(\"token_ids_decoded_to_sentence: \", decoded_telugu_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../Data/trained_models/tokenizers/telugu_tokenizer/vocab.json',\n",
       " '../../Data/trained_models/tokenizers/telugu_tokenizer/merges.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained tokenizers to disk.\n",
    "en_tokenizer.save_model(ENGLISH_TOKENIZER_SAVE_PATH)\n",
    "te_tokenizer.save_model(TELUGU_TOKENIZER_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_encoded_to_token_ids:  [44, 3089, 577, 393, 2188, 394, 1267, 28779, 8204, 17]\n"
     ]
    }
   ],
   "source": [
    "# Load the saved English tokenizer from disk.\n",
    "en_tokenizer_loaded = ByteLevelBPETokenizer.from_file(vocab_filename=f\"{ENGLISH_TOKENIZER_SAVE_PATH}/vocab.json\", \n",
    "                                                      merges_filename=f\"{ENGLISH_TOKENIZER_SAVE_PATH}/merges.txt\")\n",
    "# Confirm that the loaded tokenizer is working as expected.\n",
    "encoded_english_sentence_2 = en_tokenizer_loaded.encode(\"I hope people will find this repository useful.\")\n",
    "print(\"sentence_encoded_to_token_ids: \", encoded_english_sentence_2.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_encoded_to_token_ids:  [325, 269, 370, 263, 303, 265, 272, 283, 274, 291, 264, 281, 265, 268, 457, 263, 280, 283, 278, 266, 496, 305, 299, 264, 302, 276, 308, 265, 268, 263, 268, 264, 289, 364, 273, 263, 267, 285, 263, 388, 266, 272, 263, 280, 269, 512, 263, 277, 275, 560, 263, 1379, 537, 266, 285, 310, 610, 294, 286, 294, 273, 264]\n"
     ]
    }
   ],
   "source": [
    "# Load the saved Telugu tokenizer from disk.\n",
    "te_tokenizer_loaded = ByteLevelBPETokenizer.from_file(vocab_filename=f\"{TELUGU_TOKENIZER_SAVE_PATH}/vocab.json\", \n",
    "                                                      merges_filename=f\"{TELUGU_TOKENIZER_SAVE_PATH}/merges.txt\")\n",
    "# Confirm that the loaded tokenizer is working as expected.\n",
    "encoded_telugu_sentence_2 = te_tokenizer_loaded.encode(\"అంతర్జాతీయ విమాన సర్వీసులపై నిషేధాన్ని కేంద్ర ప్రభుత్వం అక్టోబర్ 31 వరకు పొడగించింది\")\n",
    "print(\"sentence_encoded_to_token_ids: \", encoded_telugu_sentence_2.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
