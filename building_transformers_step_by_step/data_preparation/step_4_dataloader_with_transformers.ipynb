{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "# \n",
    "# 1) How to create dataset iterators for English-Telugu translation data to be used by Transformers?\n",
    "# 3) How to create DataLoaders that group the data based on the length of the sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources to go through to understand this notebook:\n",
    "#\n",
    "# 1) https://www.youtube.com/watch?v=IGu7ivuy1Ag\n",
    "#       -- Gives a high level walk through of how the input and the output are structured and transformed in a transformer model.\n",
    "#       -- This is important to understand so that we how how to batch our input data.\n",
    "# 2) https://www.youtube.com/watch?v=dZzVA6VbAR8\n",
    "#       -- Decent video that gives an overview about the Data Preparation process in transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from tokenizers import ByteLevelBPETokenizer  # type: ignore\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Callable, Dict, List, Iterator, Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we created a Pytorch Dataloader on a simple dataset of tuples. However, the \\\n",
    "input data in the transformer models is a bunch of English, Telugu sentences. In this notebook, we \\\n",
    "build on the concepts from step_3 and show how to build Pytorch DataLoader on the English - Telugu \\\n",
    "sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path (directory) where all the data used by this project is stored.\n",
    "AI4_BHARAT_DATA_PATH = \"../../Data/AI4Bharat\"\n",
    "# Number of tokens in the vocabulary of the tokenizer.\n",
    "VOCAB_SIZE = 30000\n",
    "START_TOKEN = \"<sos>\"\n",
    "END_TOKEN = \"<eos>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer model expects a sequence of tokens as input and outputs one vector per token.   \\\n",
    "The DataLoaders should just provide the batches of sequence of tokens to the transformer model  \\\n",
    "during training given the training dataset as input. So, the tokenization should be done within \\\n",
    "the collate function of the Dataloaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "{'idx': 0, 'src': 'Have you heard about Foie gras?', 'tgt': 'ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?'}\n",
      "250000\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "{'idx': 0, 'src': 'Have you heard about Foie gras?', 'tgt': 'ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?'}\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# Load the train dataset we have created and saved to disk in 'step_1_data_exploration.ipynb' notebook. \n",
    "en_te_translation_dataset = datasets.load_from_disk(f\"{AI4_BHARAT_DATA_PATH}/train_dataset\")\n",
    "print(type(en_te_translation_dataset))\n",
    "print(en_te_translation_dataset[0])\n",
    "print(len(en_te_translation_dataset))\n",
    "print(\"-\" * 150)\n",
    "en_te_debug_dataset = datasets.load_from_disk(f\"{AI4_BHARAT_DATA_PATH}/debug_dataset\")\n",
    "print(en_te_debug_dataset[0])\n",
    "print(len(en_te_debug_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Tokenizers.\n",
    "\n",
    "We first train tokenizers and pass these tokenizers to the DataLoader so that tokenization is    <br>\n",
    "handled within the DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_extractor(data_point: dict[str, str], language: str) -> str:\n",
    "    \"\"\"Extracts the appropriate text from the example in the dataset based on the language.\n",
    "\n",
    "    Args:\n",
    "        data_point (dict[str, str]): A single example from the dataset containing the text in the form of \n",
    "                                     a dictionary. The sources sentence is stored in the key 'src' and the \n",
    "                                     target sentence is stored in the key 'tgt'.\n",
    "        language (str): Language of the text to be extracted from the data_point.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Raises an error if the language is not 'english' or 'telugu'.\n",
    "\n",
    "    Returns:\n",
    "        str: The text in the data_point.\n",
    "    \"\"\"\n",
    "    if language == \"english\":\n",
    "        return data_point[\"src\"]\n",
    "    elif language == \"telugu\":\n",
    "        return data_point[\"tgt\"]\n",
    "    raise ValueError(\"Language should be either 'english' or 'telugu'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easier to create classes to create tokenizers and keep track of the required data. We will also    <br>\n",
    "use these classes in the actual implementation of the model. The code used within the tokenizer <br>\n",
    "classes is same as what we have seen in step_2, but just a bit organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a base class that will be inherited by the actual tokenizer classes.\n",
    "class BaseTokenizer(ABC):\n",
    "    \"\"\"A class created to hold different kinds of tokenizers and handle the token encoding in a common way.\n",
    "       Here, we only use SpacyTokenizer and HuggingFaceTokenizer.\"\"\"\n",
    "    def __init__(self, language: str, tokenizer_type: str):\n",
    "        self.language = language\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "        self.special_tokens = [START_TOKEN, END_TOKEN, PAD_TOKEN, UNK_TOKEN]\n",
    "\n",
    "    # Abstract methods need to be overridden by the child class. It raises TypeError if not overridden.\n",
    "    @abstractmethod\n",
    "    def initialize_tokenizer_and_build_vocab(self, \n",
    "                                             data_iterator: datasets.arrow_dataset.Dataset, \n",
    "                                             text_extractor: Callable[[dict[str, str], str], str], \n",
    "                                             max_vocab_size: Optional[int] = VOCAB_SIZE):\n",
    "        \"\"\"Initializes the tokenizers and builds the vocabulary for the given dataset.\n",
    "\n",
    "        Args:\n",
    "            data_iterator (datasets.arrow_dataset.Dataset): An iterator that gives input sentences (text) when iterated upon.\n",
    "            text_extractor (Callable[[dict[str, str], str], str]): A function that extracts the appropriate text from the input \n",
    "                dataset. This parameter is added to make the tokenizer class independent of the input dataset format. If not \n",
    "                provided as an argument, we will have to extract the text from the dataset within the 'CustomTokenizer' class \n",
    "                which makes it dependent on the dataset format. \n",
    "            max_vocab_size (int, optional): Maximum size of the vocabulary to create from the input data corpus. Defaults \n",
    "                                            to VOCAB_SIZE (30000).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    # Abstract methods need to be overridden by the child class. It raises TypeError if not overridden.\n",
    "    @abstractmethod\n",
    "    def tokenize(self, text: str) -> list[str]:\n",
    "        \"\"\"Returns the individual tokens (possibly readable text) for the given text.\"\"\"\n",
    "        pass\n",
    "\n",
    "    # Abstract methods need to be overridden by the child class. It raises TypeError if not overridden.\n",
    "    @abstractmethod\n",
    "    def encode(self, text: str) -> list[str]:\n",
    "        \"\"\"Returns the encoded token ids for the given text.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"Converts the series of token ids back to the original text.\n",
    "\n",
    "        Args:\n",
    "            token_ids (List[int]): A list of token ids corresponding to some text.\n",
    "\n",
    "        Returns:\n",
    "            str: The original text corresponding to the token ids.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    # Abstract methods need to be overridden by the child class. It raises TypeError if not overridden.\n",
    "    @abstractmethod\n",
    "    def get_token_id(self, token: str) -> int:\n",
    "        \"\"\"Returns the token id for the given token. If the token is not present in the vocabulary, it returns None.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer 'step_2_training_bpe_tokenizer.ipynb' and 'step_2_alternate_tokenization_with_spacy.ipynb' notebooks to understand this class better.\n",
    "class SpacyTokenizer(BaseTokenizer):\n",
    "    \"\"\"Creates a tokenizer that tokenizes the text using the Spacy tokenizer models.\"\"\"\n",
    "    def __init__(self, language: str):\n",
    "        super().__init__(language, \"spacy\")\n",
    "    \n",
    "    def initialize_tokenizer_and_build_vocab(self, \n",
    "                                             data_iterator: datasets.arrow_dataset.Dataset, \n",
    "                                             text_extractor: Callable[[dict[str, str], str], str], \n",
    "                                             max_vocab_size: int = 40000):\n",
    "        # Load spacy models for English text tokenization.\n",
    "        if self.language == \"english\":\n",
    "            self.tokenizer = spacy.load(\"en_core_web_sm\").tokenizer          \n",
    "        elif self.language == \"telugu\":\n",
    "            # Load spacy model for Telugu text tokenization.\n",
    "            self.tokenizer = spacy.blank(\"te\").tokenizer            \n",
    "        else:\n",
    "            # Raise an error for unknown language\n",
    "            pass\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.__build_vocab(data_iterator=data_iterator, text_extractor=text_extractor)\n",
    "\n",
    "    def tokenize(self, text: str) -> list[str]:\n",
    "        return [token.text for token in self.tokenizer(text)]\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        return self.vocab(self.tokenize(text))\n",
    "    \n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"Converts the series of token ids back to the original text.\n",
    "\n",
    "        Args:\n",
    "            token_ids (List[int]): A list of token ids corresponding to some text.\n",
    "\n",
    "        Returns:\n",
    "            str: The original text corresponding to the token ids.\n",
    "        \"\"\"\n",
    "        token_strings = self.vocab.lookup_tokens(token_ids)\n",
    "        # Here we are just attaching all the individual token strings with a space in between to form \n",
    "        # the original text. This does not give the exact original text but a close approximation. \n",
    "        # Using this here for simplicity. This might not always be the correct way to decode the text.\n",
    "        return \" \".join(token_strings)\n",
    "\n",
    "    def get_token_id(self, token: str) -> int:\n",
    "        return self.vocab([token])[0]\n",
    "\n",
    "    def __yield_tokens(self, data_iterator: datasets.arrow_dataset.Dataset, text_extractor: Callable[[dict[str, str], str], str]):\n",
    "        \"\"\"Returns a generator object that emits tokens for each sentence in the dataset\"\"\"\n",
    "        for data_point in data_iterator:\n",
    "            yield self.tokenize(text_extractor(data_point, self.language))\n",
    "\n",
    "    def __build_vocab(self, data_iterator: datasets.arrow_dataset.Dataset, text_extractor: Callable[[dict[str, str], str], str]):\n",
    "        \"\"\"Builds the vocabulary for the given dataset\"\"\"\n",
    "        self.vocab = build_vocab_from_iterator(iterator=self.__yield_tokens(data_iterator=data_iterator, text_extractor=text_extractor), \n",
    "                                               min_freq=2, \n",
    "                                               specials=self.special_tokens, \n",
    "                                               special_first=True, \n",
    "                                               max_tokens=self.max_vocab_size)\n",
    "        self.vocab.set_default_index(self.vocab[UNK_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_telugu_tokenizer: BaseTokenizer = SpacyTokenizer(language=\"telugu\")\n",
    "spacy_telugu_tokenizer.initialize_tokenizer_and_build_vocab(data_iterator=en_te_translation_dataset, text_extractor=text_extractor, max_vocab_size=VOCAB_SIZE)\n",
    "spacy_english_tokenizer: BaseTokenizer = SpacyTokenizer(language=\"english\")\n",
    "spacy_english_tokenizer.initialize_tokenizer_and_build_vocab(data_iterator=en_te_translation_dataset, text_extractor=text_extractor, max_vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ఇక', 'ఫ్రూట్', 'ఫ్లైస్', 'గురించి', 'మీరు', 'విన్నారా', '?']\n",
      "[94, 23082, 3, 72, 40, 18178, 7]\n",
      "ఇక ఫ్రూట్ <unk> గురించి మీరు విన్నారా ?\n",
      "['Have', 'you', 'heard', 'about', 'Foie', 'gras', '?']\n",
      "[1017, 46, 1003, 76, 3, 3, 20]\n",
      "Have you heard about <unk> <unk> ?\n",
      "token(<sos>) = 0 , token(<eos>) = 1\n",
      "token(<unk>) = 3 , token(<pad>) = 2\n",
      "token(NON_EXISTENT_TOKEN) = 3\n"
     ]
    }
   ],
   "source": [
    "# Note that we cannot add special tokens to the sentence and ask the spacy_tokenizer to tokenize. If we do that, it \n",
    "# will break the special tokens into simpler tokens again. This is because the spacy tokenizer was not trained \n",
    "# specifically to keep the special tokens intact during tokenization. It doesn't know about special tokens by \n",
    "# default. spacy uses a default tokenizer which is already trained by some rules. We haven't added any rules about \n",
    "# tokenization ourselves. However, since we added the special tokens to the vocabulary, spacy tokenizer maps them to \n",
    "# the corresponding token ids and returns them when prompted for ids.\n",
    "telugu_sentence = \"ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?\"\n",
    "print(spacy_telugu_tokenizer.tokenize(text=telugu_sentence))\n",
    "# This might print <unk> token id (3) for some of the tokens. This is because the spacy tokenizer might not have some\n",
    "# of these tokens in its vocabulary. We limited the vocabulary to 30000 tokens which probably left out a huge number\n",
    "# of tokens from the vocabulary.\n",
    "print(spacy_telugu_tokenizer.encode(text=telugu_sentence))\n",
    "print(spacy_telugu_tokenizer.decode(token_ids=spacy_telugu_tokenizer.encode(text=telugu_sentence)))  # type: ignore\n",
    "english_sentence = \"Have you heard about Foie gras?\"\n",
    "print(spacy_english_tokenizer.tokenize(text=english_sentence))\n",
    "print(spacy_english_tokenizer.encode(text=english_sentence))\n",
    "print(spacy_english_tokenizer.decode(token_ids=spacy_english_tokenizer.encode(text=english_sentence)))  # type: ignore\n",
    "print(\"token(<sos>) =\", spacy_english_tokenizer.get_token_id(START_TOKEN), \", token(<eos>) =\", spacy_english_tokenizer.get_token_id(END_TOKEN))\n",
    "print(\"token(<unk>) =\", spacy_english_tokenizer.get_token_id(UNK_TOKEN), \", token(<pad>) =\", spacy_english_tokenizer.get_token_id(PAD_TOKEN))\n",
    "print(\"token(NON_EXISTENT_TOKEN) =\", spacy_english_tokenizer.get_token_id(\"ఫ్రూట్\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer 'step_2_training_bpe_tokenizer_and_vocab.ipynb' notebook to understand this class better.\n",
    "# We train our own tokenizer in HuggingFace since it doesn't provide a tokenizer for Telugu by default.\n",
    "class BPETokenizer(BaseTokenizer):\n",
    "    \"\"\"Trains a tokenizer using HuggingFace libraries\"\"\"\n",
    "    def __init__(self, language: str):\n",
    "        super().__init__(language, \"hugging_face\")\n",
    "\n",
    "    def initialize_tokenizer_and_build_vocab(self, \n",
    "                                             data_iterator: datasets.arrow_dataset.Dataset, \n",
    "                                             text_extractor: Callable[[dict[str, str], str], str], \n",
    "                                             max_vocab_size: Optional[int] = VOCAB_SIZE):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.tokenizer = self.__train_tokenizer(data_iterator=data_iterator, text_extractor=text_extractor, max_vocab_size=max_vocab_size)\n",
    "\n",
    "    def tokenize(self, text: str) -> list[str]:\n",
    "        encoded_text = self.tokenizer.encode(text)\n",
    "        return encoded_text.tokens\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        encoded_text = self.tokenizer.encode(text)\n",
    "        return encoded_text.ids\n",
    "\n",
    "    # Spacy tokenizer doesn't have a built-in mechanism to generate text from token ids. It tokenizes the text \n",
    "    # based on a bunch of rules and doesn't know how to join the tokens back to form the original text. This \n",
    "    # is one of the additional advantages of using a BPE tokenizer over a spacy tokenizer because BPE algorithm \n",
    "    # is designed to both encode and decode text.\n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"Converts the series of token ids back to the original text.\n",
    "\n",
    "        Args:\n",
    "            token_ids (List[int]): A list of token ids corresponding to some text.\n",
    "\n",
    "        Returns:\n",
    "            str: The original text corresponding to the token ids.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.decode(token_ids)\n",
    "\n",
    "    def get_token_id(self, token: str) -> int:\n",
    "        return self.tokenizer.token_to_id(token)\n",
    "\n",
    "    # We need an iterator to train the tokenizer. Using an iterator ensures that not all \n",
    "    # the data is loaded into memory at once.\n",
    "    def __get_data_iterator(self, data_iterator: datasets.arrow_dataset.Dataset, text_extractor: Callable[[dict[str, str], str], str]):\n",
    "        for data_point in data_iterator:\n",
    "            yield text_extractor(data_point=data_point, language=self.language)\n",
    "\n",
    "    def __train_tokenizer(self, data_iterator: datasets.arrow_dataset.Dataset, \n",
    "                          text_extractor: Callable[[dict[str, str], str], str], \n",
    "                          max_vocab_size: Optional[int]=VOCAB_SIZE) -> ByteLevelBPETokenizer:\n",
    "        # Use BPE to train a ByteLevel BPE tokenizer.\n",
    "        tokenizer = ByteLevelBPETokenizer()\n",
    "        # train_from_iterator is used so that the entire dataset is not loaded into memory at once.\n",
    "        tokenizer.train_from_iterator(iterator=self.__get_data_iterator(data_iterator=data_iterator, text_extractor=text_extractor), \n",
    "                                      vocab_size= max_vocab_size, \n",
    "                                      special_tokens=self.special_tokens)\n",
    "        return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bpe_telugu_tokenizer: BaseTokenizer = BPETokenizer(language=\"telugu\")\n",
    "bpe_telugu_tokenizer.initialize_tokenizer_and_build_vocab(data_iterator=en_te_translation_dataset, text_extractor=text_extractor, max_vocab_size=30000)\n",
    "bpe_english_tokenizer: BaseTokenizer = BPETokenizer(language=\"english\")\n",
    "bpe_english_tokenizer.initialize_tokenizer_and_build_vocab(data_iterator=en_te_translation_dataset, text_extractor=text_extractor, max_vocab_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['à°ĩà°ķ', 'Ġà°«', 'à±į', 'à°°', 'à±Ĥ', 'à°Ł', 'à±į', 'Ġà°«', 'à±į', 'à°²', 'à±Ī', 'à°¸', 'à±į', 'Ġà°Ĺ', 'à±ģ', 'à°°', 'à°¿à°Ĥ', 'à°ļ', 'à°¿', 'Ġà°®', 'à±Ģ', 'à°°', 'à±ģ', 'Ġà°µ', 'à°¿', 'à°¨', 'à±į', 'à°¨', 'à°¾', 'à°°', 'à°¾?']\n",
      "[539, 360, 263, 267, 298, 277, 263, 360, 263, 270, 305, 278, 263, 322, 266, 267, 294, 286, 264, 293, 283, 267, 266, 291, 264, 268, 263, 268, 265, 267, 440]\n",
      "ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "['Have', 'Ġyou', 'Ġheard', 'Ġabout', 'ĠF', 'o', 'ie', 'Ġgras', '?']\n",
      "[3519, 452, 3123, 629, 510, 82, 485, 26172, 34]\n",
      "Have you heard about Foie gras?\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "token(<sos>) = 0 , token(<eos>) = 1\n",
      "token(<unk>) = 3 , token(<pad>) = 2\n",
      "token(NON_EXISTENT_TOKEN) = None\n"
     ]
    }
   ],
   "source": [
    "# This tokenizer conveniently handles special tokens by default. It can keep the special\n",
    "# tokens intact during tokenization. It knows about special tokens by default.\n",
    "telugu_sentence = \"ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?\"\n",
    "print(bpe_telugu_tokenizer.tokenize(text=telugu_sentence))\n",
    "print(bpe_telugu_tokenizer.encode(text=telugu_sentence))\n",
    "print(bpe_telugu_tokenizer.decode(token_ids=bpe_telugu_tokenizer.encode(text=telugu_sentence)))\n",
    "print(\"-\" * 150)\n",
    "english_sentence = \"Have you heard about Foie gras?\"\n",
    "print(bpe_english_tokenizer.tokenize(text=english_sentence))\n",
    "print(bpe_english_tokenizer.encode(text=english_sentence))\n",
    "print(bpe_english_tokenizer.decode(token_ids=bpe_english_tokenizer.encode(text=english_sentence)))\n",
    "print(\"-\" * 150)\n",
    "print(\"token(<sos>) =\", bpe_english_tokenizer.get_token_id(START_TOKEN), \", token(<eos>) =\", bpe_english_tokenizer.get_token_id(END_TOKEN))\n",
    "print(\"token(<unk>) =\", bpe_english_tokenizer.get_token_id(UNK_TOKEN), \", token(<pad>) =\", bpe_english_tokenizer.get_token_id(PAD_TOKEN))\n",
    "# Notice that BPE doesn't return the token id 3 (<UNK>) for the string \"NON_EXISTENT_TOKEN\" as it did for the spacy tokenizer. \n",
    "# This is because NON_EXISTENT_TOKEN can be broken down into smaller tokens by the BPE tokenizer. BPE never returns an unk\n",
    "# token id. It always breaks down the unknown token into smaller tokens. It is not even useful to have an unk token id in BPE.\n",
    "print(\"token(NON_EXISTENT_TOKEN) =\", bpe_english_tokenizer.get_token_id(\"NON_EXISTENT_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating pytorch Dataset from HuggingFace dataset.\n",
    "\n",
    "Lets create Pytorch Dataset based on the HuggingFace translation dataset. This is necessary to <br>\n",
    "to create the DataLoader in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to wrap our HuggingFace dataset into the torch Dataset to integrate with pytorch DataLoader.\n",
    "class HuggingFaceDatasetWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset: datasets.arrow_dataset.Dataset):\n",
    "        \"\"\"Initializes the HuggingFaceDatasetWrapper with the given dataset.\n",
    "\n",
    "        Args:\n",
    "            hf_dataset (datasets.arrow_dataset.Dataset): The hugging face dataset to be wrapped.\n",
    "        \"\"\"\n",
    "        self.dataset = hf_dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Extracts the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Length of the dataset.\n",
    "        \"\"\"\n",
    "        return self.dataset.num_rows\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Extracts the data_point at a particular index in the dataset.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the data_point to be extracted from the dataset.\n",
    "\n",
    "        Returns:\n",
    "            dict: Data_point at the given index in the dataset. This turns out to be a dictionary for our dataset but\n",
    "                  it could be any type in general.\n",
    "        \"\"\"\n",
    "        # Return the dataset at a particular index.\n",
    "        # The index provided will always be less then length (64 in this case) returned by __len__ function.\n",
    "        return self.dataset[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_translation_dataset = HuggingFaceDatasetWrapper(hf_dataset=en_te_translation_dataset)\n",
    "wrapped_debug_dataset = HuggingFaceDatasetWrapper(hf_dataset=en_te_debug_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 0, 'src': 'Have you heard about Foie gras?', 'tgt': 'ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?'}\n",
      "{'idx': 1, 'src': 'I never thought of acting in films.', 'tgt': 'సూర్య సినిమాల్లో నటించాలని ఎప్పుడూ అనుకోలేదు.'}\n",
      "{'idx': 2, 'src': 'Installed Software', 'tgt': 'స్థాపించబడిన సాఫ్ట్\\u200dవేర్'}\n",
      "{'idx': 3, 'src': 'A case has been registered under Sections 302 and 376, IPC.', 'tgt': 'నిందితులపై సెక్షన్ 376 మరియు 302ల కింద కేసు నమోదు చేశాం.'}\n",
      "{'idx': 4, 'src': 'Of this, 10 people succumbed to the injuries.', 'tgt': 'అందులో 10 మంది తీవ్రంగా గాయపడ్డారు.'}\n",
      "{'idx': 5, 'src': 'Her acting has been praised by critics.', 'tgt': 'నటనకు గాను విమర్శకుల నుంచి ప్రశంసలు పొందింది.'}\n",
      "{'idx': 6, 'src': 'The Bibles viewpoint on this is clearly indicated at Colossians 3: 9: Do not be lying to one another.', 'tgt': 'ఈ విషయంపై బైబిలు దృక్కోణం కొలొస్సయులు 3 :\\u2060 9లో “ఒకనితో ఒకడు అబద్ధమాడకుడి ” అని స్పష్టంగా సూచించబడింది.'}\n",
      "{'idx': 7, 'src': 'The incident was recorded in the CCTV footage.', 'tgt': 'ఈ ప్రమాద దృశ్యాలు సీసీటీవీ ఫుటేజ్\\u200cలో రికార్డ్ అయ్యాయి.'}\n",
      "{'idx': 8, 'src': 'Respect privacy', 'tgt': 'గోప్యత పాటించండి'}\n",
      "{'idx': 9, 'src': '5 lakh would be provided.', 'tgt': '5లక్షలు సాయం అందజేశారు.'}\n",
      "{'idx': 10, 'src': 'Super Bowl.', 'tgt': '\"\"\"సూపర్ బౌల్.\"'}\n"
     ]
    }
   ],
   "source": [
    "# A sample iteration on the Pytorch dataset to show the output.\n",
    "for idx, data_point in enumerate(wrapped_debug_dataset):\n",
    "    if idx > 10:\n",
    "        break\n",
    "    print(data_point)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# 'len' function returns the number of examples in the dataset. 'len' internally calls the '__len__'\n",
    "# method which is implemented above to return the 'num_rows' which is the number of examples in the\n",
    "# dataset.\n",
    "print(len(wrapped_debug_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating basic pytorch DataLoaders\n",
    "\n",
    "Lets create DataLoader object using the above translation dataset as shown in `step_3_datasets_and_dataloaders.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The role of collate_fn is to iterate on each batch of the dataset and convert the data in the batch into the format \n",
    "# that is required by the model (transformers in this case). A more extensively commented collate_fn is implemented \n",
    "# below in the same notebook. You will understand better there if something is not clear in this function.\n",
    "def transformer_collate_fn(batch: List[Dict]):\n",
    "    \"\"\"Converts the raw data in the batch into the format required by the model. In this case, it\n",
    "       converts the text to token ids, adds start, end, and padding tokens and batches the converted \n",
    "       data to be used by the model. \n",
    "\n",
    "    Args:\n",
    "        batch (List[Dict]): A batch of raw data points from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the source and target tensors in the batch.\n",
    "    \"\"\"\n",
    "    # Maximum length of the sentence. Every sentence in the batch will be brought to this length by padding.\n",
    "    MAX_LEN = 100\n",
    "    # Holds the list of processed source sentences (English sentences) from the batch. \n",
    "    src_list = []\n",
    "    # Holds the list of processed target sentences (Telugu sentences) from the batch. The data at corresponding indices \n",
    "    # in src_list and tgt_list are a pair i.e., tgt sentence at index 'j' is translation of src sentence at index 'j'.\n",
    "    tgt_list = []\n",
    "    # start of sentence token id to be added at the beginning of the sentence.\n",
    "    sos_id = torch.tensor([0])\n",
    "    # end of sentence token id to be added at the end of the sentence.\n",
    "    eos_id = torch.tensor([1])\n",
    "    index = 0\n",
    "    for data_point in batch:\n",
    "        src_sentence = data_point[\"src\"]\n",
    "        tgt_sentence = data_point[\"tgt\"]\n",
    "        # This if block is just to print the first sentence pair in the batch to understand how the batch looks.\n",
    "        if index == 0:\n",
    "            print(f\"src_sentence: {src_sentence}\")\n",
    "            print(f\"tgt_sentence: {tgt_sentence}\")\n",
    "            index += 1\n",
    "        # List of tensors to concatenate and dimension along which we want to concatenate the list of tensors.\n",
    "        processed_src = torch.cat([sos_id, torch.tensor(spacy_english_tokenizer.encode(src_sentence)), eos_id], dim=0)\n",
    "        processed_tgt = torch.cat([sos_id, torch.tensor(spacy_telugu_tokenizer.encode(tgt_sentence)), eos_id], dim=0)\n",
    "        # value parameter corresponds to the index we use to represent the padding token.\n",
    "        processed_src = torch.nn.functional.pad(processed_src, (0, MAX_LEN - len(processed_src)), value=2)\n",
    "        processed_tgt = torch.nn.functional.pad(processed_tgt, (0, MAX_LEN - len(processed_tgt)), value=2)\n",
    "        src_list.append(processed_src)\n",
    "        tgt_list.append(processed_tgt)\n",
    "    src = torch.stack(src_list)\n",
    "    tgt = torch.stack(tgt_list)\n",
    "    return (src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "src_sentence: Have you heard about Foie gras?\n",
      "tgt_sentence: ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?\n",
      "src: tensor([[   0, 1017,   46,  ...,    2,    2,    2],\n",
      "        [   0,   33,  375,  ...,    2,    2,    2],\n",
      "        [   0,    3, 5208,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [   0,   12, 1107,  ...,    2,    2,    2],\n",
      "        [   0,  125,   54,  ...,    2,    2,    2],\n",
      "        [   0, 1684, 1076,  ...,    2,    2,    2]])\n",
      "tgt: tensor([[    0,    94, 23082,  ...,     2,     2,     2],\n",
      "        [    0,  3189,  1262,  ...,     2,     2,     2],\n",
      "        [    0, 14009,     3,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [    0,  7898,   290,  ...,     2,     2,     2],\n",
      "        [    0,    15,     3,  ...,     2,     2,     2],\n",
      "        [    0,   648,  1622,  ...,     2,     2,     2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence: Will you enter politics?\n",
      "tgt_sentence: నయనతార రాజకీయాల్లోకి అడుగుపెట్టనున్నారా?\n",
      "src: tensor([[  0, 754,  46,  ...,   2,   2,   2],\n",
      "        [  0, 186,  33,  ...,   2,   2,   2],\n",
      "        [  0,  14,  71,  ...,   2,   2,   2],\n",
      "        ...,\n",
      "        [  0,   3,   8,  ...,   2,   2,   2],\n",
      "        [  0,  71, 964,  ...,   2,   2,   2],\n",
      "        [  0,  33,  18,  ...,   2,   2,   2]])\n",
      "tgt: tensor([[   0, 2515, 3018,  ...,    2,    2,    2],\n",
      "        [   0,   33,  223,  ...,    2,    2,    2],\n",
      "        [   0,  318,  495,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [   0, 8159,    5,  ...,    2,    2,    2],\n",
      "        [   0, 1432,    3,  ...,    2,    2,    2],\n",
      "        [   0,   11,  216,  ...,    2,    2,    2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence: This incident has shocked the state.\n",
      "tgt_sentence: ఈ సంఘటన రాష్ట్ర ప్రజలను నివ్వెరపరిచింది.\n",
      "src: tensor([[    0,    50,   100,  ...,     2,     2,     2],\n",
      "        [    0,     3, 13303,  ...,     2,     2,     2],\n",
      "        [    0,    12,  1697,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [    0,    71,    13,  ...,     2,     2,     2],\n",
      "        [    0,  1166,  1558,  ...,     2,     2,     2],\n",
      "        [    0,    14,  8882,  ...,     2,     2,     2]])\n",
      "tgt: tensor([[   0,    6,  367,  ...,    2,    2,    2],\n",
      "        [   0,    3,  702,  ...,    2,    2,    2],\n",
      "        [   0, 1122, 1079,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [   0,   16,  249,  ...,    2,    2,    2],\n",
      "        [   0, 2475,   30,  ...,    2,    2,    2],\n",
      "        [   0, 3631,    5,  ...,    2,    2,    2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence: The statue was purchased from Karnataka for Rs 35 lakh.\n",
      "tgt_sentence: కర్ణాటక నుంచి రూ 35 లక్షలు వెచ్చించి కొనుగోలు చేసిన ఈ విగ్రహాన్ని మ్యూజియంలో ప్రదర్శనకు ఉంచుతారు.\n",
      "src: tensor([[    0,    12,  2601,  ...,     2,     2,     2],\n",
      "        [    0,     3,  2719,  ...,     2,     2,     2],\n",
      "        [    0,  2611,  4075,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [    0,   161,     6,  ...,     2,     2,     2],\n",
      "        [    0,     3,     7,  ...,     2,     2,     2],\n",
      "        [    0,     3, 27447,  ...,     2,     2,     2]])\n",
      "tgt: tensor([[    0,   696,    30,  ...,     2,     2,     2],\n",
      "        [    0,  1036,     3,  ...,     2,     2,     2],\n",
      "        [    0,   402, 28324,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [    0,   387,    55,  ...,     2,     2,     2],\n",
      "        [    0,  1320, 13627,  ...,     2,     2,     2],\n",
      "        [    0,  7472,  6126,  ...,     2,     2,     2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence: He can summon objects (He once summoned an over-sized lawnmowers to mow down his foes).\n",
      "tgt_sentence: దీనినే శూన్యస్థితి అంటారు (కారణం ఆప్రాంతంలో వున్న గాలినంతటిని తొలగించడం వలన ఆనిర్ధిష్ట స్థలంలో శూన్యంగా వుంటుంది కనుక).\n",
      "src: tensor([[   0,   32,   65,  ...,    2,    2,    2],\n",
      "        [   0, 2787, 4538,  ...,    2,    2,    2],\n",
      "        [   0,  123,    9,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [   0,   33,   18,  ...,    2,    2,    2],\n",
      "        [   0,  426,  166,  ...,    2,    2,    2],\n",
      "        [   0,  144,  362,  ...,    2,    2,    2]])\n",
      "tgt: tensor([[   0, 9524,    3,  ...,    2,    2,    2],\n",
      "        [   0,  404,    3,  ...,    2,    2,    2],\n",
      "        [   0,  374,    3,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [   0,   61,  815,  ...,    2,    2,    2],\n",
      "        [   0, 5048,    3,  ...,    2,    2,    2],\n",
      "        [   0,   15,    3,  ...,    2,    2,    2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence: Vijay Mallya says he met Arun Jaitley before leaving India\n",
      "tgt_sentence: రుణాలు చెల్లిస్తా, భారత్ వచ్చేముందు జైట్లీని కలిశా!: విజయ్ మాల్యా సంచలనం | Vijay Mallya says he met Arun Jaitley before leaving India, FM denies - Telugu Oneindia\n",
      "src: tensor([[    0,   477,  5540,  ...,     2,     2,     2],\n",
      "        [    0,   179,   145,  ...,     2,     2,     2],\n",
      "        [    0,  9950,     8,  ...,     2,     2,     2],\n",
      "        ...,\n",
      "        [    0, 12944,    10,  ...,     2,     2,     2],\n",
      "        [    0,   301,   120,  ...,     2,     2,     2],\n",
      "        [    0,   316,   364,  ...,     2,     2,     2]])\n",
      "tgt: tensor([[   0, 2471,    3,  ...,    2,    2,    2],\n",
      "        [   0,  733,  729,  ...,    2,    2,    2],\n",
      "        [   0, 9526,    5,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [   0, 2874, 5763,  ...,    2,    2,    2],\n",
      "        [   0,    3,    3,  ...,    2,    2,    2],\n",
      "        [   0,  293,  434,  ...,    2,    2,    2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "src_sentence: Everyone accepted that.\n",
      "tgt_sentence: దానికి అందరూ వత్తాసు పలికారు.\n",
      "src: tensor([[    0,   682,  1986,    21,     4,     1,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    0,    49,  2253,    47,    59,    11, 19352,    25,     5,  7292,\n",
      "           549,     6,   627,    11,   285,    20,     1,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    0,  1395,  1593,   129,     1,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    0,    12,  1918,  1505,   351,    49,    69,    47,   126,     5,\n",
      "          1898,     7,   159,    91,    16,  6735,    45,   752,   318,     4,\n",
      "             1,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    0,  1053,    68,    56,   158,   170,     4,     1,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    0,  3636,     6,  3811,   542,    30,  1334,  6008,    47,   244,\n",
      "          2687,   194,   484,  2447,  7795,    21,    44,   188,     9,   119,\n",
      "            18,  3490,     9,  2166,     5,  7740,     7,   834,   417,    28,\n",
      "           160,   182,  3232,   582,     6,  2897,     8,  1554,   210,     1,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    0,    99,     6,     5,  1060,  6214,    13,   793,  1272,     4,\n",
      "             1,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    0,   106,    18,     5,   182,  2974,    19,     5,   483,     4,\n",
      "             1,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2]])\n",
      "tgt: tensor([[    0,   344,   506,     3,  2647,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    0,    14, 24305, 19472,     4,    13,     1,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    0,     3,  4789,   117,  6037,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    0,  2672,  1529,   351,    49,  1534,   537,     3,  8552,     4,\n",
      "             1,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    0,    26,   485, 27013,   616,     4,     1,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    0,  5715,     9,   426,  2813,    70,   199,  2006,   644,   510,\n",
      "          2099,  2300,  7161,   271,     5,  1705,   103,   106,  5581,   259,\n",
      "           922,     5,  4235,   670,     5,   103,   210,  4279,   697,   752,\n",
      "           922,  3008,  5016,  8856,     1,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    0,    23,     5,    11,   507,  1081,   942,     3,  3510,     4,\n",
      "             1,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    0,   178,   458,  3325,  7075,    43,   799,     4,     1,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2]])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset=wrapped_debug_dataset, batch_size=32, collate_fn=transformer_collate_fn)\n",
    "# Debug dataset has 200 examples. We used the batch_size of 32. So, the number of batches or the length\n",
    "# of the dataloader is ceil(200/32) = 7\n",
    "print(len(dataloader))\n",
    "# See that all the English and Telugu sentences are converted into token ids. Also note that start token id (0)\n",
    "# is added at the beginning of every sequence, end token id (1) is added at the end (before padding tokens)\n",
    "# of every sequence, and pad token id (2) is optionally added to a few sequences depending on the length of \n",
    "# the sequence.\n",
    "for src, tgt in dataloader:\n",
    "    print(f\"src: {src}\")\n",
    "    print(f\"tgt: {tgt}\")\n",
    "    print(\"-\" * 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating length aware pytorch DataLoaders\n",
    "\n",
    "The above created DataLoader has a few issues and is not optimal to be used with Transformers. We will discuss <br>\n",
    "the issues below and show how to handle these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issues with the DataLoader above:\n",
    "#\n",
    "# ISSUE 1: The above DataLoader batches sentences randomly. So, it is possible a sentence of length 1 is batched \n",
    "#          together with a sentence of length 100. This causes the the data pre-processor to add 97 pad tokens \n",
    "#          (excluding <sos>, <eos> ids) to a sentence of length 1 which results in a huge waste of computation \n",
    "#          during training. \n",
    "#\n",
    "# ISSUE 2: In the above DataLoader, every batch contains sentences of length 100 exactly. This is not optimal.\n",
    "#          Ideally, different batches can contain sentences of different lengths but every sentence within a batch \n",
    "#          should be of same length.\n",
    "#\n",
    "# ISSUE 3: This is a minor issue but note that, if any sentence results in more than 100 tokens, the above code \n",
    "#          will fail badly.\n",
    "#\n",
    "# We will handle the above 3 issues in the next parts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader expects the sampler to provide the indices of the data points from the Dataset. These indices are used \n",
    "# to construct the batches internally that are later passed to 'collate_fn'. We are creating a sampler which groups \n",
    "# sentences of similar lengths into batches. We still need to be aware that the grouping is not perfect and it is \n",
    "# done here based on the number of words separated by 'spaces' in the sentence and not the actual number of tokens\n",
    "# we obtain while using the tokenization algorithms. The actual number of tokens could be very different depending on \n",
    "# the tokenization algorithm which splits sentences (and words into tokens) based on a number of other factors.\n",
    "class LengthAwareSampler(Sampler):\n",
    "    def __init__(self, dataset: HuggingFaceDatasetWrapper, batch_size: int):\n",
    "        # dataset is the Dataset wrapper we created on top of HuggingFace dataset.\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.sorted_indices = self.extract_lengths()\n",
    "        print(\"sorted_indices: \", self.sorted_indices)\n",
    "    \n",
    "    # We don't want the entire dataset to be loaded into the memory at once. So, we first iterate over the entire \n",
    "    # dataset, extract the lengths of the sentences and sort the indices of the sentences according to the sentence \n",
    "    # lengths. This is to ensure that sentences of similar lengths are grouped together in a batch to minimize the \n",
    "    # overall padding necessary. When we iterate over the dataset, we only load the necessary data from the dataset \n",
    "    # into memory and not the entire dataset at once --> This loading logic could be a little different based on the \n",
    "    # hugging face implementations. Please look into the hugging face documentation for more details.\n",
    "    def extract_lengths(self) -> list[int]:\n",
    "        \"\"\"Sorts the indices of the dataset based on the lengths of the sentences in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            list[int]: Indices of the dataset sorted in ascending order (small to big) based on the lengths of \n",
    "                       the sentences in the dataset.\n",
    "        \"\"\"\n",
    "        # Note that the lengths are calculated based on the number of words separated by space in the sentence and \n",
    "        # not the number of tokens.\n",
    "        self.lengths = [len(data_point[\"src\"].split(\" \")) + len(data_point[\"tgt\"].split(\" \")) for data_point in self.dataset]\n",
    "        print(\"calculated (src + tgt) sentence lengths: \", self.lengths)\n",
    "        # Create an indices list in the first step i.e., [0, 1, 2, ..., 299] --> For debug_dataset.\n",
    "        # Sorts the indices list based on the calculated in the previous step i.e., after sorting\n",
    "        # value at 0th index is the example for which the len(src_sentence) + len(tgt_sentence) is minimum and\n",
    "        # value at 199th index is the example for which the len(src_sentence) + len(tgt_sentence) is maximum. \n",
    "        return sorted(range(len(self.dataset)), key=lambda index: self.lengths[index])\n",
    "\n",
    "    # The __iter__ function is called once per epoch. The returned iterator is iterated on to get the list of indices \n",
    "    # for the data points in a batch.\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        \"\"\"Provides an iterator that yields the indices of the dataset in the order of the sentence lengths.\n",
    "\n",
    "        Returns:\n",
    "            Generator: A generator object that yields the indices of the dataset in the order of the sentence lengths.\n",
    "\n",
    "        Yields:\n",
    "            Generator[list[int], None, None]: A generator object that yields the indices of the dataset in the order\n",
    "                                              of the sentence lengths.\n",
    "        \"\"\"\n",
    "        # Create the batches of indices based on the sentence lengths. \n",
    "        # batches look like: [[0, 5, 90], [23, 4, 5], ...] if batch_size is 3.\n",
    "        # [0, 5, 90] is a batch corresponding to the sentences at indices 0, 5 and 90 in the original dataset.\n",
    "        # [23, 4, 5] is a batch corresponding to the sentences at indices 23, 4 and 5 in the original dataset.\n",
    "        batches = [self.sorted_indices[index: index + self.batch_size] for index in range(0, len(self.dataset), self.batch_size)]\n",
    "        # Shuffle the batches to ensure that the order of batches is different in every epoch. We want the model to \n",
    "        # see the data in different order in every epoch. So, we shuffle the order of the batches within the dataset.\n",
    "        random.shuffle(batches)\n",
    "        # Flatten the list of batches to get an iterable of indices. At the end, the dataloader expects an iterable of\n",
    "        # indices to get the data points from the dataset. So, we convert the list of batches back to an iterable of \n",
    "        # indices.\n",
    "        print(\"batches: \", batches)\n",
    "        return iter([index for batch in batches for index in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_debug_dataset = HuggingFaceDatasetWrapper(hf_dataset=en_te_debug_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated (src + tgt) sentence lengths:  [12, 12, 4, 20, 13, 13, 34, 15, 4, 8, 4, 24, 14, 9, 10, 15, 18, 5, 40, 21, 7, 27, 15, 13, 21, 7, 7, 6, 6, 15, 48, 21, 7, 45, 23, 8, 30, 25, 27, 23, 5, 40, 7, 27, 6, 11, 14, 42, 14, 18, 25, 30, 12, 11, 11, 25, 33, 13, 8, 23, 13, 6, 31, 15, 11, 18, 31, 13, 16, 19, 15, 9, 6, 9, 25, 11, 49, 15, 21, 7, 6, 18, 8, 6, 22, 12, 9, 22, 9, 12, 7, 18, 11, 22, 8, 23, 23, 41, 5, 5, 7, 15, 8, 7, 14, 21, 80, 19, 13, 4, 4, 17, 15, 43, 27, 22, 9, 12, 10, 25, 22, 32, 10, 5, 6, 10, 17, 6, 29, 10, 7, 14, 4, 7, 22, 52, 23, 5, 68, 7, 12, 13, 8, 21, 30, 34, 9, 8, 61, 43, 29, 20, 9, 40, 31, 8, 10, 10, 6, 7, 35, 10, 5, 14, 48, 15, 19, 22, 31, 16, 6, 28, 45, 19, 36, 6, 9, 9, 9, 8, 15, 5, 43, 17, 15, 14, 32, 11, 25, 6, 6, 42, 7, 15, 7, 24, 8, 63, 14, 14]\n",
      "sorted_indices:  [2, 8, 10, 109, 110, 132, 17, 40, 98, 99, 123, 137, 162, 181, 27, 28, 44, 61, 72, 80, 83, 124, 127, 158, 170, 175, 189, 190, 20, 25, 26, 32, 42, 79, 90, 100, 103, 130, 133, 139, 159, 192, 194, 9, 35, 58, 82, 94, 102, 142, 147, 155, 179, 196, 13, 71, 73, 86, 88, 116, 146, 152, 176, 177, 178, 14, 118, 122, 125, 129, 156, 157, 161, 45, 53, 54, 64, 75, 92, 187, 0, 1, 52, 85, 89, 117, 140, 4, 5, 23, 57, 60, 67, 108, 141, 12, 46, 48, 104, 131, 163, 185, 198, 199, 7, 15, 22, 29, 63, 70, 77, 101, 112, 165, 180, 184, 193, 68, 169, 111, 126, 183, 16, 49, 65, 81, 91, 69, 107, 166, 173, 3, 151, 19, 24, 31, 78, 105, 143, 84, 87, 93, 115, 120, 134, 167, 34, 39, 59, 95, 96, 136, 11, 195, 37, 50, 55, 74, 119, 188, 21, 38, 43, 114, 171, 128, 150, 36, 51, 144, 62, 66, 154, 168, 121, 186, 56, 6, 145, 160, 174, 18, 41, 153, 97, 47, 191, 113, 149, 182, 33, 172, 30, 164, 76, 135, 148, 197, 138, 106]\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy length aware sampler object and play around with it.\n",
    "length_aware_sampler = LengthAwareSampler(dataset=wrapped_debug_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches:  [[87, 93, 115, 120, 134, 167, 34, 39, 59, 95], [96, 136, 11, 195, 37, 50, 55, 74, 119, 188], [57, 60, 67, 108, 141, 12, 46, 48, 104, 131], [77, 101, 112, 165, 180, 184, 193, 68, 169, 111], [163, 185, 198, 199, 7, 15, 22, 29, 63, 70], [21, 38, 43, 114, 171, 128, 150, 36, 51, 144], [173, 3, 151, 19, 24, 31, 78, 105, 143, 84], [123, 137, 162, 181, 27, 28, 44, 61, 72, 80], [0, 1, 52, 85, 89, 117, 140, 4, 5, 23], [174, 18, 41, 153, 97, 47, 191, 113, 149, 182], [83, 124, 127, 158, 170, 175, 189, 190, 20, 25], [126, 183, 16, 49, 65, 81, 91, 69, 107, 166], [2, 8, 10, 109, 110, 132, 17, 40, 98, 99], [26, 32, 42, 79, 90, 100, 103, 130, 133, 139], [159, 192, 194, 9, 35, 58, 82, 94, 102, 142], [147, 155, 179, 196, 13, 71, 73, 86, 88, 116], [146, 152, 176, 177, 178, 14, 118, 122, 125, 129], [62, 66, 154, 168, 121, 186, 56, 6, 145, 160], [156, 157, 161, 45, 53, 54, 64, 75, 92, 187], [33, 172, 30, 164, 76, 135, 148, 197, 138, 106]]\n",
      "87\n",
      "93\n",
      "115\n",
      "120\n",
      "134\n",
      "167\n",
      "34\n",
      "39\n",
      "59\n",
      "95\n",
      "96\n",
      "136\n",
      "11\n",
      "195\n",
      "37\n",
      "50\n",
      "55\n",
      "74\n",
      "119\n",
      "188\n",
      "57\n",
      "60\n",
      "67\n",
      "108\n",
      "141\n",
      "12\n",
      "46\n",
      "48\n",
      "104\n",
      "131\n",
      "77\n",
      "101\n",
      "112\n",
      "165\n",
      "180\n",
      "184\n",
      "193\n",
      "68\n",
      "169\n",
      "111\n",
      "163\n",
      "185\n",
      "198\n",
      "199\n",
      "7\n",
      "15\n",
      "22\n",
      "29\n",
      "63\n",
      "70\n",
      "21\n",
      "38\n",
      "43\n",
      "114\n",
      "171\n",
      "128\n",
      "150\n",
      "36\n",
      "51\n",
      "144\n",
      "173\n",
      "3\n",
      "151\n",
      "19\n",
      "24\n",
      "31\n",
      "78\n",
      "105\n",
      "143\n",
      "84\n",
      "123\n",
      "137\n",
      "162\n",
      "181\n",
      "27\n",
      "28\n",
      "44\n",
      "61\n",
      "72\n",
      "80\n",
      "0\n",
      "1\n",
      "52\n",
      "85\n",
      "89\n",
      "117\n",
      "140\n",
      "4\n",
      "5\n",
      "23\n",
      "174\n",
      "18\n",
      "41\n",
      "153\n",
      "97\n",
      "47\n",
      "191\n",
      "113\n",
      "149\n",
      "182\n",
      "83\n",
      "124\n",
      "127\n",
      "158\n",
      "170\n",
      "175\n",
      "189\n",
      "190\n",
      "20\n",
      "25\n",
      "126\n",
      "183\n",
      "16\n",
      "49\n",
      "65\n",
      "81\n",
      "91\n",
      "69\n",
      "107\n",
      "166\n",
      "2\n",
      "8\n",
      "10\n",
      "109\n",
      "110\n",
      "132\n",
      "17\n",
      "40\n",
      "98\n",
      "99\n",
      "26\n",
      "32\n",
      "42\n",
      "79\n",
      "90\n",
      "100\n",
      "103\n",
      "130\n",
      "133\n",
      "139\n",
      "159\n",
      "192\n",
      "194\n",
      "9\n",
      "35\n",
      "58\n",
      "82\n",
      "94\n",
      "102\n",
      "142\n",
      "147\n",
      "155\n",
      "179\n",
      "196\n",
      "13\n",
      "71\n",
      "73\n",
      "86\n",
      "88\n",
      "116\n",
      "146\n",
      "152\n",
      "176\n",
      "177\n",
      "178\n",
      "14\n",
      "118\n",
      "122\n",
      "125\n",
      "129\n",
      "62\n",
      "66\n",
      "154\n",
      "168\n",
      "121\n",
      "186\n",
      "56\n",
      "6\n",
      "145\n",
      "160\n",
      "156\n",
      "157\n",
      "161\n",
      "45\n",
      "53\n",
      "54\n",
      "64\n",
      "75\n",
      "92\n",
      "187\n",
      "33\n",
      "172\n",
      "30\n",
      "164\n",
      "76\n",
      "135\n",
      "148\n",
      "197\n",
      "138\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "# Iterate on the sampler object to retrieve the indices of the data points (examples).\n",
    "for data_point_index in length_aware_sampler:\n",
    "    print(data_point_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_aware_collate_fn(batch: list, \n",
    "                            english_tokenizer: BaseTokenizer, \n",
    "                            telugu_tokenizer: BaseTokenizer, \n",
    "                            sos_id: int = 0, \n",
    "                            eos_id: int = 1, \n",
    "                            pad_id: int = 2) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Converts the raw data in the batch into the format required by the MachineTranslationTransformer model. It encodes the\n",
    "       sentences into token ids, adds start, end and padding tokens and batches the converted data back to be used by the \n",
    "       model.\n",
    "\n",
    "    Args:\n",
    "        batch (list): Holds the raw data points (the actual english (src) and telugu (tgt) sentences batched) from the dataset.\n",
    "        english_tokenizer (BaseTokenizer): Tokenizer to tokenize and encode the english sentences into corresponding token ids.\n",
    "        telugu_tokenizer (BaseTokenizer): Tokenizer to tokenize and encode the english sentences into corresponding token ids.\n",
    "        sos_id (int, optional): start of sentence token id. Defaults to 0.\n",
    "        eos_id (int, optional): end of sentence token id. Defaults to 1.\n",
    "        pad_id (int, optional): padding token id. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        Tuple [Tensor, Tensor]: Returns the encoded source and target tensors in the batch which can be used by the transformer model\n",
    "                                as input.\n",
    "    \"\"\"\n",
    "    print(\"batch_type: \", type(batch))\n",
    "    # Holds all the encoded src sentences (english sentences) from the batch. encoded sentence means sentence divided \n",
    "    # into tokens and tokens converted into their integer ids.\n",
    "    # [[0, 223, 4345, 545, 1], [0, 23, 234, 67, 1]] is an example for the processed_src_sentences variable where\n",
    "    # [0, 223, 4345, 545, 1] represents an encoded sentence from the batch and 0 at the start is <sos> and 1 at the \n",
    "    # end is <eos>. \n",
    "    processed_src_sentences = []\n",
    "    # Holds all the encoded tgt sentences (telugu sentences) from the batch.\n",
    "    processed_tgt_sentences = []\n",
    "    index = 0\n",
    "    for data_point in batch:\n",
    "        # src is english sentence.\n",
    "        src_sentence = data_point[\"src\"]\n",
    "        # tgt is telugu sentence.\n",
    "        tgt_sentence = data_point[\"tgt\"]\n",
    "        # start of sentence id to append at the start of every sentence.\n",
    "        sos_tensor = torch.tensor([sos_id], dtype=torch.int64)\n",
    "        # end of sentence id to append at the end of every sentence.\n",
    "        eos_tensor = torch.tensor([eos_id], dtype=torch.int64)\n",
    "        if index < 10:\n",
    "            # This conditional block is to print during experiments and can be removed from the actual code later.\n",
    "            print(f\"src_sentence: {src_sentence}\")\n",
    "            print(f\"tgt_sentence: {tgt_sentence}\")\n",
    "            index += 1\n",
    "        # It is important to set the dtype to 'torch.int64' because we map token_ids (integers) to their embeddings in the \n",
    "        # transformer model.'<sos>' and '<eos>' tokens are not added to the src sentences. They are only added to the target \n",
    "        # sentences.\n",
    "        encoded_src_sentence = torch.tensor(english_tokenizer.encode(src_sentence), dtype=torch.int64)\n",
    "        # prepares the tensor in the format 'token_id(<sos>) token_id1 token_id2 ... last_token_id token_id(<eos>)'. \n",
    "        encoded_tgt_sentence = torch.cat([sos_tensor, torch.tensor(telugu_tokenizer.encode(tgt_sentence), dtype=torch.int64), eos_tensor], dim=0)\n",
    "        processed_src_sentences.append(encoded_src_sentence)\n",
    "        processed_tgt_sentences.append(encoded_tgt_sentence)\n",
    "    # Finds the maximum length of the src_sequences in the batch so that src sequences are padded to get all the sequences\n",
    "    # to the same length i.e., max_src_seq_len.\n",
    "    max_src_seq_len = max(src_ids.size(0) for src_ids in processed_src_sentences)\n",
    "    # Finds the maximum length of the tgt_sequences in the batch so that src sequences are padded to get all the sequences\n",
    "    # to the same length i.e., max_tgt_seq_len.\n",
    "    max_tgt_seq_len = max(tgt_ids.size(0) for tgt_ids in processed_tgt_sentences)\n",
    "    # We pad the sentences with pad token so that every sentence in the batch is of same length. Also, notice \n",
    "    # that the pad token is appended after (not before) the <eos> token is appended to every sentence.\n",
    "    src_ids = [torch.nn.functional.pad(input=src_ids, pad=(0, max_src_seq_len - src_ids.size(0)), mode=\"constant\", value=pad_id) for src_ids in processed_src_sentences]\n",
    "    tgt_ids = [torch.nn.functional.pad(input=tgt_ids, pad=(0, max_tgt_seq_len - tgt_ids.size(0)), mode=\"constant\", value=pad_id) for tgt_ids in processed_tgt_sentences]\n",
    "    # stack the src tensors along dimension 0. This then becomes a 2D tensor of shape (BATCH_SIZE, MAX_SENTENCE_LENGTH).\n",
    "    src = torch.stack(tensors=src_ids, dim=0)\n",
    "    tgt = torch.stack(tensors=tgt_ids, dim=0)\n",
    "    return (src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"The collate_fn is called by the DataLoader with just the batch of data points from the dataset. So, we wrap the \n",
    "       length_aware_collate_fn function with the required parameters to create a collate_fn that can be used by the \n",
    "       DataLoader.\n",
    "\n",
    "    Args:\n",
    "        batch (_type_): Batch of raw data points from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        _type_: Returns the batch of data points in the format required by the MachineTranslationTransformer model.\n",
    "    \"\"\"\n",
    "    sos_id = spacy_english_tokenizer.get_token_id(token=\"<sos>\")\n",
    "    eos_id = spacy_english_tokenizer.get_token_id(token=\"<eos>\")\n",
    "    pad_id = spacy_telugu_tokenizer.get_token_id(token=\"<pad>\")\n",
    "    return length_aware_collate_fn(batch=batch, english_tokenizer=spacy_english_tokenizer, telugu_tokenizer=spacy_telugu_tokenizer, sos_id=sos_id, eos_id=eos_id, pad_id=pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated (src + tgt) sentence lengths:  [12, 12, 4, 20, 13, 13, 34, 15, 4, 8, 4, 24, 14, 9, 10, 15, 18, 5, 40, 21, 7, 27, 15, 13, 21, 7, 7, 6, 6, 15, 48, 21, 7, 45, 23, 8, 30, 25, 27, 23, 5, 40, 7, 27, 6, 11, 14, 42, 14, 18, 25, 30, 12, 11, 11, 25, 33, 13, 8, 23, 13, 6, 31, 15, 11, 18, 31, 13, 16, 19, 15, 9, 6, 9, 25, 11, 49, 15, 21, 7, 6, 18, 8, 6, 22, 12, 9, 22, 9, 12, 7, 18, 11, 22, 8, 23, 23, 41, 5, 5, 7, 15, 8, 7, 14, 21, 80, 19, 13, 4, 4, 17, 15, 43, 27, 22, 9, 12, 10, 25, 22, 32, 10, 5, 6, 10, 17, 6, 29, 10, 7, 14, 4, 7, 22, 52, 23, 5, 68, 7, 12, 13, 8, 21, 30, 34, 9, 8, 61, 43, 29, 20, 9, 40, 31, 8, 10, 10, 6, 7, 35, 10, 5, 14, 48, 15, 19, 22, 31, 16, 6, 28, 45, 19, 36, 6, 9, 9, 9, 8, 15, 5, 43, 17, 15, 14, 32, 11, 25, 6, 6, 42, 7, 15, 7, 24, 8, 63, 14, 14]\n",
      "sorted_indices:  [2, 8, 10, 109, 110, 132, 17, 40, 98, 99, 123, 137, 162, 181, 27, 28, 44, 61, 72, 80, 83, 124, 127, 158, 170, 175, 189, 190, 20, 25, 26, 32, 42, 79, 90, 100, 103, 130, 133, 139, 159, 192, 194, 9, 35, 58, 82, 94, 102, 142, 147, 155, 179, 196, 13, 71, 73, 86, 88, 116, 146, 152, 176, 177, 178, 14, 118, 122, 125, 129, 156, 157, 161, 45, 53, 54, 64, 75, 92, 187, 0, 1, 52, 85, 89, 117, 140, 4, 5, 23, 57, 60, 67, 108, 141, 12, 46, 48, 104, 131, 163, 185, 198, 199, 7, 15, 22, 29, 63, 70, 77, 101, 112, 165, 180, 184, 193, 68, 169, 111, 126, 183, 16, 49, 65, 81, 91, 69, 107, 166, 173, 3, 151, 19, 24, 31, 78, 105, 143, 84, 87, 93, 115, 120, 134, 167, 34, 39, 59, 95, 96, 136, 11, 195, 37, 50, 55, 74, 119, 188, 21, 38, 43, 114, 171, 128, 150, 36, 51, 144, 62, 66, 154, 168, 121, 186, 56, 6, 145, 160, 174, 18, 41, 153, 97, 47, 191, 113, 149, 182, 33, 172, 30, 164, 76, 135, 148, 197, 138, 106]\n"
     ]
    }
   ],
   "source": [
    "length_aware_sampler = LengthAwareSampler(dataset=wrapped_debug_dataset, batch_size=10)\n",
    "dataloader = DataLoader(dataset=wrapped_debug_dataset, batch_size=10, sampler=length_aware_sampler, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fd54910be20> <class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "print(dataloader, type(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x7fd536bbc640> <class 'torch.utils.data.dataloader._SingleProcessDataLoaderIter'>\n"
     ]
    }
   ],
   "source": [
    "data_iterator = iter(dataloader)\n",
    "print(data_iterator, type(data_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_type:  <class 'list'>\n",
      "src_sentence: Its called illeism.\n",
      "tgt_sentence: ఇది దిగజారుడుతనానికి గా పిలవబడింది.\n",
      "src_sentence: Everyone accepted that.\n",
      "tgt_sentence: దానికి అందరూ వత్తాసు పలికారు.\n",
      "src_sentence: Heavy rainfall...\n",
      "tgt_sentence: ఏకధాటిగా కురుస్తున్న భారీ వర్షాలకు .\n",
      "src_sentence: 5 lakh would be provided.\n",
      "tgt_sentence: 5లక్షలు సాయం అందజేశారు.\n",
      "src_sentence: The children were terrified.\n",
      "tgt_sentence: పిల్లలు తీవ్ర భయాందోళనకు గురయ్యారు.\n",
      "src_sentence: Doctors, too, are delighted.\n",
      "tgt_sentence: వైద్యులు సంతోషం వ్యక్తం చేస్తున్నారు.\n",
      "src_sentence: But they will disappear soon.\n",
      "tgt_sentence: కానీ వెంటనే అదృశ్యమవుతుంది.\n",
      "src_sentence: From culture to commerce.\n",
      "tgt_sentence: సంస్కృతి నుంచి వాణిజ్యం వ‌ర‌కు\n",
      "src_sentence: It means elder sister.\n",
      "tgt_sentence: అంటే పెద్దన్నయ్య అని అర్థం.\n",
      "src_sentence: Peoples lifestyle started changing.\n",
      "tgt_sentence: మనుషుల లైఫ్ స్టైల్ మారింది.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "shapes:  torch.Size([10, 7]) torch.Size([10, 7])\n",
      "batched_data:  (tensor([[  144,   362,     3,     4,     2,     2,     2],\n",
      "        [  682,  1986,    21,     4,     2,     2,     2],\n",
      "        [ 1395,  1593,   129,     2,     2,     2,     2],\n",
      "        [  253,   120,   118,    23,   776,     4,     2],\n",
      "        [   12,   207,    38, 12093,     4,     2,     2],\n",
      "        [ 1388,     6,   166,     6,    17,  4903,     4],\n",
      "        [   52,    70,    27, 14046,   196,     4,     2],\n",
      "        [ 1166,  1558,     9,  4679,     4,     2,     2],\n",
      "        [   42,   756,  4032,  1626,     4,     2,     2],\n",
      "        [ 5351,  5150,   343,  1930,     4,     2,     2]]), tensor([[    0,    15,     3,    78,     3,     4,     1],\n",
      "        [    0,   344,   506,     3,  2647,     4,     1],\n",
      "        [    0,     3,  4789,   117,  6037,     4,     1],\n",
      "        [    0, 10507,  1031,  3192,     4,     1,     2],\n",
      "        [    0,   372,   213,  9267,  2338,     4,     1],\n",
      "        [    0,   691,  2994,   238,   129,     4,     1],\n",
      "        [    0,    25,   140,     3,     4,     1,     2],\n",
      "        [    0,  2475,    30,  5386,  5275,     1,     2],\n",
      "        [    0,   128,     3,    24,   390,     4,     1],\n",
      "        [    0,  7231,  2401,  7741,   488,     4,     1]]))\n"
     ]
    }
   ],
   "source": [
    "batched_data = next(data_iterator)\n",
    "print(\"-\" * 150)\n",
    "print(\"shapes: \", batched_data[0].shape, batched_data[1].shape)\n",
    "# Notice that <sos> and <eos> token ids are only added to the target sentences and not to the source sentences.\n",
    "# This is inline with what will be fed to the transformer model during training.\n",
    "print(\"batched_data: \", batched_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 1697, 18, 345, 520, 10, 4726, 4] \n",
      "\n",
      "[6, 1628, 456, 149, 15006, 1997, 432, 4]\n"
     ]
    }
   ],
   "source": [
    "# Lets verify the correctness of dataloader.\n",
    "# Notice that the encoded tokens printed in the above cell for the first sentence and the encoded\n",
    "# tokens printed here are the same for both English sentence and Telugu sentence.\n",
    "english_sentence = \"The smartphone was recently launched in Indonesia.\"\n",
    "print(spacy_english_tokenizer.encode(text=english_sentence), \"\\n\")\n",
    "telugu_sentence = \"ఈ స్మార్ట్ ఫోన్ ఇప్పటికే ఇండోనేషియాలో లాంచ్ అయింది.\"\n",
    "print(spacy_telugu_tokenizer.encode(text=telugu_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
