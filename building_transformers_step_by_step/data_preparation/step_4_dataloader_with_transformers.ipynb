{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, you learn:\n",
    "# \n",
    "# 1) How to create dataset iterators for English-Telugu translation data to be used by Transformers?\n",
    "# 3) How to create DataLoaders that group the data based on the length of the sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources to go through to understand this notebook:\n",
    "#\n",
    "# 1) https://www.youtube.com/watch?v=IGu7ivuy1Ag\n",
    "#       -- Gives a high level walk through of how the input and the output are structured and transformed in a transformer model.\n",
    "#       -- This is important to understand so that we how how to batch our input data.\n",
    "# 2) https://www.youtube.com/watch?v=dZzVA6VbAR8\n",
    "#       -- Decent video that gives an overview about the Data Preparation process in transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from tokenizers import ByteLevelBPETokenizer  # type: ignore\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Callable, List, Iterator, Optional, Tuple\n",
    "\n",
    "import datasets\n",
    "import random\n",
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path (directory) where all the data used by this project is stored.\n",
    "AI4_BHARAT_DATA_PATH = \"../../Data/AI4Bharat\"\n",
    "# Number of tokens in the vocabulary of the tokenizer.\n",
    "VOCAB_SIZE = 50000\n",
    "START_TOKEN = \"<sos>\"\n",
    "END_TOKEN = \"<eos>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "{'idx': 0, 'src': 'Have you heard about Foie gras?', 'tgt': 'ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?'}\n",
      "250000\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "{'idx': 0, 'src': 'Have you heard about Foie gras?', 'tgt': 'ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?'}\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# Load the train dataset we have created and saved to disk in 'step_1_data_exploration.ipynb' notebook. \n",
    "en_te_translation_dataset = datasets.load_from_disk(f\"{AI4_BHARAT_DATA_PATH}/train_dataset\")\n",
    "print(type(en_te_translation_dataset))\n",
    "print(en_te_translation_dataset[0])\n",
    "print(len(en_te_translation_dataset))\n",
    "print(\"-\" * 150)\n",
    "en_te_debug_dataset = datasets.load_from_disk(f\"{AI4_BHARAT_DATA_PATH}/debug_dataset\")\n",
    "print(en_te_debug_dataset[0])\n",
    "print(len(en_te_debug_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_extractor(data_point: dict[str, str], language: str) -> str:\n",
    "    \"\"\"Extracts the appropriate text from the example in the dataset based on the language.\n",
    "\n",
    "    Args:\n",
    "        data_point (dict[str, str]): A single example from the dataset containing the text in the form of \n",
    "                                     a dictionary. The sources sentence is stored in the key 'src' and the \n",
    "                                     target sentence is stored in the key 'tgt'.\n",
    "        language (str): Language of the text to be extracted from the data_point.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Raises an error if the language is not 'english' or 'telugu'.\n",
    "\n",
    "    Returns:\n",
    "        str: The text in the data_point.\n",
    "    \"\"\"\n",
    "    if language == \"english\":\n",
    "        return data_point[\"src\"]\n",
    "    elif language == \"telugu\":\n",
    "        return data_point[\"tgt\"]\n",
    "    raise ValueError(\"Language should be either 'english' or 'telugu'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a base class that will be inherited by the actual tokenizer classes.\n",
    "class BaseTokenizer(ABC):\n",
    "    \"\"\"A class created to hold different kinds of tokenizers and handle the token encoding in a common way.\n",
    "       Here, we only use SpacyTokenizer and HuggingFaceTokenizer.\"\"\"\n",
    "    def __init__(self, language: str, tokenizer_type: str):\n",
    "        self.language = language\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "        self.special_tokens = [START_TOKEN, END_TOKEN, PAD_TOKEN, UNK_TOKEN]\n",
    "\n",
    "    # Abstract methods need to be overridden by the child class. It raises TypeError if not overridden.\n",
    "    @abstractmethod\n",
    "    def initialize_tokenizer_and_build_vocab(self, \n",
    "                                             data_iterator: datasets.arrow_dataset.Dataset, \n",
    "                                             text_extractor: Callable[[dict[str, str], str], str], \n",
    "                                             max_vocab_size: Optional[int] = VOCAB_SIZE):\n",
    "        \"\"\"Initializes the tokenizers and builds the vocabulary for the given dataset.\n",
    "\n",
    "        Args:\n",
    "            data_iterator (datasets.arrow_dataset.Dataset): An iterator that gives input sentences (text) when iterated upon.\n",
    "            text_extractor (Callable[[dict[str, str], str], str]): A function that extracts the appropriate text from the input \n",
    "                dataset. This parameter is added to make the tokenizer independent of the input dataset format. If not provided \n",
    "                as an argument, we will have to extract the text from the dataset within the 'CustomTokenizer' class which makes \n",
    "                it dependent on the dataset format. \n",
    "            max_vocab_size (int, optional): Maximum size of the vocabulary to create from the input data corpus. Defaults to 40000.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    # Abstract methods need to be overridden by the child class. It raises TypeError if not overridden.\n",
    "    @abstractmethod\n",
    "    def tokenize(self, text: str) -> list[str]:\n",
    "        \"\"\"Returns the individual tokens (language text) for the given text.\"\"\"\n",
    "        pass\n",
    "\n",
    "    # Abstract methods need to be overridden by the child class. It raises TypeError if not overridden.\n",
    "    @abstractmethod\n",
    "    def encode(self, text: str) -> list[str]:\n",
    "        \"\"\"Returns the encoded token ids for the given text.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    # Abstract methods need to be overridden by the child class. It raises TypeError if not overridden.\n",
    "    @abstractmethod\n",
    "    def get_token_id(self, token: str) -> int:\n",
    "        \"\"\"Returns the token id for the given token.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer 'step_2_training_bpe_tokenizer.ipynb' and 'step_2_alternate_tokenization_with_spacy.ipynb' notebooks to understand this class better.\n",
    "class SpacyTokenizer(BaseTokenizer):\n",
    "    \"\"\"Creates a tokenizer that tokenizes the text using the Spacy tokenizer models.\"\"\"\n",
    "    def __init__(self, language: str):\n",
    "        super().__init__(language, \"spacy\")\n",
    "    \n",
    "    def initialize_tokenizer_and_build_vocab(self, \n",
    "                                             data_iterator: datasets.arrow_dataset.Dataset, \n",
    "                                             text_extractor: Callable[[dict[str, str], str], str], \n",
    "                                             max_vocab_size: int = 40000):\n",
    "        # Load spacy models for English text tokenization.\n",
    "        if self.language == \"english\":\n",
    "            self.tokenizer = spacy.load(\"en_core_web_sm\").tokenizer          \n",
    "        elif self.language == \"telugu\":\n",
    "            # Load spacy model for Telugu text tokenization.\n",
    "            self.tokenizer = spacy.blank(\"te\").tokenizer            \n",
    "        else:\n",
    "            # Raise an error for unknown language\n",
    "            pass\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.__build_vocab(data_iterator=data_iterator, text_extractor=text_extractor)\n",
    "\n",
    "    def tokenize(self, text: str) -> list[str]:\n",
    "        return [token.text for token in self.tokenizer(text)]\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        return self.vocab(self.tokenize(text))\n",
    "    \n",
    "    def get_token_id(self, token: str) -> int:\n",
    "        return self.vocab([token])[0]\n",
    "\n",
    "    def __yield_tokens(self, data_iterator: datasets.arrow_dataset.Dataset, text_extractor: Callable[[dict[str, str], str], str]):\n",
    "        \"\"\"Returns a generator object that emits tokens for each sentence in the dataset\"\"\"\n",
    "        for data_point in data_iterator:\n",
    "            yield self.tokenize(text_extractor(data_point, self.language))\n",
    "\n",
    "    def __build_vocab(self, data_iterator: datasets.arrow_dataset.Dataset, text_extractor: Callable[[dict[str, str], str], str]):\n",
    "        \"\"\"Builds the vocabulary for the given dataset\"\"\"\n",
    "        self.vocab = build_vocab_from_iterator(iterator=self.__yield_tokens(data_iterator=data_iterator, text_extractor=text_extractor), \n",
    "                                               min_freq=2, \n",
    "                                               specials=self.special_tokens, \n",
    "                                               special_first=True, \n",
    "                                               max_tokens=self.max_vocab_size)\n",
    "        self.vocab.set_default_index(self.vocab[UNK_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_telugu_tokenizer: BaseTokenizer = SpacyTokenizer(language=\"telugu\")\n",
    "spacy_telugu_tokenizer.initialize_tokenizer_and_build_vocab(data_iterator=en_te_translation_dataset, text_extractor=text_extractor, max_vocab_size=VOCAB_SIZE)\n",
    "spacy_english_tokenizer: BaseTokenizer = SpacyTokenizer(language=\"english\")\n",
    "spacy_english_tokenizer.initialize_tokenizer_and_build_vocab(data_iterator=en_te_translation_dataset, text_extractor=text_extractor, max_vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ఇక', 'ఫ్రూట్', 'ఫ్లైస్', 'గురించి', 'మీరు', 'విన్నారా', '?']\n",
      "[94, 23082, 3, 72, 40, 18178, 7]\n",
      "['Have', 'you', 'heard', 'about', 'Foie', 'gras', '?']\n",
      "[1017, 46, 1003, 76, 3, 3, 20]\n",
      "token(<sos>) = 0 , token(<eos>) = 1\n",
      "token(<unk>) = 3 , token(<pad>) = 2\n",
      "token(NON_EXISTENT_TOKEN) = 3\n"
     ]
    }
   ],
   "source": [
    "# Note that we cannot add special tokens to the sentence and ask the spacy_tokenizer to tokenize. If we do that, it \n",
    "# will break the special tokens into simpler tokens again. This is because the spacy tokenizer was not trained \n",
    "# specifically to keep the special tokens intact during tokenization. It doesn't know about special tokens by \n",
    "# default. spacy uses a default tokenizer which is already trained by some rules. We haven't added any rules about \n",
    "# tokenization ourselves. However, since we added the special tokens to the vocabulary, spacy tokenizer maps them to \n",
    "# the corresponding token ids and returns them when prompted for ids.\n",
    "telugu_sentence = \"ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?\"\n",
    "print(spacy_telugu_tokenizer.tokenize(text=telugu_sentence))\n",
    "print(spacy_telugu_tokenizer.encode(text=telugu_sentence))\n",
    "english_sentence = \"Have you heard about Foie gras?\"\n",
    "print(spacy_english_tokenizer.tokenize(text=english_sentence))\n",
    "print(spacy_english_tokenizer.encode(text=english_sentence))\n",
    "print(\"token(<sos>) =\", spacy_english_tokenizer.get_token_id(START_TOKEN), \", token(<eos>) =\", spacy_english_tokenizer.get_token_id(END_TOKEN))\n",
    "print(\"token(<unk>) =\", spacy_english_tokenizer.get_token_id(UNK_TOKEN), \", token(<pad>) =\", spacy_english_tokenizer.get_token_id(PAD_TOKEN))\n",
    "print(\"token(NON_EXISTENT_TOKEN) =\", spacy_english_tokenizer.get_token_id(\"ఫ్రూట్\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer 'step_2_training_bpe_tokenizer_and_vocab.ipynb' notebook to understand this class better.\n",
    "# We train our own tokenizer in HuggingFace since it doesn't provide a tokenizer for Telugu by default.\n",
    "class BPETokenizer(BaseTokenizer):\n",
    "    \"\"\"Trains a tokenizer using HuggingFace libraries\"\"\"\n",
    "    def __init__(self, language: str):\n",
    "        super().__init__(language, \"hugging_face\")\n",
    "\n",
    "    def initialize_tokenizer_and_build_vocab(self, \n",
    "                                             data_iterator: datasets.arrow_dataset.Dataset, \n",
    "                                             text_extractor: Callable[[dict[str, str], str], str], \n",
    "                                             max_vocab_size: Optional[int] = VOCAB_SIZE):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.tokenizer = self.__train_tokenizer(data_iterator=data_iterator, text_extractor=text_extractor, max_vocab_size=max_vocab_size)\n",
    "\n",
    "    def tokenize(self, text: str) -> list[str]:\n",
    "        encoded_text = self.tokenizer.encode(text)\n",
    "        return encoded_text.tokens\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        encoded_text = self.tokenizer.encode(text)\n",
    "        return encoded_text.ids\n",
    "\n",
    "    # We do not have a decode method for spacy tokenizer since it doesn't have a built-in mechanism to generate \n",
    "    # text from token ids. It tokenizes the text based on a bunch of rules and doesn't know how to join the \n",
    "    # tokens back to form the original text. This is one of the additional advantages of using a BPE tokenizer\n",
    "    # over a spacy tokenizer because BPE algorithm is designed to both encode and decode text.\n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"Converts the series of token ids back to the original text.\n",
    "\n",
    "        Args:\n",
    "            token_ids (List[int]): A list of token ids corresponding to some text.\n",
    "\n",
    "        Returns:\n",
    "            str: The original text corresponding to the token ids.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.decode(token_ids)\n",
    "\n",
    "    def get_token_id(self, token: str) -> int:\n",
    "        return self.tokenizer.token_to_id(token)\n",
    "\n",
    "    # We need an iterator to train the tokenizer. Using an iterator ensures that not all \n",
    "    # the data is loaded into memory at once.\n",
    "    def __get_data_iterator(self, data_iterator: datasets.arrow_dataset.Dataset, text_extractor: Callable[[dict[str, str], str], str]):\n",
    "        for data_point in data_iterator:\n",
    "            yield text_extractor(data_point=data_point, language=self.language)\n",
    "\n",
    "    def __train_tokenizer(self, data_iterator: datasets.arrow_dataset.Dataset, \n",
    "                          text_extractor: Callable[[dict[str, str], str], str], \n",
    "                          max_vocab_size: Optional[int]=VOCAB_SIZE) -> ByteLevelBPETokenizer:\n",
    "        # Use BPE to train a ByteLevel BPE tokenizer.\n",
    "        tokenizer = ByteLevelBPETokenizer()\n",
    "        # train_from_iterator is used so that the entire dataset is not loaded into memory at once.\n",
    "        tokenizer.train_from_iterator(iterator=self.__get_data_iterator(data_iterator=data_iterator, text_extractor=text_extractor), \n",
    "                                      vocab_size= max_vocab_size, \n",
    "                                      special_tokens=self.special_tokens)\n",
    "        return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bpe_telugu_tokenizer: BaseTokenizer = BPETokenizer(language=\"telugu\")\n",
    "bpe_telugu_tokenizer.initialize_tokenizer_and_build_vocab(data_iterator=en_te_translation_dataset, text_extractor=text_extractor, max_vocab_size=30000)\n",
    "bpe_english_tokenizer: BaseTokenizer = BPETokenizer(language=\"english\")\n",
    "bpe_english_tokenizer.initialize_tokenizer_and_build_vocab(data_iterator=en_te_translation_dataset, text_extractor=text_extractor, max_vocab_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['à°ĩà°ķ', 'Ġà°«', 'à±į', 'à°°', 'à±Ĥ', 'à°Ł', 'à±į', 'Ġà°«', 'à±į', 'à°²', 'à±Ī', 'à°¸', 'à±į', 'Ġà°Ĺ', 'à±ģ', 'à°°', 'à°¿à°Ĥ', 'à°ļ', 'à°¿', 'Ġà°®', 'à±Ģ', 'à°°', 'à±ģ', 'Ġà°µ', 'à°¿', 'à°¨', 'à±į', 'à°¨', 'à°¾', 'à°°', 'à°¾?']\n",
      "[539, 360, 263, 267, 298, 277, 263, 360, 263, 270, 305, 278, 263, 322, 266, 267, 294, 286, 264, 293, 283, 267, 266, 291, 264, 268, 263, 268, 265, 267, 440]\n",
      "ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "['Have', 'Ġyou', 'Ġheard', 'Ġabout', 'ĠF', 'o', 'ie', 'Ġgras', '?']\n",
      "[3519, 452, 3123, 629, 510, 82, 485, 26172, 34]\n",
      "Have you heard about Foie gras?\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "token(<sos>) = 0 , token(<eos>) = 1\n",
      "token(<unk>) = 3 , token(<pad>) = 2\n",
      "token(NON_EXISTENT_TOKEN) = None\n"
     ]
    }
   ],
   "source": [
    "# This tokenizer conveniently handles special tokens by default. It can keep the special\n",
    "# tokens intact during tokenization. This is because the tokenizer was trained specifically\n",
    "# to keep the special tokens intact during tokenization. It knows about special tokens by \n",
    "# default.\n",
    "telugu_sentence = \"ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?\"\n",
    "print(bpe_telugu_tokenizer.tokenize(text=telugu_sentence))\n",
    "print(bpe_telugu_tokenizer.encode(text=telugu_sentence))\n",
    "print(bpe_telugu_tokenizer.decode(token_ids=bpe_telugu_tokenizer.encode(text=telugu_sentence)))\n",
    "print(\"-\" * 150)\n",
    "english_sentence = \"Have you heard about Foie gras?\"\n",
    "print(bpe_english_tokenizer.tokenize(text=english_sentence))\n",
    "print(bpe_english_tokenizer.encode(text=english_sentence))\n",
    "print(bpe_english_tokenizer.decode(token_ids=bpe_english_tokenizer.encode(text=english_sentence)))\n",
    "print(\"-\" * 150)\n",
    "print(\"token(<sos>) =\", bpe_english_tokenizer.get_token_id(START_TOKEN), \", token(<eos>) =\", bpe_english_tokenizer.get_token_id(END_TOKEN))\n",
    "print(\"token(<unk>) =\", bpe_english_tokenizer.get_token_id(UNK_TOKEN), \", token(<pad>) =\", bpe_english_tokenizer.get_token_id(PAD_TOKEN))\n",
    "# Notice that BPE doesn't return the token id 3 (<UNK>) for the string \"NON_EXISTENT_TOKEN\" as it did for the spacy tokenizer. \n",
    "# This is because NON_EXISTENT_TOKEN can be broken down into smaller tokens by the BPE tokenizer. BPE never returns an unk\n",
    "# token id. It always breaks down the unknown token into smaller tokens. It is not even useful to have an unk token id in BPE.\n",
    "print(\"token(NON_EXISTENT_TOKEN) =\", bpe_english_tokenizer.get_token_id(\"NON_EXISTENT_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating pytorch Dataset from HuggingFace dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to wrap our HuggingFace dataset into the torch Dataset to integrate with pytorch DataLoader.\n",
    "class HuggingFaceDatasetWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset: datasets.arrow_dataset.Dataset):\n",
    "        \"\"\"Initializes the HuggingFaceDatasetWrapper with the given dataset.\n",
    "\n",
    "        Args:\n",
    "            hf_dataset (datasets.arrow_dataset.Dataset): The hugging face dataset to be wrapped.\n",
    "        \"\"\"\n",
    "        self.dataset = hf_dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Extracts the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Length of the dataset.\n",
    "        \"\"\"\n",
    "        return self.dataset.num_rows\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Extracts the data_point at a particular index in the dataset.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the data_point to be extracted from the dataset.\n",
    "\n",
    "        Returns:\n",
    "            dict: Data_point at the given index in the dataset. This turns out to be a dictionary for our dataset but\n",
    "                  it could be any type in general.\n",
    "        \"\"\"\n",
    "        # Return the dataset at a particular index.\n",
    "        # The index provided will always be less then length (64 in this case) returned by __len__ function.\n",
    "        return self.dataset[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_translation_dataset = HuggingFaceDatasetWrapper(hf_dataset=en_te_translation_dataset)\n",
    "wrapped_debug_dataset = HuggingFaceDatasetWrapper(hf_dataset=en_te_debug_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 0, 'src': 'Have you heard about Foie gras?', 'tgt': 'ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?'}\n",
      "{'idx': 1, 'src': 'I never thought of acting in films.', 'tgt': 'సూర్య సినిమాల్లో నటించాలని ఎప్పుడూ అనుకోలేదు.'}\n",
      "{'idx': 2, 'src': 'Installed Software', 'tgt': 'స్థాపించబడిన సాఫ్ట్\\u200dవేర్'}\n",
      "{'idx': 3, 'src': 'A case has been registered under Sections 302 and 376, IPC.', 'tgt': 'నిందితులపై సెక్షన్ 376 మరియు 302ల కింద కేసు నమోదు చేశాం.'}\n",
      "{'idx': 4, 'src': 'Of this, 10 people succumbed to the injuries.', 'tgt': 'అందులో 10 మంది తీవ్రంగా గాయపడ్డారు.'}\n",
      "{'idx': 5, 'src': 'Her acting has been praised by critics.', 'tgt': 'నటనకు గాను విమర్శకుల నుంచి ప్రశంసలు పొందింది.'}\n",
      "{'idx': 6, 'src': 'The Bibles viewpoint on this is clearly indicated at Colossians 3: 9: Do not be lying to one another.', 'tgt': 'ఈ విషయంపై బైబిలు దృక్కోణం కొలొస్సయులు 3 :\\u2060 9లో “ఒకనితో ఒకడు అబద్ధమాడకుడి ” అని స్పష్టంగా సూచించబడింది.'}\n",
      "{'idx': 7, 'src': 'The incident was recorded in the CCTV footage.', 'tgt': 'ఈ ప్రమాద దృశ్యాలు సీసీటీవీ ఫుటేజ్\\u200cలో రికార్డ్ అయ్యాయి.'}\n",
      "{'idx': 8, 'src': 'Respect privacy', 'tgt': 'గోప్యత పాటించండి'}\n",
      "{'idx': 9, 'src': '5 lakh would be provided.', 'tgt': '5లక్షలు సాయం అందజేశారు.'}\n",
      "{'idx': 10, 'src': 'Super Bowl.', 'tgt': '\"\"\"సూపర్ బౌల్.\"'}\n"
     ]
    }
   ],
   "source": [
    "# A sample iteration on the Pytorch dataset to show the output.\n",
    "for idx, data_point in enumerate(wrapped_debug_dataset):\n",
    "    if idx > 10:\n",
    "        break\n",
    "    print(data_point)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# 'len' function returns the number of examples in the dataset. 'len' internally calls the '__len__'\n",
    "# method which is immplemented above to return the 'num_rows' which is the number of examples in the\n",
    "# dataset.\n",
    "print(len(wrapped_debug_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating basic pytorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The role of collate_fn is to iterate on each batch of the dataset and convert the data in the batch into the format \n",
    "# that is required by the model (transformers in this case). A more extensively commented collate_fn is implemented \n",
    "# below in the same notebook. You will understand better there if something is not clear in this function.\n",
    "def transformer_collate_fn(batch):\n",
    "    \"\"\"Converts the raw data in the batch into the format required by the model. In this case, it\n",
    "       adds the text to token ids, adds start, end and padding tokens and batches the converted \n",
    "       data to be used by the model. \n",
    "\n",
    "    Args:\n",
    "        batch (_type_): A batch of raw data points from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the source and target tensors in the batch.\n",
    "    \"\"\"\n",
    "    # Maximum length of the sentence.\n",
    "    MAX_LEN = 100\n",
    "    # Holds the list of processed source sentences (English sentences) from the batch. \n",
    "    src_list = []\n",
    "    # Holds the list of processed target sentences (Telugu sentences) from the batch. The data at corresponding indices in\n",
    "    # src_list and tgt_list are a pair i.e., tgt sentence at index 'j' is translation of src sentence at index 'j'.\n",
    "    tgt_list = []\n",
    "    # start of sentence token id to be added at the beginning of the sentence.\n",
    "    sos_id = torch.tensor([0])\n",
    "    # end of sentence token id to be added at the end of the sentence.\n",
    "    eos_id = torch.tensor([1])\n",
    "    index = 0\n",
    "    for data_point in batch:\n",
    "        src_sentence = data_point[\"src\"]\n",
    "        tgt_sentence = data_point[\"tgt\"]\n",
    "        # This if block is just to print the first sentence pair in the batch to understand how the batch looks.\n",
    "        if index == 0:\n",
    "            print(f\"src_sentence: {src_sentence}\")\n",
    "            print(f\"tgt_sentence: {tgt_sentence}\")\n",
    "            index += 1\n",
    "        # List of tensors to concatenate and dimension along which we want to concatenate the list of tensors.\n",
    "        processed_src = torch.cat([sos_id, torch.tensor(spacy_english_tokenizer.encode(src_sentence)), eos_id], dim=0)\n",
    "        processed_tgt = torch.cat([sos_id, torch.tensor(spacy_telugu_tokenizer.encode(tgt_sentence)), eos_id], dim=0)\n",
    "        # value parameter corresponds to the index we use to represent the padding token.\n",
    "        processed_src = torch.nn.functional.pad(processed_src, (0, MAX_LEN - len(processed_src)), value=2)\n",
    "        processed_tgt = torch.nn.functional.pad(processed_tgt, (0, MAX_LEN - len(processed_tgt)), value=2)\n",
    "        src_list.append(processed_src)\n",
    "        tgt_list.append(processed_tgt)\n",
    "    src = torch.stack(src_list)\n",
    "    tgt = torch.stack(tgt_list)\n",
    "    return (src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "src_sentence: Have you heard about Foie gras?\n",
      "tgt_sentence: ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?\n",
      "src: tensor([[    0,  1082,    46,  1015,    77,     3,     3,    20,     1,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    0,    33,   350,   960,     7,  1742,    10,   267,     4,     1,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    0, 20804,  5347,     1,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2]])\n",
      "tgt: tensor([[    0,    92, 20630,     3,    71,    42,     3,     7,     1,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    0,  3298,  1205, 16494,   531,  5470,     4,     1,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    0,  9491, 29092,     1,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset=wrapped_translation_dataset, batch_size=32, collate_fn=transformer_collate_fn)\n",
    "print(len(dataloader))\n",
    "for src, tgt in dataloader:\n",
    "    print(f\"src: {src}\")\n",
    "    print(f\"tgt: {tgt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating length aware pytorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issues with the DataLoader above:\n",
    "#\n",
    "# ISSUE 1: The above DataLoader batches sentences randomly. So, it is possible a sentence of length 1 is batched \n",
    "#          together with a sentence of length 100. This causes the the data pre-processor to add 97 pad tokens \n",
    "#          (excluding <sos>, <eos> ids) to a sentence of length 1 which results in a huge waste of computation \n",
    "#          during training. \n",
    "#\n",
    "# ISSUE 2: Even if we batch sentences of similar length, the padding for any sentence will not change if we \n",
    "#          maintain the same length (100 in the above case) for every sentence in every batch. Ideally, different \n",
    "#          batches can contain sentences of different lengths but every sentence within a batch should be of same \n",
    "#          length.\n",
    "#\n",
    "# We will handle the above 2 issues below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader expects the sampler to provide the indices of the data points from the Dataset. These indices are used \n",
    "# to construct the batches internally that are later passed to collate_fn. We are creating a sampler which groups \n",
    "# sentences of similar lengths into batches. We still need to be aware that the grouping is not perfect and it is \n",
    "# done here based on the number of words separated by 'spaces' in the sentence and not the actual number of tokens\n",
    "# we obtain while using the tokenization algorithms. The actual number of tokens could be very different depending on \n",
    "# the tokenization algorithm which splits sentences (and words into tokens) based on a number of other factors.\n",
    "class LengthAwareSampler(Sampler):\n",
    "    def __init__(self, dataset: HuggingFaceDatasetWrapper, batch_size: int):\n",
    "        # dataset is the Dataset wrapper we created on top of HuggingFace dataset.\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.sorted_indices = self.extract_lengths()\n",
    "        print(\"sorted_indices: \", self.sorted_indices)\n",
    "    \n",
    "    # We don't want the entire dataset to be loaded into the memory at once. So, we first iterate over the entire \n",
    "    # dataset, extract the lengths of the sentences and sort the indices of the sentences according to the sentence \n",
    "    # lengths. This is to ensure that sentences of similar lengths are grouped together in a batch to minimize the \n",
    "    # overall padding necessary. When we iterate over the dataset, we only load the necessary data from the dataset \n",
    "    # into memory and not the entire dataset at once --> This loading logic could be a little different based on the \n",
    "    # hugging face implementations. Please look into the hugging face documentation for more details.\n",
    "    def extract_lengths(self) -> list[int]:\n",
    "        \"\"\"Sorts the indices of the dataset based on the lengths of the sentences in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            list[int]: Indices of the dataset sorted in ascending order (small to big) based on the lengths of \n",
    "                       the sentences in the dataset.\n",
    "        \"\"\"\n",
    "        # Note that the lengths are calculated based on the number of words separated by space in the sentence and \n",
    "        # not the number of tokens.\n",
    "        self.lengths = [len(data_point[\"src\"].split(\" \")) + len(data_point[\"tgt\"].split(\" \")) for data_point in self.dataset]\n",
    "        print(\"calculated (src + tgt) sentence lengths: \", self.lengths)\n",
    "        # Create an indices list in the first step i.e., [0, 1, 2, ..., 299] --> For debug_dataset.\n",
    "        # Sorts the indices list based on the calculated in the previous step i.e., after sorting\n",
    "        # value at 0th index is the example for which the len(src_sentence) + len(tgt_sentence) is minimum and\n",
    "        # value at 199th index is the example for which the len(src_sentence) + len(tgt_sentence) is maximum. \n",
    "        return sorted(range(len(self.dataset)), key=lambda index: self.lengths[index])\n",
    "\n",
    "    # The __iter__ function is called once per epoch. The returned iterator is iterated on to get the list of indices \n",
    "    # for the data points in a batch.\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        \"\"\"Provides an iterator that yields the indices of the dataset in the order of the sentence lengths.\n",
    "\n",
    "        Returns:\n",
    "            Generator: A generator object that yields the indices of the dataset in the order of the sentence lengths.\n",
    "\n",
    "        Yields:\n",
    "            Generator[list[int], None, None]: A generator object that yields the indices of the dataset in the order\n",
    "                                              of the sentence lengths.\n",
    "        \"\"\"\n",
    "        # Create the batches of indices based on the sentence lengths. \n",
    "        # batches look like: [[0, 5, 90], [23, 4, 5], ...] if batch_size is 3.\n",
    "        # [0, 5, 90] is a batch corresponding to the sentences at indices 0, 5 and 90 in the original dataset.\n",
    "        # [23, 4, 5] is a batch corresponding to the sentences at indices 23, 4 and 5 in the original dataset.\n",
    "        batches = [self.sorted_indices[index: index + self.batch_size] for index in range(0, len(self.dataset), self.batch_size)]\n",
    "        # Shuffle the batches to ensure that the order of batches is different in every epoch. We want the model to \n",
    "        # see the data in different order in every epoch. So, we shuffle the order of the batches within the dataset.\n",
    "        random.shuffle(batches)\n",
    "        # Flatten the list of batches to get an iterable of indices. At the end, the dataloader expects an iterable of\n",
    "        # indices to get the data points from the dataset. So, we convert the list of batches back to an iterable of \n",
    "        # indices.\n",
    "        print(\"batches: \", batches)\n",
    "        return iter([index for batch in batches for index in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_debug_dataset = HuggingFaceDatasetWrapper(hf_dataset=en_te_debug_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated (src + tgt) sentence lengths:  [12, 12, 4, 20, 13, 13, 34, 15, 4, 8, 4, 24, 14, 9, 10, 15, 18, 5, 40, 21, 7, 27, 15, 13, 21, 7, 7, 6, 6, 15, 48, 21, 7, 45, 23, 8, 30, 25, 27, 23, 5, 40, 7, 27, 6, 11, 14, 42, 14, 18, 25, 30, 12, 11, 11, 25, 33, 13, 8, 23, 13, 6, 31, 15, 11, 18, 31, 13, 16, 19, 15, 9, 6, 9, 25, 11, 49, 15, 21, 7, 6, 18, 8, 6, 22, 12, 9, 22, 9, 12, 7, 18, 11, 22, 8, 23, 23, 41, 5, 5, 7, 15, 8, 7, 14, 21, 80, 19, 13, 4, 4, 17, 15, 43, 27, 22, 9, 12, 10, 25, 22, 32, 10, 5, 6, 10, 17, 6, 29, 10, 7, 14, 4, 7, 22, 52, 23, 5, 68, 7, 12, 13, 8, 21, 30, 34, 9, 8, 61, 43, 29, 20, 9, 40, 31, 8, 10, 10, 6, 7, 35, 10, 5, 14, 48, 15, 19, 22, 31, 16, 6, 28, 45, 19, 36, 6, 9, 9, 9, 8, 15, 5, 43, 17, 15, 14, 32, 11, 25, 6, 6, 42, 7, 15, 7, 24, 8, 63, 14, 14]\n",
      "sorted_indices:  [2, 8, 10, 109, 110, 132, 17, 40, 98, 99, 123, 137, 162, 181, 27, 28, 44, 61, 72, 80, 83, 124, 127, 158, 170, 175, 189, 190, 20, 25, 26, 32, 42, 79, 90, 100, 103, 130, 133, 139, 159, 192, 194, 9, 35, 58, 82, 94, 102, 142, 147, 155, 179, 196, 13, 71, 73, 86, 88, 116, 146, 152, 176, 177, 178, 14, 118, 122, 125, 129, 156, 157, 161, 45, 53, 54, 64, 75, 92, 187, 0, 1, 52, 85, 89, 117, 140, 4, 5, 23, 57, 60, 67, 108, 141, 12, 46, 48, 104, 131, 163, 185, 198, 199, 7, 15, 22, 29, 63, 70, 77, 101, 112, 165, 180, 184, 193, 68, 169, 111, 126, 183, 16, 49, 65, 81, 91, 69, 107, 166, 173, 3, 151, 19, 24, 31, 78, 105, 143, 84, 87, 93, 115, 120, 134, 167, 34, 39, 59, 95, 96, 136, 11, 195, 37, 50, 55, 74, 119, 188, 21, 38, 43, 114, 171, 128, 150, 36, 51, 144, 62, 66, 154, 168, 121, 186, 56, 6, 145, 160, 174, 18, 41, 153, 97, 47, 191, 113, 149, 182, 33, 172, 30, 164, 76, 135, 148, 197, 138, 106]\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy length aware sampler object and play around with it.\n",
    "length_aware_sampler = LengthAwareSampler(dataset=wrapped_debug_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches:  [[174, 18, 41, 153, 97, 47, 191, 113, 149, 182], [159, 192, 194, 9, 35, 58, 82, 94, 102, 142], [0, 1, 52, 85, 89, 117, 140, 4, 5, 23], [126, 183, 16, 49, 65, 81, 91, 69, 107, 166], [2, 8, 10, 109, 110, 132, 17, 40, 98, 99], [26, 32, 42, 79, 90, 100, 103, 130, 133, 139], [57, 60, 67, 108, 141, 12, 46, 48, 104, 131], [62, 66, 154, 168, 121, 186, 56, 6, 145, 160], [163, 185, 198, 199, 7, 15, 22, 29, 63, 70], [87, 93, 115, 120, 134, 167, 34, 39, 59, 95], [147, 155, 179, 196, 13, 71, 73, 86, 88, 116], [77, 101, 112, 165, 180, 184, 193, 68, 169, 111], [21, 38, 43, 114, 171, 128, 150, 36, 51, 144], [146, 152, 176, 177, 178, 14, 118, 122, 125, 129], [96, 136, 11, 195, 37, 50, 55, 74, 119, 188], [156, 157, 161, 45, 53, 54, 64, 75, 92, 187], [33, 172, 30, 164, 76, 135, 148, 197, 138, 106], [83, 124, 127, 158, 170, 175, 189, 190, 20, 25], [123, 137, 162, 181, 27, 28, 44, 61, 72, 80], [173, 3, 151, 19, 24, 31, 78, 105, 143, 84]]\n",
      "174\n",
      "18\n",
      "41\n",
      "153\n",
      "97\n",
      "47\n",
      "191\n",
      "113\n",
      "149\n",
      "182\n",
      "159\n",
      "192\n",
      "194\n",
      "9\n",
      "35\n",
      "58\n",
      "82\n",
      "94\n",
      "102\n",
      "142\n",
      "0\n",
      "1\n",
      "52\n",
      "85\n",
      "89\n",
      "117\n",
      "140\n",
      "4\n",
      "5\n",
      "23\n",
      "126\n",
      "183\n",
      "16\n",
      "49\n",
      "65\n",
      "81\n",
      "91\n",
      "69\n",
      "107\n",
      "166\n",
      "2\n",
      "8\n",
      "10\n",
      "109\n",
      "110\n",
      "132\n",
      "17\n",
      "40\n",
      "98\n",
      "99\n",
      "26\n",
      "32\n",
      "42\n",
      "79\n",
      "90\n",
      "100\n",
      "103\n",
      "130\n",
      "133\n",
      "139\n",
      "57\n",
      "60\n",
      "67\n",
      "108\n",
      "141\n",
      "12\n",
      "46\n",
      "48\n",
      "104\n",
      "131\n",
      "62\n",
      "66\n",
      "154\n",
      "168\n",
      "121\n",
      "186\n",
      "56\n",
      "6\n",
      "145\n",
      "160\n",
      "163\n",
      "185\n",
      "198\n",
      "199\n",
      "7\n",
      "15\n",
      "22\n",
      "29\n",
      "63\n",
      "70\n",
      "87\n",
      "93\n",
      "115\n",
      "120\n",
      "134\n",
      "167\n",
      "34\n",
      "39\n",
      "59\n",
      "95\n",
      "147\n",
      "155\n",
      "179\n",
      "196\n",
      "13\n",
      "71\n",
      "73\n",
      "86\n",
      "88\n",
      "116\n",
      "77\n",
      "101\n",
      "112\n",
      "165\n",
      "180\n",
      "184\n",
      "193\n",
      "68\n",
      "169\n",
      "111\n",
      "21\n",
      "38\n",
      "43\n",
      "114\n",
      "171\n",
      "128\n",
      "150\n",
      "36\n",
      "51\n",
      "144\n",
      "146\n",
      "152\n",
      "176\n",
      "177\n",
      "178\n",
      "14\n",
      "118\n",
      "122\n",
      "125\n",
      "129\n",
      "96\n",
      "136\n",
      "11\n",
      "195\n",
      "37\n",
      "50\n",
      "55\n",
      "74\n",
      "119\n",
      "188\n",
      "156\n",
      "157\n",
      "161\n",
      "45\n",
      "53\n",
      "54\n",
      "64\n",
      "75\n",
      "92\n",
      "187\n",
      "33\n",
      "172\n",
      "30\n",
      "164\n",
      "76\n",
      "135\n",
      "148\n",
      "197\n",
      "138\n",
      "106\n",
      "83\n",
      "124\n",
      "127\n",
      "158\n",
      "170\n",
      "175\n",
      "189\n",
      "190\n",
      "20\n",
      "25\n",
      "123\n",
      "137\n",
      "162\n",
      "181\n",
      "27\n",
      "28\n",
      "44\n",
      "61\n",
      "72\n",
      "80\n",
      "173\n",
      "3\n",
      "151\n",
      "19\n",
      "24\n",
      "31\n",
      "78\n",
      "105\n",
      "143\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "# Iterate on the sampler object to retrieve the indices of the data points (examples).\n",
    "for data_point_index in length_aware_sampler:\n",
    "    print(data_point_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_aware_collate_fn(batch: list, english_tokenizer: BaseTokenizer, telugu_tokenizer: BaseTokenizer, sos_id: int = 0, eos_id: int = 1, pad_id: int = 2) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Converts the raw data in the batch into the format required by the MachineTranslationTransformer model. It encodes the\n",
    "       sentences into token ids, adds start, end and padding tokens and batches the converted data back to be used by the \n",
    "       model.\n",
    "\n",
    "    Args:\n",
    "        batch (list): Holds the raw data points (the actual english (src) and telugu (tgt) sentences batched) from the dataset.\n",
    "        english_tokenizer (BaseTokenizer): Tokenizer to tokenize and encode the english sentences into corresponding token ids.\n",
    "        telugu_tokenizer (BaseTokenizer): Tokenizer to tokenize and encode the english sentences into corresponding token ids.\n",
    "        sos_id (int, optional): start of sentence token id. Defaults to 0.\n",
    "        eos_id (int, optional): end of sentence token id. Defaults to 1.\n",
    "        pad_id (int, optional): padding token id. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        Tuple [Tensor, Tensor]: Returns the encoded source and target tensors in the batch which can be used by the transformer model\n",
    "                                as input.\n",
    "    \"\"\"\n",
    "    print(\"batch_type: \", type(batch))\n",
    "    # Holds all the encoded src sentences (english sentences) from the batch. encoded sentence means sentence divided \n",
    "    # into tokens and tokens converted into their integer ids.\n",
    "    # [[0, 223, 4345, 545, 1], [0, 23, 234, 67, 1]] is an example for the processed_src_sentences variable where\n",
    "    # [0, 223, 4345, 545, 1] represents an encoded sentence from the batch and 0 at the start is <sos> and 1 at the \n",
    "    # end is <eos>. \n",
    "    processed_src_sentences = []\n",
    "    # Holds all the encoded tgt sentences (telugu sentences) from the batch.\n",
    "    processed_tgt_sentences = []\n",
    "    index = 0\n",
    "    for data_point in batch:\n",
    "        # src is english sentence.\n",
    "        src_sentence = data_point[\"src\"]\n",
    "        # tgt is telugu sentence.\n",
    "        tgt_sentence = data_point[\"tgt\"]\n",
    "        # start of sentence id to append at the start of every sentence.\n",
    "        sos_tensor = torch.tensor([sos_id], dtype=torch.int64)\n",
    "        # end of sentence id to append at the end of every sentence.\n",
    "        eos_tensor = torch.tensor([eos_id], dtype=torch.int64)\n",
    "        if index < 10:\n",
    "            # This conditional block is to print during experiments and can be removed from the actual code later.\n",
    "            print(f\"src_sentence: {src_sentence}\")\n",
    "            print(f\"tgt_sentence: {tgt_sentence}\")\n",
    "            index += 1\n",
    "        # It is important to set the dtype to 'torch.int64' because we map token_ids to their embeddings in the transformer model.\n",
    "        # '<sos>' and '<eos>' tokens are not added to the src sentences. They are only added to the target sentences.\n",
    "        encoded_src_sentence = torch.tensor(english_tokenizer.encode(src_sentence), dtype=torch.int64)\n",
    "        # prepares the tensor in the format 'token_id(<sos>) token_id1 token_id2 ... last_token_id token_id(<eos>)'. \n",
    "        encoded_tgt_sentence = torch.cat([sos_tensor, torch.tensor(telugu_tokenizer.encode(tgt_sentence), dtype=torch.int64), eos_tensor], dim=0)\n",
    "        processed_src_sentences.append(encoded_src_sentence)\n",
    "        processed_tgt_sentences.append(encoded_tgt_sentence)\n",
    "    # find the maximum length (max_len) of the sentences in the batch so that sentences are padded (if needed) to get all the\n",
    "    # sentences to the same length i.e., max_len.\n",
    "    max_len = max(max(src_ids.size(0) for src_ids in processed_src_sentences), max(tgt_ids.size(0) for tgt_ids in processed_tgt_sentences))\n",
    "    # We pad the sentences with pad token so that every sentence in the batch is of same length. Also, notice \n",
    "    # that the pad token is appended after (not before) the <eos> token is appended to every sentence.\n",
    "    src_ids = [torch.nn.functional.pad(input=src_ids, pad=(0, max_len - src_ids.size(0)), mode=\"constant\", value=pad_id) for src_ids in processed_src_sentences]\n",
    "    tgt_ids = [torch.nn.functional.pad(input=tgt_ids, pad=(0, max_len - tgt_ids.size(0)), mode=\"constant\", value=pad_id) for tgt_ids in processed_tgt_sentences]\n",
    "    # stack the src tensors along dimension 0. This then becomes a 2D tensor of shape (BATCH_SIZE, MAX_SENTENCE_LENGTH).\n",
    "    src = torch.stack(tensors=src_ids, dim=0)\n",
    "    tgt = torch.stack(tensors=tgt_ids, dim=0)\n",
    "    return (src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"The collate_fn is called by the DataLoader with just the batch of data points from the dataset. So, we wrap the \n",
    "       length_aware_collate_fn function with the required parameters to create a collate_fn that can be used by the \n",
    "       DataLoader.\n",
    "\n",
    "    Args:\n",
    "        batch (_type_): Batch of raw data points from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        _type_: Returns the batch of data points in the format required by the MachineTranslationTransformer model.\n",
    "    \"\"\"\n",
    "    sos_id = spacy_english_tokenizer.get_token_id(token=\"<sos>\")\n",
    "    eos_id = spacy_english_tokenizer.get_token_id(token=\"<eos>\")\n",
    "    pad_id = spacy_telugu_tokenizer.get_token_id(token=\"<pad>\")\n",
    "    return length_aware_collate_fn(batch=batch, english_tokenizer=spacy_english_tokenizer, telugu_tokenizer=spacy_telugu_tokenizer, sos_id=sos_id, eos_id=eos_id, pad_id=pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated (src + tgt) sentence lengths:  [12, 12, 4, 20, 13, 13, 34, 15, 4, 8, 4, 24, 14, 9, 10, 15, 18, 5, 40, 21, 7, 27, 15, 13, 21, 7, 7, 6, 6, 15, 48, 21, 7, 45, 23, 8, 30, 25, 27, 23, 5, 40, 7, 27, 6, 11, 14, 42, 14, 18, 25, 30, 12, 11, 11, 25, 33, 13, 8, 23, 13, 6, 31, 15, 11, 18, 31, 13, 16, 19, 15, 9, 6, 9, 25, 11, 49, 15, 21, 7, 6, 18, 8, 6, 22, 12, 9, 22, 9, 12, 7, 18, 11, 22, 8, 23, 23, 41, 5, 5, 7, 15, 8, 7, 14, 21, 80, 19, 13, 4, 4, 17, 15, 43, 27, 22, 9, 12, 10, 25, 22, 32, 10, 5, 6, 10, 17, 6, 29, 10, 7, 14, 4, 7, 22, 52, 23, 5, 68, 7, 12, 13, 8, 21, 30, 34, 9, 8, 61, 43, 29, 20, 9, 40, 31, 8, 10, 10, 6, 7, 35, 10, 5, 14, 48, 15, 19, 22, 31, 16, 6, 28, 45, 19, 36, 6, 9, 9, 9, 8, 15, 5, 43, 17, 15, 14, 32, 11, 25, 6, 6, 42, 7, 15, 7, 24, 8, 63, 14, 14]\n",
      "sorted_indices:  [2, 8, 10, 109, 110, 132, 17, 40, 98, 99, 123, 137, 162, 181, 27, 28, 44, 61, 72, 80, 83, 124, 127, 158, 170, 175, 189, 190, 20, 25, 26, 32, 42, 79, 90, 100, 103, 130, 133, 139, 159, 192, 194, 9, 35, 58, 82, 94, 102, 142, 147, 155, 179, 196, 13, 71, 73, 86, 88, 116, 146, 152, 176, 177, 178, 14, 118, 122, 125, 129, 156, 157, 161, 45, 53, 54, 64, 75, 92, 187, 0, 1, 52, 85, 89, 117, 140, 4, 5, 23, 57, 60, 67, 108, 141, 12, 46, 48, 104, 131, 163, 185, 198, 199, 7, 15, 22, 29, 63, 70, 77, 101, 112, 165, 180, 184, 193, 68, 169, 111, 126, 183, 16, 49, 65, 81, 91, 69, 107, 166, 173, 3, 151, 19, 24, 31, 78, 105, 143, 84, 87, 93, 115, 120, 134, 167, 34, 39, 59, 95, 96, 136, 11, 195, 37, 50, 55, 74, 119, 188, 21, 38, 43, 114, 171, 128, 150, 36, 51, 144, 62, 66, 154, 168, 121, 186, 56, 6, 145, 160, 174, 18, 41, 153, 97, 47, 191, 113, 149, 182, 33, 172, 30, 164, 76, 135, 148, 197, 138, 106]\n"
     ]
    }
   ],
   "source": [
    "length_aware_sampler = LengthAwareSampler(dataset=wrapped_debug_dataset, batch_size=10)\n",
    "dataloader = DataLoader(dataset=wrapped_debug_dataset, batch_size=10, sampler=length_aware_sampler, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fa84fa63be0> <class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "print(dataloader, type(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x7fa83609fc10> <class 'torch.utils.data.dataloader._SingleProcessDataLoaderIter'>\n"
     ]
    }
   ],
   "source": [
    "data_iterator = iter(dataloader)\n",
    "print(data_iterator, type(data_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches:  [[2, 8, 10, 109, 110, 132, 17, 40, 98, 99], [77, 101, 112, 165, 180, 184, 193, 68, 169, 111], [126, 183, 16, 49, 65, 81, 91, 69, 107, 166], [62, 66, 154, 168, 121, 186, 56, 6, 145, 160], [146, 152, 176, 177, 178, 14, 118, 122, 125, 129], [57, 60, 67, 108, 141, 12, 46, 48, 104, 131], [163, 185, 198, 199, 7, 15, 22, 29, 63, 70], [173, 3, 151, 19, 24, 31, 78, 105, 143, 84], [83, 124, 127, 158, 170, 175, 189, 190, 20, 25], [156, 157, 161, 45, 53, 54, 64, 75, 92, 187], [147, 155, 179, 196, 13, 71, 73, 86, 88, 116], [174, 18, 41, 153, 97, 47, 191, 113, 149, 182], [21, 38, 43, 114, 171, 128, 150, 36, 51, 144], [33, 172, 30, 164, 76, 135, 148, 197, 138, 106], [87, 93, 115, 120, 134, 167, 34, 39, 59, 95], [0, 1, 52, 85, 89, 117, 140, 4, 5, 23], [159, 192, 194, 9, 35, 58, 82, 94, 102, 142], [123, 137, 162, 181, 27, 28, 44, 61, 72, 80], [26, 32, 42, 79, 90, 100, 103, 130, 133, 139], [96, 136, 11, 195, 37, 50, 55, 74, 119, 188]]\n",
      "batch_type:  <class 'list'>\n",
      "src_sentence: Installed Software\n",
      "tgt_sentence: స్థాపించబడిన సాఫ్ట్‍వేర్\n",
      "src_sentence: Respect privacy\n",
      "tgt_sentence: గోప్యత పాటించండి\n",
      "src_sentence: Super Bowl.\n",
      "tgt_sentence: \"\"\"సూపర్ బౌల్.\"\n",
      "src_sentence: entrance test\n",
      "tgt_sentence: ప్రవేశ పరీక్షవిధానం\n",
      "src_sentence: Chemical detoxification\n",
      "tgt_sentence: రసాయన శుద్ధి\n",
      "src_sentence: Nuzivedu Road\n",
      "tgt_sentence: నూజివీడు రోడ్డు\n",
      "src_sentence: It cannot work.\n",
      "tgt_sentence: ఏ పనిచేయలేరు.\n",
      "src_sentence: India getting close!\n",
      "tgt_sentence: భారత్ దూసుకుపోతోంది!\n",
      "src_sentence: Prices drop\n",
      "tgt_sentence: ధరలు తగ్గేవి ఇవే\n",
      "src_sentence: Towards the rear\n",
      "tgt_sentence: వెనుక వైపు\n",
      "----------------------------------\n",
      "shapes:  torch.Size([10, 9]) torch.Size([10, 9])\n",
      "batched_data:  (tensor([[34019,  5208,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [11724,  7775,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [ 1678, 26312,     4,     2,     2,     2,     2,     2,     2],\n",
      "        [ 4557,   794,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [ 6638,     3,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    3,  1507,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [   42,    65,    29,   181,     4,     2,     2,     2,     2],\n",
      "        [   45,   641,   919,    77,     2,     2,     2,     2,     2],\n",
      "        [ 2611,  4075,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [10362,     5,  1448,     2,     2,     2,     2,     2,     2]]), tensor([[    0, 14009, 30335,     1,     2,     2,     2,     2,     2],\n",
      "        [    0, 16189,  9559,     1,     2,     2,     2,     2,     2],\n",
      "        [    0,     8,     8,     8,   517, 25870,     4,     8,     1],\n",
      "        [    0,  2397,     3,     1,     2,     2,     2,     2,     2],\n",
      "        [    0,  3739,  5392,     1,     2,     2,     2,     2,     2],\n",
      "        [    0, 25425,   690,     1,     2,     2,     2,     2,     2],\n",
      "        [    0,    89,     3,     4,     1,     2,     2,     2,     2],\n",
      "        [    0,   127, 11641,    34,     1,     2,     2,     2,     2],\n",
      "        [    0,   402, 28324,  3084,     1,     2,     2,     2,     2],\n",
      "        [    0,   803,   848,     1,     2,     2,     2,     2,     2]]))\n"
     ]
    }
   ],
   "source": [
    "batched_data = next(data_iterator)\n",
    "print(\"----------------------------------\")\n",
    "print(\"shapes: \", batched_data[0].shape, batched_data[1].shape)\n",
    "# Notice that <sos> and <eos> token ids are only added to the target sentences and not to the source sentences.\n",
    "# This is inline with what will be fed to the transformer model during training.\n",
    "print(\"batched_data: \", batched_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 1432, 18, 365, 491, 10, 4137, 4] \n",
      "\n",
      "[6, 1707, 492, 139, 10600, 2606, 446, 4]\n"
     ]
    }
   ],
   "source": [
    "# Lets verify the correctness of dataloader.\n",
    "# Notice that the encoded tokens printed in the above cell for the first sentence and the encoded\n",
    "# tokens printed here are the same for both English sentence and Telugu sentence.\n",
    "english_sentence = \"The smartphone was recently launched in Indonesia.\"\n",
    "print(spacy_english_tokenizer.encode(text=english_sentence), \"\\n\")\n",
    "telugu_sentence = \"ఈ స్మార్ట్ ఫోన్ ఇప్పటికే ఇండోనేషియాలో లాంచ్ అయింది.\"\n",
    "print(spacy_telugu_tokenizer.encode(text=telugu_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".attention_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
